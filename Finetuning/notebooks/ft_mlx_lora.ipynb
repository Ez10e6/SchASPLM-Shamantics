{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d952e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from utils_ft import get_model_path, get_output_dir, get_root_path, to_relative\n",
    "from scripts.train_mlx import run_mlx_training, fuse_model\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Config\n",
    "#MODEL_TYPE = \"qwen\"\n",
    "MODEL_TYPE = \"llama\" \n",
    "base_model = get_model_path(MODEL_TYPE)\n",
    "data_folder = os.path.join(get_root_path(), \"Finetuning\", \"data\")\n",
    "adapter_path = os.path.join(get_root_path(), \"Finetuning\", \"adapters\", MODEL_TYPE)\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "fused_output = get_output_dir(MODEL_TYPE, \"mlx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27cf0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing MLX Fine-tuning (CLI Mode) ---\n",
      "--- Model: ./local_models/meta-llama/Meta-Llama-3-8B-Instruct-bfloat16 ---\n",
      "--- Data Folder: ./Finetuning/data ---\n",
      "Generating configuration file at: ./Finetuning/adapters/llama/train_config.yaml\n",
      "Executing MLX CLI training...\n",
      "Loading configuration file /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama/train_config.yaml\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/local_models/meta-llama/Meta-Llama-3-8B-Instruct-bfloat16' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.261% (20.972M/8030.261M)\n",
      "Starting training..., iters: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 0.873, Val took 1.726s\n",
      "Iter 10: Train loss 1.342, Learning Rate 5.000e-05, It/sec 0.420, Tokens/sec 324.135, Trained Tokens 7710, Peak mem 19.047 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20: Val loss 1.581, Val took 1.274s\n",
      "Iter 20: Train loss 0.912, Learning Rate 4.998e-05, It/sec 0.500, Tokens/sec 324.027, Trained Tokens 14189, Peak mem 19.047 GB\n",
      "Iter 30: Train loss 0.816, Learning Rate 4.995e-05, It/sec 0.366, Tokens/sec 325.506, Trained Tokens 23087, Peak mem 19.047 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 40: Val loss 0.994, Val took 1.884s\n",
      "Iter 40: Train loss 0.861, Learning Rate 4.989e-05, It/sec 0.449, Tokens/sec 327.114, Trained Tokens 30372, Peak mem 19.047 GB\n",
      "Iter 50: Train loss 0.655, Learning Rate 4.983e-05, It/sec 0.378, Tokens/sec 327.784, Trained Tokens 39048, Peak mem 19.047 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60: Val loss 0.219, Val took 0.549s\n",
      "Iter 60: Train loss 0.464, Learning Rate 4.973e-05, It/sec 0.437, Tokens/sec 327.569, Trained Tokens 46538, Peak mem 19.047 GB\n",
      "Iter 70: Train loss 0.369, Learning Rate 4.965e-05, It/sec 0.469, Tokens/sec 328.888, Trained Tokens 53545, Peak mem 19.047 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 80: Val loss 0.353, Val took 1.693s\n",
      "Iter 80: Train loss 0.435, Learning Rate 4.951e-05, It/sec 0.508, Tokens/sec 328.178, Trained Tokens 60006, Peak mem 19.047 GB\n",
      "Iter 90: Train loss 0.429, Learning Rate 4.940e-05, It/sec 0.308, Tokens/sec 336.409, Trained Tokens 70939, Peak mem 19.047 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100: Val loss 0.483, Val took 0.553s\n",
      "Iter 100: Train loss 0.215, Learning Rate 4.921e-05, It/sec 0.566, Tokens/sec 329.380, Trained Tokens 76755, Peak mem 19.047 GB\n",
      "Iter 100: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.339, Learning Rate 4.908e-05, It/sec 0.376, Tokens/sec 328.896, Trained Tokens 85511, Peak mem 19.047 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 120: Val loss 0.441, Val took 0.543s\n",
      "Iter 120: Train loss 0.166, Learning Rate 4.886e-05, It/sec 0.460, Tokens/sec 327.942, Trained Tokens 92646, Peak mem 19.047 GB\n",
      "Iter 130: Train loss 0.257, Learning Rate 4.869e-05, It/sec 0.373, Tokens/sec 333.847, Trained Tokens 101601, Peak mem 19.130 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 140: Val loss 0.870, Val took 1.364s\n",
      "Iter 140: Train loss 0.113, Learning Rate 4.843e-05, It/sec 0.412, Tokens/sec 328.735, Trained Tokens 109588, Peak mem 19.130 GB\n",
      "Iter 150: Train loss 0.089, Learning Rate 4.824e-05, It/sec 0.578, Tokens/sec 326.417, Trained Tokens 115234, Peak mem 19.130 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 160: Val loss 0.316, Val took 1.098s\n",
      "Iter 160: Train loss 0.143, Learning Rate 4.794e-05, It/sec 0.390, Tokens/sec 333.036, Trained Tokens 123780, Peak mem 19.130 GB\n",
      "Iter 170: Train loss 0.119, Learning Rate 4.773e-05, It/sec 0.389, Tokens/sec 332.967, Trained Tokens 132344, Peak mem 19.130 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 180: Val loss 0.177, Val took 0.547s\n",
      "Iter 180: Train loss 0.063, Learning Rate 4.739e-05, It/sec 0.498, Tokens/sec 331.142, Trained Tokens 138987, Peak mem 19.130 GB\n",
      "Iter 190: Train loss 0.069, Learning Rate 4.716e-05, It/sec 0.434, Tokens/sec 328.332, Trained Tokens 146549, Peak mem 19.130 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200: Val loss 0.924, Val took 1.887s\n",
      "Iter 200: Train loss 0.086, Learning Rate 4.678e-05, It/sec 0.362, Tokens/sec 329.699, Trained Tokens 155662, Peak mem 19.130 GB\n",
      "Iter 200: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.061, Learning Rate 4.652e-05, It/sec 0.429, Tokens/sec 333.563, Trained Tokens 163437, Peak mem 19.130 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 220: Val loss 0.585, Val took 0.548s\n",
      "Iter 220: Train loss 0.042, Learning Rate 4.611e-05, It/sec 0.418, Tokens/sec 333.617, Trained Tokens 171409, Peak mem 19.130 GB\n",
      "Iter 230: Train loss 0.034, Learning Rate 4.582e-05, It/sec 0.466, Tokens/sec 327.950, Trained Tokens 178454, Peak mem 19.130 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 240: Val loss 1.122, Val took 1.364s\n",
      "Iter 240: Train loss 0.032, Learning Rate 4.538e-05, It/sec 0.427, Tokens/sec 331.353, Trained Tokens 186221, Peak mem 19.130 GB\n",
      "Iter 250: Train loss 0.029, Learning Rate 4.507e-05, It/sec 0.372, Tokens/sec 330.573, Trained Tokens 195115, Peak mem 19.130 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 260: Val loss 0.364, Val took 1.118s\n",
      "Iter 260: Train loss 0.029, Learning Rate 4.459e-05, It/sec 0.420, Tokens/sec 324.638, Trained Tokens 202852, Peak mem 19.130 GB\n",
      "Iter 270: Train loss 0.020, Learning Rate 4.426e-05, It/sec 0.417, Tokens/sec 327.073, Trained Tokens 210694, Peak mem 19.130 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 280: Val loss 0.911, Val took 0.553s\n",
      "Iter 280: Train loss 0.017, Learning Rate 4.375e-05, It/sec 0.471, Tokens/sec 326.268, Trained Tokens 217615, Peak mem 19.130 GB\n",
      "Iter 290: Train loss 0.019, Learning Rate 4.340e-05, It/sec 0.511, Tokens/sec 331.831, Trained Tokens 224113, Peak mem 19.130 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 300: Val loss 0.350, Val took 1.093s\n",
      "Iter 300: Train loss 0.020, Learning Rate 4.286e-05, It/sec 0.398, Tokens/sec 332.872, Trained Tokens 232467, Peak mem 19.130 GB\n",
      "Iter 300: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama/0000300_adapters.safetensors.\n",
      "Saved final weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama/adapters.safetensors.\n",
      "✓ Training completed successfully. Adapters saved.\n"
     ]
    }
   ],
   "source": [
    "run_mlx_training(base_model, data_folder, adapter_path, iters=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d4f114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing LoRA adapters into base model (CLI Mode)...\n",
      "--- Base Model: ./local_models/meta-llama/Meta-Llama-3-8B-Instruct-bfloat16 ---\n",
      "--- Adapter Path: ./Finetuning/adapters/llama ---\n",
      "--- Save Path: ./local_models/ft_llama_mlx ---\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/local_models/meta-llama/Meta-Llama-3-8B-Instruct-bfloat16' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Fused model saved to: ./local_models/ft_llama_mlx\n",
      "Success! Model ready at /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/local_models/ft_llama_mlx\n"
     ]
    }
   ],
   "source": [
    "fuse_model(base_model, adapter_path, fused_output)\n",
    "print(f\"Success! Model ready at {fused_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7a222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schasplm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
