{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d952e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from utils_ft import get_model_path, get_output_dir, get_root_path, to_relative\n",
    "from scripts.train_mlx import run_mlx_training, fuse_model\n",
    "\n",
    "# Config\n",
    "MODEL_TYPE = \"qwen\"\n",
    "#MODEL_TYPE = \"llama\" \n",
    "base_model = get_model_path(MODEL_TYPE)\n",
    "data_folder = os.path.join(get_root_path(), \"Finetuning\", \"data\")\n",
    "adapter_path = os.path.join(get_root_path(), \"Finetuning\", \"adapters\", MODEL_TYPE + \"_mlx\")\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "fused_output = get_output_dir(MODEL_TYPE, \"mlx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27cf0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing MLX Fine-tuning (CLI Mode) ---\n",
      "--- Model: ./local_models/Qwen/Qwen2.5-7B-Instruct-bfloat16 ---\n",
      "--- Data Folder: ./Finetuning/data ---\n",
      "Generating configuration file at: ./Finetuning/adapters/qwen_mlx/train_config.yaml\n",
      "Executing MLX CLI training...\n",
      "Loading configuration file /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/qwen_mlx/train_config.yaml\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/local_models/Qwen/Qwen2.5-7B-Instruct-bfloat16' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.303% (23.069M/7615.617M)\n",
      "Starting training..., iters: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:06<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 1.241, Val took 6.897s\n",
      "Iter 10: Train loss 1.298, Learning Rate 5.000e-05, It/sec 0.404, Tokens/sec 321.968, Trained Tokens 7960, Peak mem 18.454 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:05<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20: Val loss 0.821, Val took 5.088s\n",
      "Iter 20: Train loss 0.895, Learning Rate 4.998e-05, It/sec 0.473, Tokens/sec 316.392, Trained Tokens 14642, Peak mem 18.454 GB\n",
      "Iter 30: Train loss 0.740, Learning Rate 4.995e-05, It/sec 0.357, Tokens/sec 324.431, Trained Tokens 23718, Peak mem 18.454 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:05<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 40: Val loss 0.524, Val took 5.924s\n",
      "Iter 40: Train loss 0.793, Learning Rate 4.989e-05, It/sec 0.439, Tokens/sec 324.363, Trained Tokens 31105, Peak mem 18.454 GB\n",
      "Iter 50: Train loss 0.601, Learning Rate 4.983e-05, It/sec 0.368, Tokens/sec 324.757, Trained Tokens 39932, Peak mem 18.454 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:05<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60: Val loss 0.562, Val took 5.532s\n",
      "Iter 60: Train loss 0.418, Learning Rate 4.973e-05, It/sec 0.429, Tokens/sec 326.385, Trained Tokens 47539, Peak mem 18.454 GB\n",
      "Iter 70: Train loss 0.327, Learning Rate 4.965e-05, It/sec 0.444, Tokens/sec 325.126, Trained Tokens 54862, Peak mem 18.454 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:04<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 80: Val loss 0.291, Val took 4.511s\n",
      "Iter 80: Train loss 0.382, Learning Rate 4.951e-05, It/sec 0.490, Tokens/sec 324.004, Trained Tokens 61476, Peak mem 18.454 GB\n",
      "Iter 90: Train loss 0.370, Learning Rate 4.940e-05, It/sec 0.306, Tokens/sec 338.746, Trained Tokens 72550, Peak mem 18.454 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:05<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100: Val loss 0.498, Val took 5.007s\n",
      "Iter 100: Train loss 0.190, Learning Rate 4.921e-05, It/sec 0.543, Tokens/sec 323.148, Trained Tokens 78502, Peak mem 18.454 GB\n",
      "Iter 100: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/qwen_mlx/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/qwen_mlx/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.319, Learning Rate 4.908e-05, It/sec 0.373, Tokens/sec 329.058, Trained Tokens 87315, Peak mem 18.478 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:04<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 120: Val loss 0.633, Val took 4.160s\n",
      "Iter 120: Train loss 0.150, Learning Rate 4.886e-05, It/sec 0.438, Tokens/sec 326.286, Trained Tokens 94765, Peak mem 18.478 GB\n",
      "Iter 130: Train loss 0.228, Learning Rate 4.869e-05, It/sec 0.363, Tokens/sec 331.293, Trained Tokens 103900, Peak mem 18.547 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:04<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 140: Val loss 0.754, Val took 4.683s\n",
      "Iter 140: Train loss 0.111, Learning Rate 4.843e-05, It/sec 0.399, Tokens/sec 325.628, Trained Tokens 112060, Peak mem 18.547 GB\n",
      "Iter 150: Train loss 0.085, Learning Rate 4.824e-05, It/sec 0.555, Tokens/sec 319.073, Trained Tokens 117812, Peak mem 18.547 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:06<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 160: Val loss 0.817, Val took 6.515s\n",
      "Iter 160: Train loss 0.133, Learning Rate 4.794e-05, It/sec 0.384, Tokens/sec 331.033, Trained Tokens 126430, Peak mem 18.547 GB\n",
      "Iter 170: Train loss 0.098, Learning Rate 4.773e-05, It/sec 0.371, Tokens/sec 333.010, Trained Tokens 135409, Peak mem 18.547 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:06<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 180: Val loss 0.799, Val took 6.007s\n",
      "Iter 180: Train loss 0.064, Learning Rate 4.739e-05, It/sec 0.470, Tokens/sec 325.279, Trained Tokens 142327, Peak mem 18.547 GB\n",
      "Iter 190: Train loss 0.056, Learning Rate 4.716e-05, It/sec 0.427, Tokens/sec 326.590, Trained Tokens 149969, Peak mem 18.547 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:05<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200: Val loss 0.829, Val took 5.036s\n",
      "Iter 200: Train loss 0.067, Learning Rate 4.678e-05, It/sec 0.358, Tokens/sec 330.832, Trained Tokens 159201, Peak mem 18.547 GB\n",
      "Iter 200: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/qwen_mlx/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/qwen_mlx/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.074, Learning Rate 4.652e-05, It/sec 0.412, Tokens/sec 328.344, Trained Tokens 167162, Peak mem 18.547 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:05<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 220: Val loss 0.907, Val took 5.397s\n",
      "Iter 220: Train loss 0.024, Learning Rate 4.611e-05, It/sec 0.394, Tokens/sec 330.500, Trained Tokens 175550, Peak mem 18.547 GB\n",
      "Iter 230: Train loss 0.028, Learning Rate 4.582e-05, It/sec 0.460, Tokens/sec 325.107, Trained Tokens 182612, Peak mem 18.547 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:04<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 240: Val loss 0.696, Val took 4.888s\n",
      "Iter 240: Train loss 0.030, Learning Rate 4.538e-05, It/sec 0.415, Tokens/sec 325.377, Trained Tokens 190452, Peak mem 18.547 GB\n",
      "Iter 250: Train loss 0.028, Learning Rate 4.507e-05, It/sec 0.367, Tokens/sec 333.899, Trained Tokens 199557, Peak mem 18.547 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 260: Val loss 0.771, Val took 5.301s\n",
      "Iter 260: Train loss 0.032, Learning Rate 4.459e-05, It/sec 0.410, Tokens/sec 328.783, Trained Tokens 207579, Peak mem 18.547 GB\n",
      "Iter 270: Train loss 0.015, Learning Rate 4.426e-05, It/sec 0.406, Tokens/sec 326.025, Trained Tokens 215610, Peak mem 18.547 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 280: Val loss 0.359, Val took 3.331s\n",
      "Iter 280: Train loss 0.017, Learning Rate 4.375e-05, It/sec 0.460, Tokens/sec 324.649, Trained Tokens 222664, Peak mem 18.547 GB\n",
      "Iter 290: Train loss 0.016, Learning Rate 4.340e-05, It/sec 0.487, Tokens/sec 327.317, Trained Tokens 229385, Peak mem 18.547 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 300: Val loss 0.880, Val took 5.156s\n",
      "Iter 300: Train loss 0.019, Learning Rate 4.286e-05, It/sec 0.392, Tokens/sec 329.767, Trained Tokens 237795, Peak mem 18.547 GB\n",
      "Iter 300: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/qwen_mlx/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/qwen_mlx/0000300_adapters.safetensors.\n",
      "Saved final weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/qwen_mlx/adapters.safetensors.\n",
      "✓ Training completed successfully. Adapters saved.\n"
     ]
    }
   ],
   "source": [
    "run_mlx_training(base_model, data_folder, adapter_path, iters=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d4f114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing LoRA adapters into base model (CLI Mode)...\n",
      "--- Base Model: ./local_models/Qwen/Qwen2.5-7B-Instruct-bfloat16 ---\n",
      "--- Adapter Path: ./Finetuning/adapters/qwen_mlx ---\n",
      "--- Save Path: ./local_models/ft_qwen_mlx ---\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/local_models/Qwen/Qwen2.5-7B-Instruct-bfloat16' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Fused model saved to: ./local_models/ft_qwen_mlx\n",
      "Success! Model ready at /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/local_models/ft_qwen_mlx\n"
     ]
    }
   ],
   "source": [
    "fuse_model(base_model, adapter_path, fused_output)\n",
    "print(f\"Success! Model ready at {fused_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7a222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schasplm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
