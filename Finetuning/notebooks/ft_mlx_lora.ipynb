{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d952e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from utils_ft import get_model_path, get_output_dir, get_root_path, to_relative\n",
    "from scripts.train_mlx import run_mlx_training, fuse_model\n",
    "\n",
    "# Config\n",
    "MODEL_TYPE = \"qwen\"\n",
    "#MODEL_TYPE = \"llama\" \n",
    "base_model = get_model_path(MODEL_TYPE)\n",
    "data_folder = os.path.join(get_root_path(), \"Finetuning\", \"data\")\n",
    "adapter_path = os.path.join(get_root_path(), \"Finetuning\", \"adapters\", MODEL_TYPE + \"_mlx\")\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "fused_output = get_output_dir(MODEL_TYPE, \"mlx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27cf0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing MLX Fine-tuning (CLI Mode) ---\n",
      "--- Model: ./Finetuning/notebooks/meta-llama/Meta-Llama-3-8B-Instruct ---\n",
      "--- Data Folder: ./Finetuning/data ---\n",
      "Generating configuration file at: ./Finetuning/adapters/llama_mlx/train_config.yaml\n",
      "Executing MLX CLI training...\n",
      "Loading configuration file /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/train_config.yaml\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 56959.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.261% (20.972M/8030.261M)\n",
      "Starting training..., iters: 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:06<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 1.901, Val took 6.476s\n",
      "Iter 10: Train loss 2.406, Learning Rate 5.000e-05, It/sec 0.622, Tokens/sec 311.934, Trained Tokens 5019, Peak mem 19.403 GB\n",
      "Iter 20: Train loss 1.159, Learning Rate 5.000e-05, It/sec 0.497, Tokens/sec 317.462, Trained Tokens 11403, Peak mem 19.403 GB\n",
      "Iter 30: Train loss 0.917, Learning Rate 4.999e-05, It/sec 0.736, Tokens/sec 303.475, Trained Tokens 15527, Peak mem 19.865 GB\n",
      "Iter 40: Train loss 0.992, Learning Rate 4.998e-05, It/sec 0.574, Tokens/sec 318.882, Trained Tokens 21086, Peak mem 19.865 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:04<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: Val loss 0.798, Val took 4.275s\n",
      "Iter 50: Train loss 0.771, Learning Rate 4.997e-05, It/sec 0.499, Tokens/sec 316.385, Trained Tokens 27423, Peak mem 19.865 GB\n",
      "Iter 60: Train loss 0.669, Learning Rate 4.995e-05, It/sec 0.997, Tokens/sec 307.904, Trained Tokens 30512, Peak mem 19.865 GB\n",
      "Iter 70: Train loss 0.525, Learning Rate 4.994e-05, It/sec 1.041, Tokens/sec 305.957, Trained Tokens 33450, Peak mem 19.865 GB\n",
      "Iter 80: Train loss 0.703, Learning Rate 4.991e-05, It/sec 0.647, Tokens/sec 322.693, Trained Tokens 38439, Peak mem 19.865 GB\n",
      "Iter 90: Train loss 0.683, Learning Rate 4.989e-05, It/sec 0.654, Tokens/sec 313.149, Trained Tokens 43227, Peak mem 19.865 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:03<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100: Val loss 0.732, Val took 3.168s\n",
      "Iter 100: Train loss 0.610, Learning Rate 4.986e-05, It/sec 0.591, Tokens/sec 314.010, Trained Tokens 48544, Peak mem 19.865 GB\n",
      "Iter 100: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.472, Learning Rate 4.983e-05, It/sec 0.783, Tokens/sec 316.020, Trained Tokens 52578, Peak mem 19.941 GB\n",
      "Iter 120: Train loss 0.563, Learning Rate 4.979e-05, It/sec 0.626, Tokens/sec 311.607, Trained Tokens 57554, Peak mem 19.941 GB\n",
      "Iter 130: Train loss 0.636, Learning Rate 4.976e-05, It/sec 0.583, Tokens/sec 326.860, Trained Tokens 63158, Peak mem 19.941 GB\n",
      "Iter 140: Train loss 0.621, Learning Rate 4.971e-05, It/sec 0.874, Tokens/sec 318.972, Trained Tokens 66809, Peak mem 19.941 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:04<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 150: Val loss 0.878, Val took 4.272s\n",
      "Iter 150: Train loss 0.449, Learning Rate 4.967e-05, It/sec 1.211, Tokens/sec 293.750, Trained Tokens 69234, Peak mem 19.941 GB\n",
      "Iter 160: Train loss 0.579, Learning Rate 4.962e-05, It/sec 0.679, Tokens/sec 318.108, Trained Tokens 73919, Peak mem 19.941 GB\n",
      "Iter 170: Train loss 0.362, Learning Rate 4.958e-05, It/sec 1.389, Tokens/sec 299.253, Trained Tokens 76074, Peak mem 19.941 GB\n",
      "Iter 180: Train loss 0.357, Learning Rate 4.951e-05, It/sec 1.761, Tokens/sec 277.570, Trained Tokens 77650, Peak mem 19.941 GB\n",
      "Iter 190: Train loss 0.564, Learning Rate 4.947e-05, It/sec 0.477, Tokens/sec 326.471, Trained Tokens 84500, Peak mem 19.941 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:01<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200: Val loss 0.629, Val took 1.894s\n",
      "Iter 200: Train loss 0.562, Learning Rate 4.940e-05, It/sec 0.553, Tokens/sec 325.246, Trained Tokens 90377, Peak mem 19.941 GB\n",
      "Iter 200: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.388, Learning Rate 4.935e-05, It/sec 0.903, Tokens/sec 315.073, Trained Tokens 93866, Peak mem 19.941 GB\n",
      "Iter 220: Train loss 0.481, Learning Rate 4.927e-05, It/sec 0.689, Tokens/sec 322.632, Trained Tokens 98549, Peak mem 19.941 GB\n",
      "Iter 230: Train loss 0.360, Learning Rate 4.921e-05, It/sec 0.806, Tokens/sec 301.298, Trained Tokens 102288, Peak mem 19.941 GB\n",
      "Iter 240: Train loss 0.433, Learning Rate 4.913e-05, It/sec 0.641, Tokens/sec 316.547, Trained Tokens 107229, Peak mem 19.941 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:04<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 250: Val loss 0.764, Val took 4.735s\n",
      "Iter 250: Train loss 0.484, Learning Rate 4.907e-05, It/sec 0.543, Tokens/sec 328.543, Trained Tokens 113274, Peak mem 19.941 GB\n",
      "Iter 260: Train loss 0.605, Learning Rate 4.898e-05, It/sec 0.471, Tokens/sec 318.311, Trained Tokens 120038, Peak mem 20.116 GB\n",
      "Iter 270: Train loss 0.437, Learning Rate 4.891e-05, It/sec 0.889, Tokens/sec 308.294, Trained Tokens 123506, Peak mem 20.116 GB\n",
      "Iter 280: Train loss 0.357, Learning Rate 4.881e-05, It/sec 1.007, Tokens/sec 309.766, Trained Tokens 126581, Peak mem 20.116 GB\n",
      "Iter 290: Train loss 0.366, Learning Rate 4.874e-05, It/sec 0.751, Tokens/sec 314.613, Trained Tokens 130770, Peak mem 20.116 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:04<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 300: Val loss 0.982, Val took 4.291s\n",
      "Iter 300: Train loss 0.438, Learning Rate 4.863e-05, It/sec 0.562, Tokens/sec 317.134, Trained Tokens 136416, Peak mem 20.116 GB\n",
      "Iter 300: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.383, Learning Rate 4.856e-05, It/sec 0.906, Tokens/sec 304.927, Trained Tokens 139783, Peak mem 20.116 GB\n",
      "Iter 320: Train loss 0.396, Learning Rate 4.845e-05, It/sec 0.573, Tokens/sec 320.039, Trained Tokens 145369, Peak mem 20.116 GB\n",
      "Iter 330: Train loss 0.356, Learning Rate 4.837e-05, It/sec 0.630, Tokens/sec 321.755, Trained Tokens 150477, Peak mem 20.116 GB\n",
      "Iter 340: Train loss 0.508, Learning Rate 4.824e-05, It/sec 0.682, Tokens/sec 316.231, Trained Tokens 155112, Peak mem 20.116 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:03<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 350: Val loss 1.004, Val took 3.448s\n",
      "Iter 350: Train loss 0.413, Learning Rate 4.816e-05, It/sec 0.405, Tokens/sec 325.289, Trained Tokens 163148, Peak mem 20.116 GB\n",
      "Iter 360: Train loss 0.278, Learning Rate 4.803e-05, It/sec 1.062, Tokens/sec 305.319, Trained Tokens 166024, Peak mem 20.116 GB\n",
      "Iter 370: Train loss 0.349, Learning Rate 4.794e-05, It/sec 0.533, Tokens/sec 324.992, Trained Tokens 172125, Peak mem 20.116 GB\n",
      "Iter 380: Train loss 0.434, Learning Rate 4.781e-05, It/sec 0.546, Tokens/sec 323.880, Trained Tokens 178058, Peak mem 20.116 GB\n",
      "Iter 390: Train loss 0.393, Learning Rate 4.772e-05, It/sec 0.619, Tokens/sec 319.265, Trained Tokens 183217, Peak mem 20.116 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:04<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 400: Val loss 1.005, Val took 4.622s\n",
      "Iter 400: Train loss 0.259, Learning Rate 4.757e-05, It/sec 0.727, Tokens/sec 319.576, Trained Tokens 187612, Peak mem 20.116 GB\n",
      "Iter 400: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.311, Learning Rate 4.748e-05, It/sec 0.778, Tokens/sec 318.381, Trained Tokens 191704, Peak mem 20.116 GB\n",
      "Iter 420: Train loss 0.316, Learning Rate 4.733e-05, It/sec 1.102, Tokens/sec 293.021, Trained Tokens 194364, Peak mem 20.116 GB\n",
      "Iter 430: Train loss 0.494, Learning Rate 4.722e-05, It/sec 0.453, Tokens/sec 321.725, Trained Tokens 201459, Peak mem 20.116 GB\n",
      "Iter 440: Train loss 0.228, Learning Rate 4.707e-05, It/sec 1.882, Tokens/sec 275.497, Trained Tokens 202923, Peak mem 20.116 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 450: Val loss 0.378, Val took 1.970s\n",
      "Iter 450: Train loss 0.321, Learning Rate 4.696e-05, It/sec 1.026, Tokens/sec 304.536, Trained Tokens 205890, Peak mem 20.116 GB\n",
      "Iter 460: Train loss 0.312, Learning Rate 4.680e-05, It/sec 0.666, Tokens/sec 315.198, Trained Tokens 210624, Peak mem 20.116 GB\n",
      "Iter 470: Train loss 0.326, Learning Rate 4.669e-05, It/sec 0.497, Tokens/sec 317.796, Trained Tokens 217018, Peak mem 20.116 GB\n",
      "Iter 480: Train loss 0.328, Learning Rate 4.652e-05, It/sec 0.474, Tokens/sec 325.411, Trained Tokens 223884, Peak mem 20.116 GB\n",
      "Iter 490: Train loss 0.352, Learning Rate 4.640e-05, It/sec 0.653, Tokens/sec 324.147, Trained Tokens 228849, Peak mem 20.116 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:03<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500: Val loss 0.673, Val took 3.954s\n",
      "Iter 500: Train loss 0.397, Learning Rate 4.623e-05, It/sec 0.445, Tokens/sec 325.336, Trained Tokens 236161, Peak mem 20.116 GB\n",
      "Iter 500: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 0.310, Learning Rate 4.611e-05, It/sec 0.909, Tokens/sec 302.477, Trained Tokens 239487, Peak mem 20.116 GB\n",
      "Iter 520: Train loss 0.357, Learning Rate 4.593e-05, It/sec 0.418, Tokens/sec 324.185, Trained Tokens 247240, Peak mem 20.116 GB\n",
      "Iter 530: Train loss 0.331, Learning Rate 4.580e-05, It/sec 0.552, Tokens/sec 317.481, Trained Tokens 252987, Peak mem 20.116 GB\n",
      "Iter 540: Train loss 0.475, Learning Rate 4.561e-05, It/sec 0.312, Tokens/sec 324.352, Trained Tokens 263398, Peak mem 20.116 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:03<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 550: Val loss 0.934, Val took 3.986s\n",
      "Iter 550: Train loss 0.364, Learning Rate 4.549e-05, It/sec 0.647, Tokens/sec 322.719, Trained Tokens 268387, Peak mem 20.116 GB\n",
      "Iter 560: Train loss 0.371, Learning Rate 4.529e-05, It/sec 0.531, Tokens/sec 321.189, Trained Tokens 274436, Peak mem 20.116 GB\n",
      "Iter 570: Train loss 0.186, Learning Rate 4.516e-05, It/sec 1.095, Tokens/sec 294.869, Trained Tokens 277129, Peak mem 20.116 GB\n",
      "Iter 580: Train loss 0.312, Learning Rate 4.496e-05, It/sec 1.197, Tokens/sec 300.364, Trained Tokens 279638, Peak mem 20.116 GB\n",
      "Iter 590: Train loss 0.290, Learning Rate 4.482e-05, It/sec 0.509, Tokens/sec 326.040, Trained Tokens 286041, Peak mem 20.116 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:03<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 600: Val loss 1.090, Val took 3.150s\n",
      "Iter 600: Train loss 0.369, Learning Rate 4.462e-05, It/sec 0.491, Tokens/sec 324.376, Trained Tokens 292646, Peak mem 20.116 GB\n",
      "Iter 600: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 0.301, Learning Rate 4.448e-05, It/sec 0.555, Tokens/sec 315.923, Trained Tokens 298342, Peak mem 20.116 GB\n",
      "Iter 620: Train loss 0.213, Learning Rate 4.426e-05, It/sec 1.295, Tokens/sec 287.963, Trained Tokens 300565, Peak mem 20.116 GB\n",
      "Iter 630: Train loss 0.264, Learning Rate 4.412e-05, It/sec 0.553, Tokens/sec 324.964, Trained Tokens 306446, Peak mem 20.116 GB\n",
      "Iter 640: Train loss 0.201, Learning Rate 4.390e-05, It/sec 0.782, Tokens/sec 316.645, Trained Tokens 310494, Peak mem 20.116 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:05<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 650: Val loss 0.862, Val took 5.788s\n",
      "Iter 650: Train loss 0.260, Learning Rate 4.375e-05, It/sec 0.556, Tokens/sec 320.889, Trained Tokens 316267, Peak mem 20.116 GB\n",
      "Iter 660: Train loss 0.229, Learning Rate 4.353e-05, It/sec 0.462, Tokens/sec 325.767, Trained Tokens 323316, Peak mem 20.116 GB\n",
      "Iter 670: Train loss 0.299, Learning Rate 4.338e-05, It/sec 0.681, Tokens/sec 319.170, Trained Tokens 328001, Peak mem 20.116 GB\n",
      "Iter 680: Train loss 0.317, Learning Rate 4.315e-05, It/sec 0.742, Tokens/sec 314.462, Trained Tokens 332240, Peak mem 20.116 GB\n",
      "Iter 690: Train loss 0.198, Learning Rate 4.299e-05, It/sec 0.680, Tokens/sec 319.719, Trained Tokens 336943, Peak mem 20.116 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 5/5 [00:02<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 700: Val loss 0.718, Val took 2.796s\n",
      "Iter 700: Train loss 0.253, Learning Rate 4.276e-05, It/sec 0.602, Tokens/sec 324.532, Trained Tokens 342332, Peak mem 20.116 GB\n",
      "Iter 700: Saved adapter weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/adapters.safetensors and /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/0000700_adapters.safetensors.\n",
      "Saved final weights to /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/adapters/llama_mlx/adapters.safetensors.\n",
      "✓ Training completed successfully. Adapters saved.\n"
     ]
    }
   ],
   "source": [
    "run_mlx_training(base_model, data_folder, adapter_path, iters=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d4f114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing LoRA adapters into base model (CLI Mode)...\n",
      "--- Base Model: ./Finetuning/notebooks/meta-llama/Meta-Llama-3-8B-Instruct ---\n",
      "--- Adapter Path: ./Finetuning/adapters/llama_mlx ---\n",
      "--- Save Path: ./local_models/ft_llama_mlx ---\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 15784.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Fused model saved to: ./local_models/ft_llama_mlx\n",
      "Success! Model ready at /Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/local_models/ft_llama_mlx\n"
     ]
    }
   ],
   "source": [
    "fuse_model(base_model, adapter_path, fused_output)\n",
    "print(f\"Success! Model ready at {fused_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7a222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schasplm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
