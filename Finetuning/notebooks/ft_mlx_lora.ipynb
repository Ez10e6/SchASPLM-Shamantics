{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d952e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from utils_ft import get_model_path, get_output_dir, get_root_path, to_relative\n",
    "from scripts.train_mlx import run_mlx_training, fuse_model\n",
    "\n",
    "# Config\n",
    "MODEL_TYPE = \"qwen\"\n",
    "base_model = get_model_path(MODEL_TYPE)\n",
    "data_folder = os.path.join(get_root_path(), \"Finetuning\", \"data\")\n",
    "adapter_path = os.path.join(get_root_path(), \"Finetuning\", \"adapters\", MODEL_TYPE + \"_mlx\")\n",
    "fused_output = get_output_dir(MODEL_TYPE, \"mlx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing MLX Fine-tuning (CLI Mode) ---\n",
      "--- Model: ./Qwen/Qwen2.5-7B-Instruct ---\n",
      "--- Data Folder: ./../data ---\n",
      "Generating configuration file at: ./../adapters/qwen_mlx/train_config.yaml\n",
      "Executing MLX CLI training...\n",
      "Loading configuration file ../adapters/qwen_mlx/train_config.yaml\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 35737.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.303% (23.069M/7615.617M)\n",
      "Starting training..., iters: 1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 128/128 [00:49<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 2.531, Val took 49.046s\n",
      "Iter 100: Train loss 2.200, Learning Rate 1.990e-05, It/sec 1.174, Tokens/sec 44.676, Trained Tokens 3805, Peak mem 19.551 GB\n",
      "Iter 200: Train loss 0.288, Learning Rate 1.953e-05, It/sec 1.427, Tokens/sec 54.223, Trained Tokens 7606, Peak mem 19.682 GB\n",
      "Iter 300: Train loss 0.299, Learning Rate 1.888e-05, It/sec 1.133, Tokens/sec 45.008, Trained Tokens 11577, Peak mem 19.682 GB\n",
      "Iter 400: Train loss 0.229, Learning Rate 1.782e-05, It/sec 1.194, Tokens/sec 53.949, Trained Tokens 16094, Peak mem 19.682 GB\n",
      "Iter 500: Train loss 0.247, Learning Rate 1.666e-05, It/sec 1.094, Tokens/sec 46.923, Trained Tokens 20383, Peak mem 19.682 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 128/128 [00:48<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 600: Val loss 0.115, Val took 48.315s\n",
      "Iter 600: Train loss 0.132, Learning Rate 1.532e-05, It/sec 1.049, Tokens/sec 49.103, Trained Tokens 25065, Peak mem 19.945 GB\n",
      "Iter 600: Saved adapter weights to ../adapters/qwen_mlx/adapters.safetensors and ../adapters/qwen_mlx/0000600_adapters.safetensors.\n",
      "Iter 700: Train loss 0.157, Learning Rate 1.383e-05, It/sec 1.053, Tokens/sec 38.728, Trained Tokens 28743, Peak mem 19.945 GB\n",
      "Iter 800: Train loss 0.168, Learning Rate 1.195e-05, It/sec 0.920, Tokens/sec 37.109, Trained Tokens 32776, Peak mem 20.140 GB\n",
      "Iter 900: Train loss 0.197, Learning Rate 1.028e-05, It/sec 0.989, Tokens/sec 39.895, Trained Tokens 36809, Peak mem 20.436 GB\n",
      "Iter 1000: Train loss 0.135, Learning Rate 8.602e-06, It/sec 1.057, Tokens/sec 46.029, Trained Tokens 41163, Peak mem 20.436 GB\n",
      "Iter 1100: Train loss 0.177, Learning Rate 6.963e-06, It/sec 0.984, Tokens/sec 37.875, Trained Tokens 45012, Peak mem 20.436 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 128/128 [00:56<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1200: Val loss 0.182, Val took 56.994s\n",
      "Iter 1200: Train loss 0.096, Learning Rate 5.163e-06, It/sec 1.085, Tokens/sec 43.474, Trained Tokens 49017, Peak mem 20.436 GB\n",
      "Iter 1200: Saved adapter weights to ../adapters/qwen_mlx/adapters.safetensors and ../adapters/qwen_mlx/0001200_adapters.safetensors.\n",
      "Iter 1300: Train loss 0.127, Learning Rate 3.765e-06, It/sec 1.267, Tokens/sec 50.851, Trained Tokens 53032, Peak mem 20.436 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/mlx_lm/__main__.py\", line 30, in <module>\n",
      "    submodule.main()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/mlx_lm/lora.py\", line 362, in main\n",
      "    run(types.SimpleNamespace(**args))\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/mlx_lm/lora.py\", line 334, in run\n",
      "    train_model(args, model, train_set, valid_set, training_callback)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/mlx_lm/lora.py\", line 288, in train_model\n",
      "    train(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/mlx_lm/tuner/trainer.py\", line 312, in train\n",
      "    mx.eval(state, losses, n_tokens, grad_accum)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_mlx_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/Finetuning/notebooks/../scripts/train_mlx.py:82\u001b[39m, in \u001b[36mrun_mlx_training\u001b[39m\u001b[34m(model_path, data_folder, adapter_path, iters, num_layers)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExecuting MLX CLI training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Training completed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess.CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/subprocess.py:550\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    552\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/subprocess.py:1201\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1199\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1200\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/subprocess.py:1264\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1266\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/subprocess.py:2053\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2052\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2053\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2056\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2057\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/subprocess.py:2011\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2010\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2011\u001b[39m     (pid, sts) = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2013\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2014\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2015\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2016\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "run_mlx_training(base_model, data_folder, adapter_path, iters=2000, num_layers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d4f114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing LoRA adapters (Step: Final)...\n",
      "--- Staging checkpoint from: adapters.safetensors ---\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 33949.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Fused model saved to: ./local_models/ft_qwen_mlx\n",
      "Success! Model ready at ./local_models/ft_qwen_mlx\n"
     ]
    }
   ],
   "source": [
    "fuse_model(base_model, adapter_path, fused_output)\n",
    "print(f\"Success! Model ready at ./{to_relative(fused_output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "MODEL_TYPE = \"llama\" \n",
    "base_model = get_model_path(MODEL_TYPE)\n",
    "data_folder = os.path.join(get_root_path(), \"Finetuning\", \"data\")\n",
    "adapter_path = os.path.join(get_root_path(), \"Finetuning\", \"adapters\", MODEL_TYPE + \"_mlx\")\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "fused_output = get_output_dir(MODEL_TYPE, \"mlx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mlx_training(base_model, data_folder, adapter_path, iters=2000, num_layers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ef930",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_model(base_model, adapter_path, fused_output)\n",
    "print(f\"Success! Model ready at ./{to_relative(fused_output)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schasplm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
