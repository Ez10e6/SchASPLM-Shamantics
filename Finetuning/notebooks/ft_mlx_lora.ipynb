{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d952e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration ---\n",
      "Base Model:   Qwen/Qwen2.5-7B-Instruct\n",
      "Data Folder:  Finetuning/data\n",
      "Adapter Path: Finetuning/adapters/qwen_mlx\n",
      "Output Path:  local_models/ft_qwen_mlx\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory (Finetuning/) to sys.path to access utils_ft\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils_ft import (\n",
    "    get_model_path, \n",
    "    get_output_dir, \n",
    "    get_processed_data_path, \n",
    "    get_adapters_path, \n",
    "    to_relative,\n",
    "    setup_env\n",
    ")\n",
    "from scripts.train_mlx import run_mlx_training, fuse_model\n",
    "\n",
    "# 1. Setup Environment (Load .env variables if needed)\n",
    "setup_env()\n",
    "\n",
    "# 2. Config\n",
    "MODEL_TYPE = \"qwen\" \n",
    "FRAMEWORK = \"mlx\"\n",
    "\n",
    "# 3. Define Paths using the new Utils\n",
    "# This guarantees they match the structure used by the data converter\n",
    "base_model = get_model_path(MODEL_TYPE)\n",
    "data_folder = get_processed_data_path()                   # .../Finetuning/data\n",
    "adapter_path = get_adapters_path(MODEL_TYPE, FRAMEWORK)   # .../Finetuning/adapters/qwen_mlx\n",
    "fused_output = get_output_dir(MODEL_TYPE, FRAMEWORK)      # .../local_models/ft_qwen_mlx\n",
    "\n",
    "# 4. Verify Paths (Optional, prints relative paths for clarity)\n",
    "print(f\"--- Configuration ---\")\n",
    "print(f\"Base Model:   {base_model}\")\n",
    "print(f\"Data Folder:  {to_relative(data_folder)}\")\n",
    "print(f\"Adapter Path: {to_relative(adapter_path)}\")\n",
    "print(f\"Output Path:  {to_relative(fused_output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27cf0121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing MLX Fine-tuning (CLI Mode) ---\n",
      "--- Model: ./Qwen/Qwen2.5-7B-Instruct ---\n",
      "--- Data Folder: ./../data ---\n",
      "Generating configuration file at: ./../adapters/qwen_mlx/train_config.yaml\n",
      "Executing MLX CLI training...\n",
      "Loading configuration file ../adapters/qwen_mlx/train_config.yaml\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 19716.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.606% (46.137M/7615.617M)\n",
      "Starting training..., iters: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:52,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 1.130, Val took 172.328s\n",
      "Iter 20: Train loss 1.887, Learning Rate 9.992e-06, It/sec 0.493, Tokens/sec 19.884, Trained Tokens 807, Peak mem 18.259 GB\n",
      "Iter 40: Train loss 0.718, Learning Rate 9.961e-06, It/sec 0.537, Tokens/sec 20.068, Trained Tokens 1555, Peak mem 18.259 GB\n",
      "Iter 60: Train loss 0.573, Learning Rate 9.905e-06, It/sec 0.483, Tokens/sec 23.745, Trained Tokens 2539, Peak mem 18.580 GB\n",
      "Iter 80: Train loss 0.374, Learning Rate 9.825e-06, It/sec 0.448, Tokens/sec 20.314, Trained Tokens 3446, Peak mem 18.580 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:52,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100: Val loss 0.332, Val took 172.217s\n",
      "Iter 100: Train loss 0.383, Learning Rate 9.722e-06, It/sec 0.456, Tokens/sec 27.125, Trained Tokens 4636, Peak mem 18.646 GB\n",
      "Iter 100: Saved adapter weights to ../adapters/qwen_mlx/adapters.safetensors and ../adapters/qwen_mlx/0000100_adapters.safetensors.\n",
      "Iter 120: Train loss 0.314, Learning Rate 9.596e-06, It/sec 0.415, Tokens/sec 27.205, Trained Tokens 5946, Peak mem 18.765 GB\n",
      "Iter 140: Train loss 0.265, Learning Rate 9.447e-06, It/sec 0.533, Tokens/sec 18.602, Trained Tokens 6644, Peak mem 18.765 GB\n",
      "Iter 160: Train loss 0.213, Learning Rate 9.277e-06, It/sec 0.454, Tokens/sec 21.604, Trained Tokens 7595, Peak mem 18.765 GB\n",
      "Iter 180: Train loss 0.481, Learning Rate 9.086e-06, It/sec 0.499, Tokens/sec 18.295, Trained Tokens 8329, Peak mem 18.765 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:51,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200: Val loss 0.280, Val took 171.977s\n",
      "Iter 200: Train loss 0.424, Learning Rate 8.875e-06, It/sec 0.442, Tokens/sec 22.986, Trained Tokens 9369, Peak mem 18.765 GB\n",
      "Iter 200: Saved adapter weights to ../adapters/qwen_mlx/adapters.safetensors and ../adapters/qwen_mlx/0000200_adapters.safetensors.\n",
      "Iter 220: Train loss 0.280, Learning Rate 8.645e-06, It/sec 0.537, Tokens/sec 16.820, Trained Tokens 9995, Peak mem 18.765 GB\n",
      "Iter 240: Train loss 0.447, Learning Rate 8.397e-06, It/sec 0.453, Tokens/sec 20.854, Trained Tokens 10916, Peak mem 18.765 GB\n",
      "Iter 260: Train loss 0.309, Learning Rate 8.133e-06, It/sec 0.417, Tokens/sec 34.605, Trained Tokens 12574, Peak mem 20.122 GB\n",
      "Iter 280: Train loss 0.321, Learning Rate 7.854e-06, It/sec 0.440, Tokens/sec 29.630, Trained Tokens 13921, Peak mem 20.122 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:52,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 300: Val loss 0.263, Val took 172.299s\n",
      "Iter 300: Train loss 0.408, Learning Rate 7.560e-06, It/sec 0.491, Tokens/sec 16.415, Trained Tokens 14589, Peak mem 20.122 GB\n",
      "Iter 300: Saved adapter weights to ../adapters/qwen_mlx/adapters.safetensors and ../adapters/qwen_mlx/0000300_adapters.safetensors.\n",
      "Iter 320: Train loss 0.383, Learning Rate 7.254e-06, It/sec 0.454, Tokens/sec 18.692, Trained Tokens 15412, Peak mem 20.122 GB\n",
      "Iter 340: Train loss 0.380, Learning Rate 6.938e-06, It/sec 0.449, Tokens/sec 16.015, Trained Tokens 16125, Peak mem 20.122 GB\n",
      "Iter 360: Train loss 0.378, Learning Rate 6.611e-06, It/sec 0.505, Tokens/sec 32.040, Trained Tokens 17395, Peak mem 20.122 GB\n",
      "Iter 380: Train loss 0.350, Learning Rate 6.277e-06, It/sec 0.478, Tokens/sec 17.835, Trained Tokens 18141, Peak mem 20.122 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:52,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 400: Val loss 0.254, Val took 172.156s\n",
      "Iter 400: Train loss 0.357, Learning Rate 5.937e-06, It/sec 0.511, Tokens/sec 23.605, Trained Tokens 19064, Peak mem 20.122 GB\n",
      "Iter 400: Saved adapter weights to ../adapters/qwen_mlx/adapters.safetensors and ../adapters/qwen_mlx/0000400_adapters.safetensors.\n",
      "Iter 420: Train loss 0.242, Learning Rate 5.592e-06, It/sec 0.516, Tokens/sec 17.856, Trained Tokens 19756, Peak mem 20.122 GB\n",
      "Iter 440: Train loss 0.383, Learning Rate 5.244e-06, It/sec 0.425, Tokens/sec 13.786, Trained Tokens 20404, Peak mem 20.122 GB\n",
      "Iter 460: Train loss 0.323, Learning Rate 4.895e-06, It/sec 0.519, Tokens/sec 34.946, Trained Tokens 21751, Peak mem 20.122 GB\n",
      "Iter 480: Train loss 0.158, Learning Rate 4.547e-06, It/sec 0.502, Tokens/sec 23.027, Trained Tokens 22668, Peak mem 20.122 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:52,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500: Val loss 0.248, Val took 172.209s\n",
      "Iter 500: Train loss 0.216, Learning Rate 4.201e-06, It/sec 0.464, Tokens/sec 13.982, Trained Tokens 23271, Peak mem 20.122 GB\n",
      "Iter 500: Saved adapter weights to ../adapters/qwen_mlx/adapters.safetensors and ../adapters/qwen_mlx/0000500_adapters.safetensors.\n",
      "Iter 520: Train loss 0.265, Learning Rate 3.858e-06, It/sec 0.493, Tokens/sec 36.670, Trained Tokens 24759, Peak mem 20.122 GB\n",
      "Iter 540: Train loss 0.198, Learning Rate 3.521e-06, It/sec 0.486, Tokens/sec 18.578, Trained Tokens 25523, Peak mem 20.122 GB\n",
      "Iter 560: Train loss 0.330, Learning Rate 3.192e-06, It/sec 0.516, Tokens/sec 29.353, Trained Tokens 26660, Peak mem 20.122 GB\n",
      "Iter 580: Train loss 0.122, Learning Rate 2.871e-06, It/sec 0.488, Tokens/sec 18.827, Trained Tokens 27432, Peak mem 20.122 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:52,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 600: Val loss 0.247, Val took 172.046s\n",
      "Iter 600: Train loss 0.217, Learning Rate 2.561e-06, It/sec 0.509, Tokens/sec 18.515, Trained Tokens 28159, Peak mem 20.122 GB\n",
      "Iter 600: Saved adapter weights to ../adapters/qwen_mlx/adapters.safetensors and ../adapters/qwen_mlx/0000600_adapters.safetensors.\n",
      "Iter 620: Train loss 0.256, Learning Rate 2.262e-06, It/sec 0.425, Tokens/sec 12.823, Trained Tokens 28762, Peak mem 20.122 GB\n",
      "Iter 640: Train loss 0.210, Learning Rate 1.977e-06, It/sec 0.430, Tokens/sec 36.538, Trained Tokens 30461, Peak mem 20.142 GB\n",
      "Iter 660: Train loss 0.207, Learning Rate 1.707e-06, It/sec 0.486, Tokens/sec 24.494, Trained Tokens 31470, Peak mem 20.142 GB\n",
      "Iter 680: Train loss 0.282, Learning Rate 1.452e-06, It/sec 0.431, Tokens/sec 21.441, Trained Tokens 32465, Peak mem 20.142 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:52,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 700: Val loss 0.247, Val took 172.472s\n",
      "Iter 700: Train loss 0.201, Learning Rate 1.215e-06, It/sec 0.483, Tokens/sec 15.480, Trained Tokens 33106, Peak mem 20.142 GB\n",
      "Iter 700: Saved adapter weights to ../adapters/qwen_mlx/adapters.safetensors and ../adapters/qwen_mlx/0000700_adapters.safetensors.\n",
      "Iter 720: Train loss 0.298, Learning Rate 9.963e-07, It/sec 0.569, Tokens/sec 23.201, Trained Tokens 33921, Peak mem 20.142 GB\n",
      "Iter 740: Train loss 0.156, Learning Rate 7.972e-07, It/sec 0.505, Tokens/sec 19.419, Trained Tokens 34690, Peak mem 20.142 GB\n",
      "Iter 760: Train loss 0.223, Learning Rate 6.185e-07, It/sec 0.448, Tokens/sec 21.483, Trained Tokens 35650, Peak mem 20.142 GB\n",
      "Iter 780: Train loss 0.277, Learning Rate 4.611e-07, It/sec 0.395, Tokens/sec 33.973, Trained Tokens 37372, Peak mem 20.142 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:52,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 800: Val loss 0.247, Val took 172.274s\n",
      "Iter 800: Train loss 0.200, Learning Rate 3.259e-07, It/sec 0.544, Tokens/sec 15.354, Trained Tokens 37937, Peak mem 20.142 GB\n",
      "Iter 800: Saved adapter weights to ../adapters/qwen_mlx/adapters.safetensors and ../adapters/qwen_mlx/0000800_adapters.safetensors.\n",
      "Iter 820: Train loss 0.209, Learning Rate 2.134e-07, It/sec 0.506, Tokens/sec 29.729, Trained Tokens 39111, Peak mem 20.142 GB\n",
      "Iter 840: Train loss 0.196, Learning Rate 1.243e-07, It/sec 0.540, Tokens/sec 22.330, Trained Tokens 39938, Peak mem 20.142 GB\n",
      "Iter 860: Train loss 0.191, Learning Rate 5.886e-08, It/sec 0.430, Tokens/sec 20.160, Trained Tokens 40876, Peak mem 20.142 GB\n",
      "Iter 880: Train loss 0.237, Learning Rate 1.754e-08, It/sec 0.465, Tokens/sec 19.865, Trained Tokens 41730, Peak mem 20.142 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:52,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 900: Val loss 0.247, Val took 172.465s\n",
      "Iter 900: Train loss 0.259, Learning Rate 4.870e-10, It/sec 0.434, Tokens/sec 20.754, Trained Tokens 42686, Peak mem 20.142 GB\n",
      "Iter 900: Saved adapter weights to ../adapters/qwen_mlx/adapters.safetensors and ../adapters/qwen_mlx/0000900_adapters.safetensors.\n",
      "Saved final weights to ../adapters/qwen_mlx/adapters.safetensors.\n",
      "✓ Training completed.\n"
     ]
    }
   ],
   "source": [
    "run_mlx_training(base_model, data_folder, adapter_path, iters=900, num_layers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d4f114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing LoRA adapters (Step: Final)...\n",
      "--- Staging checkpoint from: adapters.safetensors ---\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 13997.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Fused model saved to: ./local_models/ft_qwen_mlx\n",
      "Success! Model ready at ./local_models/ft_qwen_mlx\n"
     ]
    }
   ],
   "source": [
    "fuse_model(base_model, adapter_path, fused_output)\n",
    "print(f\"Success! Model ready at ./{to_relative(fused_output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a7a222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration ---\n",
      "Base Model:   meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Data Folder:  Finetuning/data\n",
      "Adapter Path: Finetuning/adapters/llama_mlx\n",
      "Output Path:  local_models/ft_llama_mlx\n"
     ]
    }
   ],
   "source": [
    "# 2. Config\n",
    "MODEL_TYPE = \"llama\" \n",
    "FRAMEWORK = \"mlx\"\n",
    "\n",
    "# 3. Define Paths using the new Utils\n",
    "# This guarantees they match the structure used by the data converter\n",
    "base_model = get_model_path(MODEL_TYPE)\n",
    "data_folder = get_processed_data_path()                   # .../Finetuning/data\n",
    "adapter_path = get_adapters_path(MODEL_TYPE, FRAMEWORK)   # .../Finetuning/adapters/qwen_mlx\n",
    "fused_output = get_output_dir(MODEL_TYPE, FRAMEWORK)      # .../local_models/ft_qwen_mlx\n",
    "\n",
    "# 4. Verify Paths (Optional, prints relative paths for clarity)\n",
    "print(f\"--- Configuration ---\")\n",
    "print(f\"Base Model:   {base_model}\")\n",
    "print(f\"Data Folder:  {to_relative(data_folder)}\")\n",
    "print(f\"Adapter Path: {to_relative(adapter_path)}\")\n",
    "print(f\"Output Path:  {to_relative(fused_output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f8d8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing MLX Fine-tuning (CLI Mode) ---\n",
      "--- Model: ./meta-llama/Meta-Llama-3-8B-Instruct ---\n",
      "--- Data Folder: ./../data ---\n",
      "Generating configuration file at: ./../adapters/llama_mlx/train_config.yaml\n",
      "Executing MLX CLI training...\n",
      "Loading configuration file ../adapters/llama_mlx/train_config.yaml\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 12179.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.131% (10.486M/8030.261M)\n",
      "Starting training..., iters: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:49,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 1.660, Val took 169.101s\n",
      "Iter 20: Train loss 1.688, Learning Rate 9.992e-06, It/sec 0.530, Tokens/sec 20.553, Trained Tokens 775, Peak mem 18.094 GB\n",
      "Iter 40: Train loss 0.574, Learning Rate 9.961e-06, It/sec 0.592, Tokens/sec 21.279, Trained Tokens 1494, Peak mem 18.094 GB\n",
      "Iter 60: Train loss 0.590, Learning Rate 9.905e-06, It/sec 0.529, Tokens/sec 24.588, Trained Tokens 2423, Peak mem 18.233 GB\n",
      "Iter 80: Train loss 0.353, Learning Rate 9.825e-06, It/sec 0.480, Tokens/sec 21.180, Trained Tokens 3305, Peak mem 18.341 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:48,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100: Val loss 0.381, Val took 168.839s\n",
      "Iter 100: Train loss 0.428, Learning Rate 9.722e-06, It/sec 0.491, Tokens/sec 28.747, Trained Tokens 4475, Peak mem 18.341 GB\n",
      "Iter 100: Saved adapter weights to ../adapters/llama_mlx/adapters.safetensors and ../adapters/llama_mlx/0000100_adapters.safetensors.\n",
      "Iter 120: Train loss 0.316, Learning Rate 9.596e-06, It/sec 0.454, Tokens/sec 28.555, Trained Tokens 5734, Peak mem 18.341 GB\n",
      "Iter 140: Train loss 0.336, Learning Rate 9.447e-06, It/sec 0.593, Tokens/sec 19.907, Trained Tokens 6405, Peak mem 18.341 GB\n",
      "Iter 160: Train loss 0.249, Learning Rate 9.277e-06, It/sec 0.497, Tokens/sec 22.827, Trained Tokens 7323, Peak mem 18.341 GB\n",
      "Iter 180: Train loss 0.457, Learning Rate 9.086e-06, It/sec 0.539, Tokens/sec 19.222, Trained Tokens 8036, Peak mem 18.341 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:48,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200: Val loss 0.348, Val took 168.877s\n",
      "Iter 200: Train loss 0.436, Learning Rate 8.875e-06, It/sec 0.475, Tokens/sec 24.186, Trained Tokens 9055, Peak mem 18.341 GB\n",
      "Iter 200: Saved adapter weights to ../adapters/llama_mlx/adapters.safetensors and ../adapters/llama_mlx/0000200_adapters.safetensors.\n",
      "Iter 220: Train loss 0.353, Learning Rate 8.645e-06, It/sec 0.584, Tokens/sec 17.574, Trained Tokens 9657, Peak mem 18.341 GB\n",
      "Iter 240: Train loss 0.463, Learning Rate 8.397e-06, It/sec 0.485, Tokens/sec 21.677, Trained Tokens 10551, Peak mem 18.341 GB\n",
      "Iter 260: Train loss 0.294, Learning Rate 8.133e-06, It/sec 0.444, Tokens/sec 36.130, Trained Tokens 12180, Peak mem 19.506 GB\n",
      "Iter 280: Train loss 0.421, Learning Rate 7.854e-06, It/sec 0.472, Tokens/sec 30.320, Trained Tokens 13465, Peak mem 19.506 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:48,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 300: Val loss 0.329, Val took 168.871s\n",
      "Iter 300: Train loss 0.488, Learning Rate 7.560e-06, It/sec 0.537, Tokens/sec 17.338, Trained Tokens 14111, Peak mem 19.506 GB\n",
      "Iter 300: Saved adapter weights to ../adapters/llama_mlx/adapters.safetensors and ../adapters/llama_mlx/0000300_adapters.safetensors.\n",
      "Iter 320: Train loss 0.408, Learning Rate 7.254e-06, It/sec 0.495, Tokens/sec 19.581, Trained Tokens 14902, Peak mem 19.506 GB\n",
      "Iter 340: Train loss 0.468, Learning Rate 6.938e-06, It/sec 0.486, Tokens/sec 16.520, Trained Tokens 15582, Peak mem 19.506 GB\n",
      "Iter 360: Train loss 0.469, Learning Rate 6.611e-06, It/sec 0.556, Tokens/sec 34.535, Trained Tokens 16824, Peak mem 19.506 GB\n",
      "Iter 380: Train loss 0.392, Learning Rate 6.277e-06, It/sec 0.527, Tokens/sec 18.826, Trained Tokens 17538, Peak mem 19.506 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:48,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 400: Val loss 0.323, Val took 168.861s\n",
      "Iter 400: Train loss 0.378, Learning Rate 5.937e-06, It/sec 0.560, Tokens/sec 24.956, Trained Tokens 18429, Peak mem 19.506 GB\n",
      "Iter 400: Saved adapter weights to ../adapters/llama_mlx/adapters.safetensors and ../adapters/llama_mlx/0000400_adapters.safetensors.\n",
      "Iter 420: Train loss 0.307, Learning Rate 5.592e-06, It/sec 0.560, Tokens/sec 18.476, Trained Tokens 19089, Peak mem 19.506 GB\n",
      "Iter 440: Train loss 0.438, Learning Rate 5.244e-06, It/sec 0.455, Tokens/sec 14.256, Trained Tokens 19716, Peak mem 19.506 GB\n",
      "Iter 460: Train loss 0.345, Learning Rate 4.895e-06, It/sec 0.567, Tokens/sec 37.430, Trained Tokens 21037, Peak mem 19.506 GB\n",
      "Iter 480: Train loss 0.125, Learning Rate 4.547e-06, It/sec 0.553, Tokens/sec 24.619, Trained Tokens 21928, Peak mem 19.506 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:48,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500: Val loss 0.317, Val took 168.895s\n",
      "Iter 500: Train loss 0.156, Learning Rate 4.201e-06, It/sec 0.498, Tokens/sec 14.526, Trained Tokens 22511, Peak mem 19.506 GB\n",
      "Iter 500: Saved adapter weights to ../adapters/llama_mlx/adapters.safetensors and ../adapters/llama_mlx/0000500_adapters.safetensors.\n",
      "Iter 520: Train loss 0.274, Learning Rate 3.858e-06, It/sec 0.533, Tokens/sec 38.774, Trained Tokens 23967, Peak mem 19.506 GB\n",
      "Iter 540: Train loss 0.211, Learning Rate 3.521e-06, It/sec 0.523, Tokens/sec 19.372, Trained Tokens 24708, Peak mem 19.506 GB\n",
      "Iter 560: Train loss 0.316, Learning Rate 3.192e-06, It/sec 0.561, Tokens/sec 31.003, Trained Tokens 25814, Peak mem 19.506 GB\n",
      "Iter 580: Train loss 0.108, Learning Rate 2.871e-06, It/sec 0.536, Tokens/sec 19.025, Trained Tokens 26524, Peak mem 19.506 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:48,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 600: Val loss 0.317, Val took 168.907s\n",
      "Iter 600: Train loss 0.189, Learning Rate 2.561e-06, It/sec 0.566, Tokens/sec 19.683, Trained Tokens 27220, Peak mem 19.506 GB\n",
      "Iter 600: Saved adapter weights to ../adapters/llama_mlx/adapters.safetensors and ../adapters/llama_mlx/0000600_adapters.safetensors.\n",
      "Iter 620: Train loss 0.258, Learning Rate 2.262e-06, It/sec 0.457, Tokens/sec 13.217, Trained Tokens 27799, Peak mem 19.506 GB\n",
      "Iter 640: Train loss 0.243, Learning Rate 1.977e-06, It/sec 0.458, Tokens/sec 38.320, Trained Tokens 29471, Peak mem 19.506 GB\n",
      "Iter 660: Train loss 0.247, Learning Rate 1.707e-06, It/sec 0.526, Tokens/sec 25.902, Trained Tokens 30455, Peak mem 19.506 GB\n",
      "Iter 680: Train loss 0.240, Learning Rate 1.452e-06, It/sec 0.460, Tokens/sec 22.003, Trained Tokens 31412, Peak mem 19.506 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:48,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 700: Val loss 0.318, Val took 168.868s\n",
      "Iter 700: Train loss 0.232, Learning Rate 1.215e-06, It/sec 0.531, Tokens/sec 16.308, Trained Tokens 32026, Peak mem 19.506 GB\n",
      "Iter 700: Saved adapter weights to ../adapters/llama_mlx/adapters.safetensors and ../adapters/llama_mlx/0000700_adapters.safetensors.\n",
      "Iter 720: Train loss 0.282, Learning Rate 9.963e-07, It/sec 0.625, Tokens/sec 24.781, Trained Tokens 32819, Peak mem 19.506 GB\n",
      "Iter 740: Train loss 0.165, Learning Rate 7.972e-07, It/sec 0.556, Tokens/sec 20.514, Trained Tokens 33557, Peak mem 19.506 GB\n",
      "Iter 760: Train loss 0.189, Learning Rate 6.185e-07, It/sec 0.489, Tokens/sec 22.657, Trained Tokens 34484, Peak mem 19.506 GB\n",
      "Iter 780: Train loss 0.298, Learning Rate 4.611e-07, It/sec 0.419, Tokens/sec 35.351, Trained Tokens 36173, Peak mem 19.506 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:48,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 800: Val loss 0.318, Val took 168.891s\n",
      "Iter 800: Train loss 0.216, Learning Rate 3.259e-07, It/sec 0.597, Tokens/sec 16.186, Trained Tokens 36715, Peak mem 19.506 GB\n",
      "Iter 800: Saved adapter weights to ../adapters/llama_mlx/adapters.safetensors and ../adapters/llama_mlx/0000800_adapters.safetensors.\n",
      "Iter 820: Train loss 0.208, Learning Rate 2.134e-07, It/sec 0.552, Tokens/sec 30.560, Trained Tokens 37822, Peak mem 19.506 GB\n",
      "Iter 840: Train loss 0.146, Learning Rate 1.243e-07, It/sec 0.584, Tokens/sec 23.104, Trained Tokens 38613, Peak mem 19.506 GB\n",
      "Iter 860: Train loss 0.185, Learning Rate 5.886e-08, It/sec 0.461, Tokens/sec 20.916, Trained Tokens 39521, Peak mem 19.506 GB\n",
      "Iter 880: Train loss 0.212, Learning Rate 1.754e-08, It/sec 0.500, Tokens/sec 20.799, Trained Tokens 40353, Peak mem 19.506 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 166it [02:48,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 900: Val loss 0.318, Val took 168.943s\n",
      "Iter 900: Train loss 0.290, Learning Rate 4.870e-10, It/sec 0.466, Tokens/sec 21.722, Trained Tokens 41286, Peak mem 19.506 GB\n",
      "Iter 900: Saved adapter weights to ../adapters/llama_mlx/adapters.safetensors and ../adapters/llama_mlx/0000900_adapters.safetensors.\n",
      "Saved final weights to ../adapters/llama_mlx/adapters.safetensors.\n",
      "✓ Training completed.\n"
     ]
    }
   ],
   "source": [
    "run_mlx_training(base_model, data_folder, adapter_path, iters=900, num_layers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0ef930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing LoRA adapters (Step: 300)...\n",
      "--- Staging checkpoint from: 0000300_adapters.safetensors ---\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 19574.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Fused model saved to: ./local_models/ft_llama_mlx\n",
      "Success! Model ready at ./local_models/ft_llama_mlx\n"
     ]
    }
   ],
   "source": [
    "fuse_model(base_model, adapter_path, fused_output, 300)\n",
    "print(f\"Success! Model ready at ./{to_relative(fused_output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435f508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schasplm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
