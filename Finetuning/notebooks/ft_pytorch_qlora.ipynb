{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a344cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing PyTorch Training ---\n",
      "--- Device: MPS ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f36a91daa4b4f65997db99c4b498280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/Users/arjandeweerd/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/local_models/Qwen/Qwen2.5-7B-Instruct-bfloat16' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91220f9fa3a243578ce6ef383e3d556f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67c8b7d44b040b9971ad1314629b8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b166849ccdbd4faf940afe9f7ad2c851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5477db71ed2e4e70956dd70c493516cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 1:02:35, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.882234</td>\n",
       "      <td>1.475956</td>\n",
       "      <td>1.227683</td>\n",
       "      <td>30238.000000</td>\n",
       "      <td>0.693741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.543730</td>\n",
       "      <td>1.255234</td>\n",
       "      <td>1.273990</td>\n",
       "      <td>60476.000000</td>\n",
       "      <td>0.721265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.390804</td>\n",
       "      <td>1.119386</td>\n",
       "      <td>1.195052</td>\n",
       "      <td>90714.000000</td>\n",
       "      <td>0.745829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.292870</td>\n",
       "      <td>1.020619</td>\n",
       "      <td>1.043580</td>\n",
       "      <td>120952.000000</td>\n",
       "      <td>0.766574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.050759</td>\n",
       "      <td>0.956215</td>\n",
       "      <td>0.911583</td>\n",
       "      <td>151190.000000</td>\n",
       "      <td>0.777007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.007475</td>\n",
       "      <td>0.919510</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>181428.000000</td>\n",
       "      <td>0.785566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.921547</td>\n",
       "      <td>0.897240</td>\n",
       "      <td>0.811306</td>\n",
       "      <td>211666.000000</td>\n",
       "      <td>0.789437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.939486</td>\n",
       "      <td>0.885992</td>\n",
       "      <td>0.799605</td>\n",
       "      <td>241904.000000</td>\n",
       "      <td>0.792611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.863553</td>\n",
       "      <td>0.877846</td>\n",
       "      <td>0.796375</td>\n",
       "      <td>272142.000000</td>\n",
       "      <td>0.793922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.869738</td>\n",
       "      <td>0.877709</td>\n",
       "      <td>0.795267</td>\n",
       "      <td>302380.000000</td>\n",
       "      <td>0.792611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving merged model to: ./local_models/ft_qwen_pytorch\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from utils_ft import get_model_path, get_output_dir, setup_env, get_root_path\n",
    "from scripts.train_pytorch import train_pytorch\n",
    "\n",
    "# 1. Configuration\n",
    "# Toggle between \"llama\" and \"qwen\" as needed\n",
    "#MODEL_TYPE = \"llama\" \n",
    "MODEL_TYPE = \"qwen\"\n",
    "\n",
    "hf_token = setup_env()\n",
    "model_id = get_model_path(MODEL_TYPE)\n",
    "\n",
    "# 2. Path Setup\n",
    "# We point to the FOLDER 'data', which now contains train.jsonl and valid.jsonl\n",
    "dataset_dir = os.path.join(get_root_path(), \"Finetuning\", \"data\")\n",
    "output_dir = get_output_dir(MODEL_TYPE, \"pytorch\")\n",
    "\n",
    "# 3. Execution\n",
    "# Ensure the script receives the directory path, not a specific file\n",
    "train_pytorch(\n",
    "    model_id=model_id, \n",
    "    dataset_dir=dataset_dir, \n",
    "    output_dir=output_dir, \n",
    "    hf_token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d5bf39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     38\u001b[39m     plt.show()\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Run the visual inspection\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# For MLX, point to the adapter folder. For PyTorch, point to the output folder.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mplot_training_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mplot_training_results\u001b[39m\u001b[34m(log_dir)\u001b[39m\n\u001b[32m     22\u001b[39m train_df = df[df[train_col].notna()]\n\u001b[32m     23\u001b[39m val_df = df[df[val_col].notna()]\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m train_df.empty:\n\u001b[32m     28\u001b[39m     plt.plot(train_df[iter_col], train_df[train_col], label=\u001b[33m'\u001b[39m\u001b[33mTraining Loss\u001b[39m\u001b[33m'\u001b[39m, alpha=\u001b[32m0.7\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "def plot_training_results(log_dir):\n",
    "    report_path = os.path.join(log_dir, \"report.json\")\n",
    "    \n",
    "    if not os.path.exists(report_path):\n",
    "        print(f\"Error: report.json not found in ./{to_relative(log_dir)}\")\n",
    "        return\n",
    "\n",
    "    with open(report_path, \"r\") as f:\n",
    "        log_data = json.load(f)\n",
    "\n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(log_data)\n",
    "\n",
    "    # Detect format: MLX uses 'train_loss', HF uses 'loss'\n",
    "    train_col = 'train_loss' if 'train_loss' in df.columns else 'loss'\n",
    "    val_col = 'val_loss' if 'val_loss' in df.columns else 'eval_loss'\n",
    "    iter_col = 'iteration' if 'iteration' in df.columns else 'step'\n",
    "\n",
    "    # Filter out empty rows for specific metrics\n",
    "    train_df = df[df[train_col].notna()]\n",
    "    val_df = df[df[val_col].notna()]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    if not train_df.empty:\n",
    "        plt.plot(train_df[iter_col], train_df[train_col], label='Training Loss', alpha=0.7)\n",
    "    \n",
    "    if not val_df.empty:\n",
    "        plt.plot(val_df[iter_col], val_df[val_col], label='Validation Loss', color='red', marker='x')\n",
    "\n",
    "    plt.title('ASP Autoformalization Fine-tuning Progress')\n",
    "    plt.xlabel('Iterations / Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visual inspection\n",
    "# For MLX, point to the adapter folder. For PyTorch, point to the output folder.\n",
    "plot_training_results(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ed4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schasplm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
