CUDA not detected. Using MPS/CPU configuration (Quantization disabled).
Using Huggingface Hub Token from .env
Using dtype: torch.bfloat16
loading local model and tokenizer...
Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]Loading weights:   0%|          | 1/339 [00:00<00:00, 6364.65it/s, Materializing param=lm_head.weight]Loading weights:   0%|          | 1/339 [00:00<00:00, 4854.52it/s, Materializing param=lm_head.weight]Loading weights:   1%|          | 2/339 [00:01<03:13,  1.74it/s, Materializing param=lm_head.weight]  Loading weights:   1%|          | 2/339 [00:01<03:13,  1.74it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 2/339 [00:01<03:13,  1.74it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 3/339 [00:01<03:13,  1.74it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 3/339 [00:01<03:13,  1.74it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 4/339 [00:01<03:12,  1.74it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  Loading weights:   1%|          | 4/339 [00:01<03:12,  1.74it/s, Materializing param=model.layers.0.mlp.down_proj.weight]Loading weights:   1%|▏         | 5/339 [00:01<03:11,  1.74it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   1%|▏         | 5/339 [00:01<03:11,  1.74it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|▏         | 6/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|▏         | 6/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  Loading weights:   2%|▏         | 6/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   2%|▏         | 7/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 7/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 8/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          Loading weights:   2%|▏         | 8/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]Loading weights:   3%|▎         | 9/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 9/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 10/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 10/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 11/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]  Loading weights:   3%|▎         | 11/339 [00:01<00:55,  5.95it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]Loading weights:   4%|▎         | 12/339 [00:01<00:54,  5.95it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▎         | 12/339 [00:01<00:54,  5.95it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▍         | 13/339 [00:01<00:54,  5.95it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  Loading weights:   4%|▍         | 13/339 [00:01<00:54,  5.95it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]Loading weights:   4%|▍         | 14/339 [00:01<00:54,  5.95it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 14/339 [00:01<00:54,  5.95it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 15/339 [00:01<00:54,  5.95it/s, Materializing param=model.layers.1.input_layernorm.weight] Loading weights:   4%|▍         | 15/339 [00:01<00:54,  5.95it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   5%|▍         | 16/339 [00:01<00:54,  5.95it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  Loading weights:   5%|▍         | 16/339 [00:01<00:54,  5.95it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   5%|▌         | 17/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   5%|▌         | 17/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 17/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 18/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  Loading weights:   5%|▌         | 18/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.mlp.up_proj.weight]Loading weights:   6%|▌         | 19/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 19/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 20/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          Loading weights:   6%|▌         | 20/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]Loading weights:   6%|▌         | 21/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▌         | 21/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▋         | 22/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   6%|▋         | 22/339 [00:01<00:16, 19.34it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   7%|▋         | 23/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   7%|▋         | 23/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  Loading weights:   7%|▋         | 23/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]Loading weights:   7%|▋         | 24/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 24/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 25/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  Loading weights:   7%|▋         | 25/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]Loading weights:   8%|▊         | 26/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 26/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 27/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.2.input_layernorm.weight] Loading weights:   8%|▊         | 27/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:   8%|▊         | 28/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  Loading weights:   8%|▊         | 28/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:   9%|▊         | 29/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▊         | 29/339 [00:01<00:12, 25.06it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▉         | 30/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▉         | 30/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  Loading weights:   9%|▉         | 30/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.mlp.up_proj.weight]Loading weights:   9%|▉         | 31/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 31/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 32/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          Loading weights:   9%|▉         | 32/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]Loading weights:  10%|▉         | 33/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|▉         | 33/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|█         | 34/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 34/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 35/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  Loading weights:  10%|█         | 35/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]Loading weights:  11%|█         | 36/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 36/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 37/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  Loading weights:  11%|█         | 37/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]Loading weights:  11%|█         | 38/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  11%|█         | 38/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  12%|█▏        | 39/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.3.input_layernorm.weight] Loading weights:  12%|█▏        | 39/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  12%|█▏        | 40/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  Loading weights:  12%|█▏        | 40/339 [00:01<00:09, 32.05it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 42/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  Loading weights:  12%|█▏        | 42/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.mlp.up_proj.weight]Loading weights:  13%|█▎        | 43/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 43/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 44/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          Loading weights:  13%|█▎        | 44/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]Loading weights:  13%|█▎        | 45/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  13%|█▎        | 45/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▍        | 47/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  Loading weights:  14%|█▍        | 47/339 [00:01<00:06, 44.67it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]Loading weights:  14%|█▍        | 48/339 [00:01<00:06, 48.15it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]Loading weights:  14%|█▍        | 48/339 [00:01<00:06, 48.15it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 48/339 [00:01<00:06, 48.15it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 49/339 [00:01<00:06, 48.15it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  Loading weights:  14%|█▍        | 49/339 [00:01<00:06, 48.15it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]Loading weights:  15%|█▍        | 50/339 [00:01<00:06, 48.15it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▍        | 50/339 [00:01<00:06, 48.15it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▌        | 51/339 [00:01<00:05, 48.15it/s, Materializing param=model.layers.4.input_layernorm.weight] Loading weights:  15%|█▌        | 51/339 [00:01<00:05, 48.15it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  15%|█▌        | 52/339 [00:01<00:05, 48.15it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  Loading weights:  15%|█▌        | 52/339 [00:01<00:05, 48.15it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:01<00:05, 48.15it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:01<00:05, 48.15it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 54/339 [00:01<00:05, 48.15it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  Loading weights:  16%|█▌        | 54/339 [00:01<00:05, 48.15it/s, Materializing param=model.layers.4.mlp.up_proj.weight]Loading weights:  16%|█▌        | 55/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.mlp.up_proj.weight]Loading weights:  16%|█▌        | 55/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  16%|█▌        | 55/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  17%|█▋        | 56/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          Loading weights:  17%|█▋        | 56/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]Loading weights:  17%|█▋        | 57/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 57/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 59/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  Loading weights:  17%|█▋        | 59/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]Loading weights:  18%|█▊        | 60/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 60/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 61/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  Loading weights:  18%|█▊        | 61/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]Loading weights:  18%|█▊        | 62/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  18%|█▊        | 62/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  19%|█▊        | 63/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.5.input_layernorm.weight] Loading weights:  19%|█▊        | 63/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  19%|█▉        | 64/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  Loading weights:  19%|█▉        | 64/339 [00:02<00:06, 45.67it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:02<00:05, 45.67it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:02<00:05, 45.67it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 66/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 66/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  Loading weights:  19%|█▉        | 66/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.mlp.up_proj.weight]Loading weights:  20%|█▉        | 67/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|█▉        | 67/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|██        | 68/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          Loading weights:  20%|██        | 68/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]Loading weights:  20%|██        | 69/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  20%|██        | 69/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  21%|██        | 70/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 70/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 71/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  Loading weights:  21%|██        | 71/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]Loading weights:  21%|██        | 72/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  21%|██        | 72/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  22%|██▏       | 73/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  Loading weights:  22%|██▏       | 73/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]Loading weights:  22%|██▏       | 74/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 74/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 75/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.6.input_layernorm.weight] Loading weights:  22%|██▏       | 75/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  22%|██▏       | 76/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  Loading weights:  22%|██▏       | 76/339 [00:02<00:04, 55.63it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 78/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  Loading weights:  23%|██▎       | 78/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.mlp.up_proj.weight]Loading weights:  23%|██▎       | 79/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  23%|██▎       | 79/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  24%|██▎       | 80/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          Loading weights:  24%|██▎       | 80/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]Loading weights:  24%|██▍       | 81/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 81/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 83/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  Loading weights:  24%|██▍       | 83/339 [00:02<00:04, 62.78it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]Loading weights:  25%|██▍       | 84/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]Loading weights:  25%|██▍       | 84/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▍       | 84/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▌       | 85/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  Loading weights:  25%|██▌       | 85/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]Loading weights:  25%|██▌       | 86/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  25%|██▌       | 86/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  26%|██▌       | 87/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.7.input_layernorm.weight] Loading weights:  26%|██▌       | 87/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  26%|██▌       | 88/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  Loading weights:  26%|██▌       | 88/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  27%|██▋       | 90/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  Loading weights:  27%|██▋       | 90/339 [00:02<00:04, 61.60it/s, Materializing param=model.layers.7.mlp.up_proj.weight]Loading weights:  27%|██▋       | 91/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.mlp.up_proj.weight]Loading weights:  27%|██▋       | 91/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 91/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 92/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          Loading weights:  27%|██▋       | 92/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]Loading weights:  27%|██▋       | 93/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  27%|██▋       | 93/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 95/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  Loading weights:  28%|██▊       | 95/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]Loading weights:  28%|██▊       | 96/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  28%|██▊       | 96/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  29%|██▊       | 97/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  Loading weights:  29%|██▊       | 97/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]Loading weights:  29%|██▉       | 98/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 98/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 99/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.8.input_layernorm.weight] Loading weights:  29%|██▉       | 99/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  29%|██▉       | 100/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.8.mlp.down_proj.weight] Loading weights:  29%|██▉       | 100/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:02<00:04, 53.78it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|███       | 102/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|███       | 102/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  Loading weights:  30%|███       | 102/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.mlp.up_proj.weight]Loading weights:  30%|███       | 103/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  30%|███       | 103/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  31%|███       | 104/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          Loading weights:  31%|███       | 104/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]Loading weights:  31%|███       | 105/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███       | 105/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  32%|███▏      | 107/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  Loading weights:  32%|███▏      | 107/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]Loading weights:  32%|███▏      | 108/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 108/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 109/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  Loading weights:  32%|███▏      | 109/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]Loading weights:  32%|███▏      | 110/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  32%|███▏      | 110/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  33%|███▎      | 111/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.9.input_layernorm.weight] Loading weights:  33%|███▎      | 111/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  33%|███▎      | 112/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  Loading weights:  33%|███▎      | 112/339 [00:02<00:03, 61.02it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  34%|███▎      | 114/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  Loading weights:  34%|███▎      | 114/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.mlp.up_proj.weight]Loading weights:  34%|███▍      | 115/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 115/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 116/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          Loading weights:  34%|███▍      | 116/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]Loading weights:  35%|███▍      | 117/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 117/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:02<00:03, 66.27it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▌      | 119/339 [00:03<00:03, 66.27it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  Loading weights:  35%|███▌      | 119/339 [00:03<00:03, 66.27it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]Loading weights:  35%|███▌      | 120/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]Loading weights:  35%|███▌      | 120/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  35%|███▌      | 120/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  36%|███▌      | 121/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  Loading weights:  36%|███▌      | 121/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]Loading weights:  36%|███▌      | 122/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▌      | 122/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▋      | 123/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  36%|███▋      | 123/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  37%|███▋      | 124/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  Loading weights:  37%|███▋      | 124/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 126/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  Loading weights:  37%|███▋      | 126/339 [00:03<00:03, 63.67it/s, Materializing param=model.layers.10.mlp.up_proj.weight]Loading weights:  37%|███▋      | 127/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.mlp.up_proj.weight]Loading weights:  37%|███▋      | 127/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  37%|███▋      | 127/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  38%|███▊      | 128/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          Loading weights:  38%|███▊      | 128/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]Loading weights:  38%|███▊      | 129/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 129/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  39%|███▊      | 131/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  Loading weights:  39%|███▊      | 131/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]Loading weights:  39%|███▉      | 132/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 132/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 133/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  Loading weights:  39%|███▉      | 133/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]Loading weights:  40%|███▉      | 134/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 134/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 135/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.11.input_layernorm.weight] Loading weights:  40%|███▉      | 135/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  40%|████      | 136/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  Loading weights:  40%|████      | 136/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  40%|████      | 137/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  40%|████      | 137/339 [00:03<00:03, 55.32it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  41%|████      | 138/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  41%|████      | 138/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  Loading weights:  41%|████      | 138/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.mlp.up_proj.weight]Loading weights:  41%|████      | 139/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████      | 139/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████▏     | 140/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          Loading weights:  41%|████▏     | 140/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]Loading weights:  42%|████▏     | 141/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 141/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 143/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  Loading weights:  42%|████▏     | 143/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]Loading weights:  42%|████▏     | 144/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  42%|████▏     | 144/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  43%|████▎     | 145/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  Loading weights:  43%|████▎     | 145/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]Loading weights:  43%|████▎     | 146/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 146/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 147/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.12.input_layernorm.weight] Loading weights:  43%|████▎     | 147/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  44%|████▎     | 148/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  Loading weights:  44%|████▎     | 148/339 [00:03<00:03, 59.65it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 150/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  Loading weights:  44%|████▍     | 150/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.mlp.up_proj.weight]Loading weights:  45%|████▍     | 151/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 151/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 152/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          Loading weights:  45%|████▍     | 152/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]Loading weights:  45%|████▌     | 153/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 153/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  46%|████▌     | 155/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  Loading weights:  46%|████▌     | 155/339 [00:03<00:02, 64.98it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]Loading weights:  46%|████▌     | 156/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]Loading weights:  46%|████▌     | 156/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▌     | 156/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▋     | 157/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  Loading weights:  46%|████▋     | 157/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]Loading weights:  47%|████▋     | 158/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 158/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 159/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.13.input_layernorm.weight] Loading weights:  47%|████▋     | 159/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  47%|████▋     | 160/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  Loading weights:  47%|████▋     | 160/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  48%|████▊     | 162/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  Loading weights:  48%|████▊     | 162/339 [00:03<00:02, 62.44it/s, Materializing param=model.layers.13.mlp.up_proj.weight]Loading weights:  48%|████▊     | 163/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.mlp.up_proj.weight]Loading weights:  48%|████▊     | 163/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 163/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 164/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          Loading weights:  48%|████▊     | 164/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]Loading weights:  49%|████▊     | 165/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▊     | 165/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 167/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  Loading weights:  49%|████▉     | 167/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]Loading weights:  50%|████▉     | 168/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 168/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 169/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  Loading weights:  50%|████▉     | 169/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]Loading weights:  50%|█████     | 170/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 170/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 171/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.14.input_layernorm.weight] Loading weights:  50%|█████     | 171/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  51%|█████     | 172/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  Loading weights:  51%|█████     | 172/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  51%|█████     | 173/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████     | 173/339 [00:03<00:03, 53.68it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████▏    | 174/339 [00:03<00:02, 60.61it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████▏    | 174/339 [00:03<00:02, 60.61it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  Loading weights:  51%|█████▏    | 174/339 [00:03<00:02, 60.61it/s, Materializing param=model.layers.14.mlp.up_proj.weight]Loading weights:  52%|█████▏    | 175/339 [00:03<00:02, 60.61it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 175/339 [00:03<00:02, 60.61it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 176/339 [00:03<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          Loading weights:  52%|█████▏    | 176/339 [00:03<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]Loading weights:  52%|█████▏    | 177/339 [00:03<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  52%|█████▏    | 177/339 [00:03<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:03<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:03<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 179/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  Loading weights:  53%|█████▎    | 179/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]Loading weights:  53%|█████▎    | 180/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 180/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 181/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  Loading weights:  53%|█████▎    | 181/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]Loading weights:  54%|█████▎    | 182/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▎    | 182/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▍    | 183/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.15.input_layernorm.weight] Loading weights:  54%|█████▍    | 183/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  54%|█████▍    | 184/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  Loading weights:  54%|█████▍    | 184/339 [00:04<00:02, 60.61it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 186/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  Loading weights:  55%|█████▍    | 186/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.mlp.up_proj.weight]Loading weights:  55%|█████▌    | 187/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 187/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 188/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          Loading weights:  55%|█████▌    | 188/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]Loading weights:  56%|█████▌    | 189/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 189/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▋    | 191/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  Loading weights:  56%|█████▋    | 191/339 [00:04<00:02, 65.58it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]Loading weights:  57%|█████▋    | 192/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]Loading weights:  57%|█████▋    | 192/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 192/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 193/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  Loading weights:  57%|█████▋    | 193/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]Loading weights:  57%|█████▋    | 194/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  57%|█████▋    | 194/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  58%|█████▊    | 195/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.16.input_layernorm.weight] Loading weights:  58%|█████▊    | 195/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  58%|█████▊    | 196/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  Loading weights:  58%|█████▊    | 196/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 198/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  Loading weights:  58%|█████▊    | 198/339 [00:04<00:02, 62.60it/s, Materializing param=model.layers.16.mlp.up_proj.weight]Loading weights:  59%|█████▊    | 199/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.mlp.up_proj.weight]Loading weights:  59%|█████▊    | 199/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▊    | 199/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▉    | 200/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          Loading weights:  59%|█████▉    | 200/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]Loading weights:  59%|█████▉    | 201/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  59%|█████▉    | 201/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 203/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  Loading weights:  60%|█████▉    | 203/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]Loading weights:  60%|██████    | 204/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 204/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 205/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  Loading weights:  60%|██████    | 205/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]Loading weights:  61%|██████    | 206/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 206/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 207/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.17.input_layernorm.weight] Loading weights:  61%|██████    | 207/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  61%|██████▏   | 208/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  Loading weights:  61%|██████▏   | 208/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:04<00:02, 53.35it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 210/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 210/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  Loading weights:  62%|██████▏   | 210/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.mlp.up_proj.weight]Loading weights:  62%|██████▏   | 211/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  62%|██████▏   | 211/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  63%|██████▎   | 212/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          Loading weights:  63%|██████▎   | 212/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]Loading weights:  63%|██████▎   | 213/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 213/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 215/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  Loading weights:  63%|██████▎   | 215/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]Loading weights:  64%|██████▎   | 216/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▎   | 216/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▍   | 217/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  Loading weights:  64%|██████▍   | 217/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]Loading weights:  64%|██████▍   | 218/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  64%|██████▍   | 218/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  65%|██████▍   | 219/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.18.input_layernorm.weight] Loading weights:  65%|██████▍   | 219/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.18.input_layernorm.weight]Loading weights:  65%|██████▍   | 220/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  Loading weights:  65%|██████▍   | 220/339 [00:04<00:02, 59.23it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 222/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  Loading weights:  65%|██████▌   | 222/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.mlp.up_proj.weight]Loading weights:  66%|██████▌   | 223/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 223/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 224/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          Loading weights:  66%|██████▌   | 224/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]Loading weights:  66%|██████▋   | 225/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  66%|██████▋   | 225/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 227/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  Loading weights:  67%|██████▋   | 227/339 [00:04<00:01, 63.59it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]Loading weights:  67%|██████▋   | 228/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]Loading weights:  67%|██████▋   | 228/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  67%|██████▋   | 228/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  68%|██████▊   | 229/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  Loading weights:  68%|██████▊   | 229/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]Loading weights:  68%|██████▊   | 230/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 230/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 231/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.19.input_layernorm.weight] Loading weights:  68%|██████▊   | 231/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  68%|██████▊   | 232/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  Loading weights:  68%|██████▊   | 232/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▉   | 234/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  Loading weights:  69%|██████▉   | 234/339 [00:04<00:01, 60.51it/s, Materializing param=model.layers.19.mlp.up_proj.weight]Loading weights:  69%|██████▉   | 235/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.mlp.up_proj.weight]Loading weights:  69%|██████▉   | 235/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  69%|██████▉   | 235/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  70%|██████▉   | 236/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          Loading weights:  70%|██████▉   | 236/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]Loading weights:  70%|██████▉   | 237/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|██████▉   | 237/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|███████   | 238/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  70%|███████   | 238/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  71%|███████   | 239/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  Loading weights:  71%|███████   | 239/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]Loading weights:  71%|███████   | 240/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 240/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 241/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  Loading weights:  71%|███████   | 241/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]Loading weights:  71%|███████▏  | 242/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  71%|███████▏  | 242/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  72%|███████▏  | 243/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.20.input_layernorm.weight] Loading weights:  72%|███████▏  | 243/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  72%|███████▏  | 244/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  Loading weights:  72%|███████▏  | 244/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:05<00:01, 52.76it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  73%|███████▎  | 246/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  73%|███████▎  | 246/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  Loading weights:  73%|███████▎  | 246/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.mlp.up_proj.weight]Loading weights:  73%|███████▎  | 247/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 247/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 248/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          Loading weights:  73%|███████▎  | 248/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]Loading weights:  73%|███████▎  | 249/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  73%|███████▎  | 249/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▍  | 251/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  Loading weights:  74%|███████▍  | 251/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]Loading weights:  74%|███████▍  | 252/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  74%|███████▍  | 252/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  75%|███████▍  | 253/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  Loading weights:  75%|███████▍  | 253/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]Loading weights:  75%|███████▍  | 254/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▍  | 254/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▌  | 255/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.21.input_layernorm.weight] Loading weights:  75%|███████▌  | 255/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  76%|███████▌  | 256/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  Loading weights:  76%|███████▌  | 256/339 [00:05<00:01, 58.56it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 258/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  Loading weights:  76%|███████▌  | 258/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.mlp.up_proj.weight]Loading weights:  76%|███████▋  | 259/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  76%|███████▋  | 259/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  77%|███████▋  | 260/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          Loading weights:  77%|███████▋  | 260/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]Loading weights:  77%|███████▋  | 261/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 261/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:05<00:01, 58.98it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  78%|███████▊  | 263/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  78%|███████▊  | 263/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  Loading weights:  78%|███████▊  | 263/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]Loading weights:  78%|███████▊  | 264/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 264/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 265/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  Loading weights:  78%|███████▊  | 265/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]Loading weights:  78%|███████▊  | 266/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  78%|███████▊  | 266/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  79%|███████▉  | 267/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.22.input_layernorm.weight] Loading weights:  79%|███████▉  | 267/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.22.input_layernorm.weight]Loading weights:  79%|███████▉  | 268/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  Loading weights:  79%|███████▉  | 268/339 [00:05<00:01, 44.59it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:05<00:01, 41.32it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:05<00:01, 41.32it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:05<00:01, 41.32it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  80%|███████▉  | 270/339 [00:05<00:01, 41.32it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  Loading weights:  80%|███████▉  | 270/339 [00:05<00:01, 41.32it/s, Materializing param=model.layers.22.mlp.up_proj.weight]Loading weights:  80%|███████▉  | 271/339 [00:06<00:01, 41.32it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|███████▉  | 271/339 [00:06<00:01, 41.32it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|████████  | 272/339 [00:06<00:01, 41.32it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          Loading weights:  80%|████████  | 272/339 [00:06<00:01, 41.32it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]Loading weights:  81%|████████  | 273/339 [00:06<00:01, 41.32it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 273/339 [00:06<00:01, 41.32it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 274/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 274/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 274/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 275/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  Loading weights:  81%|████████  | 275/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]Loading weights:  81%|████████▏ | 276/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  81%|████████▏ | 276/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  82%|████████▏ | 277/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  Loading weights:  82%|████████▏ | 277/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]Loading weights:  82%|████████▏ | 278/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 278/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 279/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.23.input_layernorm.weight] Loading weights:  82%|████████▏ | 279/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.23.input_layernorm.weight]Loading weights:  83%|████████▎ | 280/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  Loading weights:  83%|████████▎ | 280/339 [00:06<00:01, 32.68it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:06<00:01, 33.49it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:06<00:01, 33.49it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:06<00:01, 33.49it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 282/339 [00:06<00:01, 33.49it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  Loading weights:  83%|████████▎ | 282/339 [00:06<00:01, 33.49it/s, Materializing param=model.layers.23.mlp.up_proj.weight]Loading weights:  83%|████████▎ | 283/339 [00:06<00:01, 33.49it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  83%|████████▎ | 283/339 [00:06<00:01, 33.49it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  84%|████████▍ | 284/339 [00:06<00:01, 33.49it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          Loading weights:  84%|████████▍ | 284/339 [00:06<00:01, 33.49it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]Loading weights:  84%|████████▍ | 285/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]Loading weights:  84%|████████▍ | 285/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 285/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  85%|████████▍ | 287/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  Loading weights:  85%|████████▍ | 287/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]Loading weights:  85%|████████▍ | 288/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▍ | 288/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▌ | 289/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  Loading weights:  85%|████████▌ | 289/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]Loading weights:  86%|████████▌ | 290/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 290/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 291/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.24.input_layernorm.weight] Loading weights:  86%|████████▌ | 291/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.24.input_layernorm.weight]Loading weights:  86%|████████▌ | 292/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.24.mlp.down_proj.weight]  Loading weights:  86%|████████▌ | 292/339 [00:06<00:01, 27.26it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:06<00:01, 30.77it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:06<00:01, 30.77it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:06<00:01, 30.77it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  87%|████████▋ | 294/339 [00:06<00:01, 30.77it/s, Materializing param=model.layers.24.mlp.up_proj.weight]  Loading weights:  87%|████████▋ | 294/339 [00:06<00:01, 30.77it/s, Materializing param=model.layers.24.mlp.up_proj.weight]Loading weights:  87%|████████▋ | 295/339 [00:06<00:01, 30.77it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 295/339 [00:06<00:01, 30.77it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 296/339 [00:06<00:01, 30.77it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]          Loading weights:  87%|████████▋ | 296/339 [00:06<00:01, 30.77it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]Loading weights:  88%|████████▊ | 297/339 [00:06<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]Loading weights:  88%|████████▊ | 297/339 [00:06<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 297/339 [00:06<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:06<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:06<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 299/339 [00:06<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]  Loading weights:  88%|████████▊ | 299/339 [00:06<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]Loading weights:  88%|████████▊ | 300/339 [00:06<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  88%|████████▊ | 300/339 [00:06<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  89%|████████▉ | 301/339 [00:07<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]  Loading weights:  89%|████████▉ | 301/339 [00:07<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]Loading weights:  89%|████████▉ | 302/339 [00:07<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 302/339 [00:07<00:01, 25.38it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 303/339 [00:07<00:01, 25.38it/s, Materializing param=model.layers.25.input_layernorm.weight] Loading weights:  89%|████████▉ | 303/339 [00:07<00:01, 25.38it/s, Materializing param=model.layers.25.input_layernorm.weight]Loading weights:  90%|████████▉ | 304/339 [00:07<00:01, 25.38it/s, Materializing param=model.layers.25.mlp.down_proj.weight]  Loading weights:  90%|████████▉ | 304/339 [00:07<00:01, 25.38it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:07<00:01, 25.10it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:07<00:01, 25.10it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:07<00:01, 25.10it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|█████████ | 306/339 [00:07<00:01, 25.10it/s, Materializing param=model.layers.25.mlp.up_proj.weight]  Loading weights:  90%|█████████ | 306/339 [00:07<00:01, 25.10it/s, Materializing param=model.layers.25.mlp.up_proj.weight]Loading weights:  91%|█████████ | 307/339 [00:07<00:01, 25.10it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 307/339 [00:07<00:01, 25.10it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 308/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 308/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]          Loading weights:  91%|█████████ | 308/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]Loading weights:  91%|█████████ | 309/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████ | 309/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  92%|█████████▏| 311/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]  Loading weights:  92%|█████████▏| 311/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]Loading weights:  92%|█████████▏| 312/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 312/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 313/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]  Loading weights:  92%|█████████▏| 313/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]Loading weights:  93%|█████████▎| 314/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 314/339 [00:07<00:01, 24.60it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 315/339 [00:07<00:00, 24.60it/s, Materializing param=model.layers.26.input_layernorm.weight] Loading weights:  93%|█████████▎| 315/339 [00:07<00:00, 24.60it/s, Materializing param=model.layers.26.input_layernorm.weight]Loading weights:  93%|█████████▎| 316/339 [00:07<00:00, 24.60it/s, Materializing param=model.layers.26.mlp.down_proj.weight]  Loading weights:  93%|█████████▎| 316/339 [00:07<00:00, 24.60it/s, Materializing param=model.layers.26.mlp.down_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:07<00:00, 30.10it/s, Materializing param=model.layers.26.mlp.down_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:07<00:00, 30.10it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:07<00:00, 30.10it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▍| 318/339 [00:07<00:00, 30.10it/s, Materializing param=model.layers.26.mlp.up_proj.weight]  Loading weights:  94%|█████████▍| 318/339 [00:07<00:00, 30.10it/s, Materializing param=model.layers.26.mlp.up_proj.weight]Loading weights:  94%|█████████▍| 319/339 [00:07<00:00, 30.10it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 319/339 [00:07<00:00, 30.10it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 320/339 [00:07<00:00, 30.10it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]          Loading weights:  94%|█████████▍| 320/339 [00:07<00:00, 30.10it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]Loading weights:  95%|█████████▍| 321/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]Loading weights:  95%|█████████▍| 321/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 321/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▌| 323/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]  Loading weights:  95%|█████████▌| 323/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]Loading weights:  96%|█████████▌| 324/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 324/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 325/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]  Loading weights:  96%|█████████▌| 325/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]Loading weights:  96%|█████████▌| 326/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▌| 326/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▋| 327/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.27.input_layernorm.weight] Loading weights:  96%|█████████▋| 327/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.27.input_layernorm.weight]Loading weights:  97%|█████████▋| 328/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.27.mlp.down_proj.weight]  Loading weights:  97%|█████████▋| 328/339 [00:07<00:00, 24.91it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:08<00:00, 29.46it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:08<00:00, 29.46it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:08<00:00, 29.46it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 330/339 [00:08<00:00, 29.46it/s, Materializing param=model.layers.27.mlp.up_proj.weight]  Loading weights:  97%|█████████▋| 330/339 [00:08<00:00, 29.46it/s, Materializing param=model.layers.27.mlp.up_proj.weight]Loading weights:  98%|█████████▊| 331/339 [00:08<00:00, 29.46it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 331/339 [00:08<00:00, 29.46it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 332/339 [00:08<00:00, 29.46it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]          Loading weights:  98%|█████████▊| 332/339 [00:08<00:00, 29.46it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]Loading weights:  98%|█████████▊| 333/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]Loading weights:  98%|█████████▊| 333/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  98%|█████████▊| 333/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▉| 335/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]  Loading weights:  99%|█████████▉| 335/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]Loading weights:  99%|█████████▉| 336/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 336/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 337/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]  Loading weights:  99%|█████████▉| 337/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]Loading weights: 100%|█████████▉| 338/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|█████████▉| 338/339 [00:08<00:00, 23.71it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|██████████| 339/339 [00:08<00:00, 23.71it/s, Materializing param=model.norm.weight]                      Loading weights: 100%|██████████| 339/339 [00:08<00:00, 23.71it/s, Materializing param=model.norm.weight]Loading weights: 100%|██████████| 339/339 [00:08<00:00, 40.24it/s, Materializing param=model.norm.weight]
The tokenizer you are loading from './local_models/Qwen/Qwen2.5-7B-Instruct' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
Using Huggingface Hub Token from .env
Using dtype: torch.bfloat16
loading local model and tokenizer...
Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]Loading weights:   0%|          | 1/339 [00:00<00:00, 110376.42it/s, Materializing param=lm_head.weight]Loading weights:   0%|          | 1/339 [00:00<00:00, 23696.63it/s, Materializing param=lm_head.weight] Loading weights:   1%|          | 2/339 [00:02<07:31,  1.34s/it, Materializing param=lm_head.weight]   Loading weights:   1%|          | 2/339 [00:02<07:31,  1.34s/it, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 2/339 [00:02<07:31,  1.34s/it, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 3/339 [00:02<07:30,  1.34s/it, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 3/339 [00:02<07:30,  1.34s/it, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 4/339 [00:02<03:15,  1.71it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 4/339 [00:02<03:15,  1.71it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  Loading weights:   1%|          | 4/339 [00:02<03:15,  1.71it/s, Materializing param=model.layers.0.mlp.down_proj.weight]Loading weights:   1%|▏         | 5/339 [00:02<03:14,  1.71it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   1%|▏         | 5/339 [00:02<03:14,  1.71it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|▏         | 6/339 [00:02<03:14,  1.71it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  Loading weights:   2%|▏         | 6/339 [00:02<03:14,  1.71it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   2%|▏         | 7/339 [00:02<01:36,  3.44it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   2%|▏         | 7/339 [00:02<01:36,  3.44it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 7/339 [00:02<01:36,  3.44it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 8/339 [00:02<01:36,  3.44it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          Loading weights:   2%|▏         | 8/339 [00:02<01:36,  3.44it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]Loading weights:   3%|▎         | 9/339 [00:02<01:35,  3.44it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 9/339 [00:02<01:35,  3.44it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 10/339 [00:02<01:35,  3.44it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 10/339 [00:02<01:35,  3.44it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 11/339 [00:02<01:35,  3.44it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]  Loading weights:   3%|▎         | 11/339 [00:02<01:35,  3.44it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]Loading weights:   4%|▎         | 12/339 [00:02<01:35,  3.44it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▎         | 12/339 [00:02<01:35,  3.44it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▍         | 13/339 [00:02<01:34,  3.44it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  Loading weights:   4%|▍         | 13/339 [00:02<01:34,  3.44it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]Loading weights:   4%|▍         | 14/339 [00:03<01:34,  3.44it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 14/339 [00:03<01:34,  3.44it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 15/339 [00:03<01:34,  3.44it/s, Materializing param=model.layers.1.input_layernorm.weight] Loading weights:   4%|▍         | 15/339 [00:03<01:34,  3.44it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   5%|▍         | 16/339 [00:03<01:33,  3.44it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  Loading weights:   5%|▍         | 16/339 [00:03<01:33,  3.44it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   5%|▌         | 17/339 [00:03<00:28, 11.29it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   5%|▌         | 17/339 [00:03<00:28, 11.29it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 17/339 [00:03<00:28, 11.29it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 18/339 [00:03<00:28, 11.29it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  Loading weights:   5%|▌         | 18/339 [00:03<00:28, 11.29it/s, Materializing param=model.layers.1.mlp.up_proj.weight]Loading weights:   6%|▌         | 19/339 [00:03<00:28, 11.29it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 19/339 [00:03<00:28, 11.29it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 20/339 [00:03<00:28, 11.29it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          Loading weights:   6%|▌         | 20/339 [00:03<00:28, 11.29it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]Loading weights:   6%|▌         | 21/339 [00:03<00:24, 13.21it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]Loading weights:   6%|▌         | 21/339 [00:03<00:24, 13.21it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▌         | 21/339 [00:03<00:24, 13.21it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▋         | 22/339 [00:03<00:24, 13.21it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   6%|▋         | 22/339 [00:03<00:24, 13.21it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   7%|▋         | 23/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  Loading weights:   7%|▋         | 23/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]Loading weights:   7%|▋         | 24/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 24/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 25/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  Loading weights:   7%|▋         | 25/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]Loading weights:   8%|▊         | 26/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 26/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 27/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.2.input_layernorm.weight] Loading weights:   8%|▊         | 27/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:   8%|▊         | 28/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  Loading weights:   8%|▊         | 28/339 [00:03<00:23, 13.21it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:   9%|▊         | 29/339 [00:03<00:14, 20.72it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:   9%|▊         | 29/339 [00:03<00:14, 20.72it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▊         | 29/339 [00:03<00:14, 20.72it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▉         | 30/339 [00:03<00:14, 20.72it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  Loading weights:   9%|▉         | 30/339 [00:03<00:14, 20.72it/s, Materializing param=model.layers.2.mlp.up_proj.weight]Loading weights:   9%|▉         | 31/339 [00:03<00:14, 20.72it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 31/339 [00:03<00:14, 20.72it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 32/339 [00:03<00:14, 20.72it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          Loading weights:   9%|▉         | 32/339 [00:03<00:14, 20.72it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]Loading weights:  10%|▉         | 33/339 [00:03<00:14, 20.72it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|▉         | 33/339 [00:03<00:14, 20.72it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|█         | 34/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|█         | 34/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 34/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 35/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  Loading weights:  10%|█         | 35/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]Loading weights:  11%|█         | 36/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 36/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 37/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  Loading weights:  11%|█         | 37/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]Loading weights:  11%|█         | 38/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  11%|█         | 38/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  12%|█▏        | 39/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.3.input_layernorm.weight] Loading weights:  12%|█▏        | 39/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  12%|█▏        | 40/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  Loading weights:  12%|█▏        | 40/339 [00:03<00:13, 22.32it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:03<00:10, 28.69it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:03<00:10, 28.69it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:03<00:10, 28.69it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 42/339 [00:03<00:10, 28.69it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  Loading weights:  12%|█▏        | 42/339 [00:03<00:10, 28.69it/s, Materializing param=model.layers.3.mlp.up_proj.weight]Loading weights:  13%|█▎        | 43/339 [00:03<00:10, 28.69it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 43/339 [00:03<00:10, 28.69it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 44/339 [00:03<00:10, 28.69it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          Loading weights:  13%|█▎        | 44/339 [00:03<00:10, 28.69it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]Loading weights:  13%|█▎        | 45/339 [00:03<00:10, 28.69it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  13%|█▎        | 45/339 [00:03<00:10, 28.69it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▍        | 47/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  Loading weights:  14%|█▍        | 47/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]Loading weights:  14%|█▍        | 48/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 48/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 49/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  Loading weights:  14%|█▍        | 49/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]Loading weights:  15%|█▍        | 50/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▍        | 50/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▌        | 51/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.4.input_layernorm.weight] Loading weights:  15%|█▌        | 51/339 [00:03<00:10, 28.39it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  15%|█▌        | 52/339 [00:04<00:08, 32.70it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  15%|█▌        | 52/339 [00:04<00:08, 32.70it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  Loading weights:  15%|█▌        | 52/339 [00:04<00:08, 32.70it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:04<00:08, 32.70it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:04<00:08, 32.70it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 54/339 [00:04<00:08, 32.70it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  Loading weights:  16%|█▌        | 54/339 [00:04<00:08, 32.70it/s, Materializing param=model.layers.4.mlp.up_proj.weight]Loading weights:  16%|█▌        | 55/339 [00:04<00:08, 32.70it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  16%|█▌        | 55/339 [00:04<00:08, 32.70it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  17%|█▋        | 56/339 [00:04<00:08, 32.70it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          Loading weights:  17%|█▋        | 56/339 [00:04<00:08, 32.70it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]Loading weights:  17%|█▋        | 57/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]Loading weights:  17%|█▋        | 57/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 57/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 59/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  Loading weights:  17%|█▋        | 59/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]Loading weights:  18%|█▊        | 60/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 60/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 61/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  Loading weights:  18%|█▊        | 61/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]Loading weights:  18%|█▊        | 62/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  18%|█▊        | 62/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  19%|█▊        | 63/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.5.input_layernorm.weight] Loading weights:  19%|█▊        | 63/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  19%|█▉        | 64/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  Loading weights:  19%|█▉        | 64/339 [00:04<00:08, 31.34it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:04<00:07, 38.57it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:04<00:07, 38.57it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:04<00:07, 38.57it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 66/339 [00:04<00:07, 38.57it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  Loading weights:  19%|█▉        | 66/339 [00:04<00:07, 38.57it/s, Materializing param=model.layers.5.mlp.up_proj.weight]Loading weights:  20%|█▉        | 67/339 [00:04<00:07, 38.57it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|█▉        | 67/339 [00:04<00:07, 38.57it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|██        | 68/339 [00:04<00:07, 38.57it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          Loading weights:  20%|██        | 68/339 [00:04<00:07, 38.57it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]Loading weights:  20%|██        | 69/339 [00:04<00:07, 38.57it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  20%|██        | 69/339 [00:04<00:07, 38.57it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  21%|██        | 70/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  21%|██        | 70/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 70/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 71/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  Loading weights:  21%|██        | 71/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]Loading weights:  21%|██        | 72/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  21%|██        | 72/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  22%|██▏       | 73/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  Loading weights:  22%|██▏       | 73/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]Loading weights:  22%|██▏       | 74/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 74/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 75/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.6.input_layernorm.weight] Loading weights:  22%|██▏       | 75/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  22%|██▏       | 76/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  Loading weights:  22%|██▏       | 76/339 [00:04<00:07, 35.18it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:04<00:06, 40.36it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:04<00:06, 40.36it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:04<00:06, 40.36it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 78/339 [00:04<00:06, 40.36it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  Loading weights:  23%|██▎       | 78/339 [00:04<00:06, 40.36it/s, Materializing param=model.layers.6.mlp.up_proj.weight]Loading weights:  23%|██▎       | 79/339 [00:04<00:06, 40.36it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  23%|██▎       | 79/339 [00:04<00:06, 40.36it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  24%|██▎       | 80/339 [00:04<00:06, 40.36it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          Loading weights:  24%|██▎       | 80/339 [00:04<00:06, 40.36it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]Loading weights:  24%|██▍       | 81/339 [00:04<00:06, 40.36it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 81/339 [00:04<00:06, 40.36it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:04<00:07, 36.14it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:04<00:07, 36.14it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:04<00:07, 36.14it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 83/339 [00:04<00:07, 36.14it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  Loading weights:  24%|██▍       | 83/339 [00:04<00:07, 36.14it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]Loading weights:  25%|██▍       | 84/339 [00:04<00:07, 36.14it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▍       | 84/339 [00:04<00:07, 36.14it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▌       | 85/339 [00:04<00:07, 36.14it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  Loading weights:  25%|██▌       | 85/339 [00:04<00:07, 36.14it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]Loading weights:  25%|██▌       | 86/339 [00:04<00:07, 36.14it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  25%|██▌       | 86/339 [00:04<00:07, 36.14it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  26%|██▌       | 87/339 [00:04<00:06, 36.14it/s, Materializing param=model.layers.7.input_layernorm.weight] Loading weights:  26%|██▌       | 87/339 [00:04<00:06, 36.14it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  26%|██▌       | 88/339 [00:04<00:06, 36.14it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  Loading weights:  26%|██▌       | 88/339 [00:04<00:06, 36.14it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:04<00:06, 41.03it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:04<00:06, 41.03it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:04<00:06, 41.03it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  27%|██▋       | 90/339 [00:05<00:06, 41.03it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  Loading weights:  27%|██▋       | 90/339 [00:05<00:06, 41.03it/s, Materializing param=model.layers.7.mlp.up_proj.weight]Loading weights:  27%|██▋       | 91/339 [00:05<00:06, 41.03it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 91/339 [00:05<00:06, 41.03it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 92/339 [00:05<00:06, 41.03it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          Loading weights:  27%|██▋       | 92/339 [00:05<00:06, 41.03it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]Loading weights:  27%|██▋       | 93/339 [00:05<00:05, 41.03it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  27%|██▋       | 93/339 [00:05<00:05, 41.03it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 95/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  Loading weights:  28%|██▊       | 95/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]Loading weights:  28%|██▊       | 96/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  28%|██▊       | 96/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  29%|██▊       | 97/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  Loading weights:  29%|██▊       | 97/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]Loading weights:  29%|██▉       | 98/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 98/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 99/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.8.input_layernorm.weight] Loading weights:  29%|██▉       | 99/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  29%|██▉       | 100/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.8.mlp.down_proj.weight] Loading weights:  29%|██▉       | 100/339 [00:05<00:06, 36.11it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:05<00:05, 41.23it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:05<00:05, 41.23it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:05<00:05, 41.23it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|███       | 102/339 [00:05<00:05, 41.23it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  Loading weights:  30%|███       | 102/339 [00:05<00:05, 41.23it/s, Materializing param=model.layers.8.mlp.up_proj.weight]Loading weights:  30%|███       | 103/339 [00:05<00:05, 41.23it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  30%|███       | 103/339 [00:05<00:05, 41.23it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  31%|███       | 104/339 [00:05<00:05, 41.23it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          Loading weights:  31%|███       | 104/339 [00:05<00:05, 41.23it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]Loading weights:  31%|███       | 105/339 [00:05<00:05, 41.23it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███       | 105/339 [00:05<00:05, 41.23it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  32%|███▏      | 107/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  Loading weights:  32%|███▏      | 107/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]Loading weights:  32%|███▏      | 108/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 108/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 109/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  Loading weights:  32%|███▏      | 109/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]Loading weights:  32%|███▏      | 110/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  32%|███▏      | 110/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  33%|███▎      | 111/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.9.input_layernorm.weight] Loading weights:  33%|███▎      | 111/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  33%|███▎      | 112/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  Loading weights:  33%|███▎      | 112/339 [00:05<00:06, 36.93it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:05<00:05, 41.99it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:05<00:05, 41.99it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:05<00:05, 41.99it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  34%|███▎      | 114/339 [00:05<00:05, 41.99it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  Loading weights:  34%|███▎      | 114/339 [00:05<00:05, 41.99it/s, Materializing param=model.layers.9.mlp.up_proj.weight]Loading weights:  34%|███▍      | 115/339 [00:05<00:05, 41.99it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 115/339 [00:05<00:05, 41.99it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 116/339 [00:05<00:05, 41.99it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          Loading weights:  34%|███▍      | 116/339 [00:05<00:05, 41.99it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]Loading weights:  35%|███▍      | 117/339 [00:05<00:05, 41.99it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 117/339 [00:05<00:05, 41.99it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▌      | 119/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  Loading weights:  35%|███▌      | 119/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]Loading weights:  35%|███▌      | 120/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  35%|███▌      | 120/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  36%|███▌      | 121/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  Loading weights:  36%|███▌      | 121/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]Loading weights:  36%|███▌      | 122/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▌      | 122/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▋      | 123/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  36%|███▋      | 123/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  37%|███▋      | 124/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  Loading weights:  37%|███▋      | 124/339 [00:05<00:05, 37.31it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:05<00:05, 42.36it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:05<00:05, 42.36it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:05<00:05, 42.36it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 126/339 [00:05<00:05, 42.36it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  Loading weights:  37%|███▋      | 126/339 [00:05<00:05, 42.36it/s, Materializing param=model.layers.10.mlp.up_proj.weight]Loading weights:  37%|███▋      | 127/339 [00:06<00:05, 42.36it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  37%|███▋      | 127/339 [00:06<00:05, 42.36it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  38%|███▊      | 128/339 [00:06<00:04, 42.36it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          Loading weights:  38%|███▊      | 128/339 [00:06<00:04, 42.36it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]Loading weights:  38%|███▊      | 129/339 [00:06<00:04, 42.36it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 129/339 [00:06<00:04, 42.36it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  39%|███▊      | 131/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  Loading weights:  39%|███▊      | 131/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]Loading weights:  39%|███▉      | 132/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 132/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 133/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  Loading weights:  39%|███▉      | 133/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]Loading weights:  40%|███▉      | 134/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 134/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 135/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.11.input_layernorm.weight] Loading weights:  40%|███▉      | 135/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  40%|████      | 136/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  Loading weights:  40%|████      | 136/339 [00:06<00:05, 37.22it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  40%|████      | 137/339 [00:06<00:04, 41.90it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  40%|████      | 137/339 [00:06<00:04, 41.90it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  40%|████      | 137/339 [00:06<00:04, 41.90it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  41%|████      | 138/339 [00:06<00:04, 41.90it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  Loading weights:  41%|████      | 138/339 [00:06<00:04, 41.90it/s, Materializing param=model.layers.11.mlp.up_proj.weight]Loading weights:  41%|████      | 139/339 [00:06<00:04, 41.90it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████      | 139/339 [00:06<00:04, 41.90it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████▏     | 140/339 [00:06<00:04, 41.90it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          Loading weights:  41%|████▏     | 140/339 [00:06<00:04, 41.90it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]Loading weights:  42%|████▏     | 141/339 [00:06<00:04, 41.90it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 141/339 [00:06<00:04, 41.90it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 143/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  Loading weights:  42%|████▏     | 143/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]Loading weights:  42%|████▏     | 144/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  42%|████▏     | 144/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  43%|████▎     | 145/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  Loading weights:  43%|████▎     | 145/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]Loading weights:  43%|████▎     | 146/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 146/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 147/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.12.input_layernorm.weight] Loading weights:  43%|████▎     | 147/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  44%|████▎     | 148/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  Loading weights:  44%|████▎     | 148/339 [00:06<00:05, 37.06it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:06<00:04, 41.68it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:06<00:04, 41.68it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:06<00:04, 41.68it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 150/339 [00:06<00:04, 41.68it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  Loading weights:  44%|████▍     | 150/339 [00:06<00:04, 41.68it/s, Materializing param=model.layers.12.mlp.up_proj.weight]Loading weights:  45%|████▍     | 151/339 [00:06<00:04, 41.68it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 151/339 [00:06<00:04, 41.68it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 152/339 [00:06<00:04, 41.68it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          Loading weights:  45%|████▍     | 152/339 [00:06<00:04, 41.68it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]Loading weights:  45%|████▌     | 153/339 [00:06<00:04, 41.68it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 153/339 [00:06<00:04, 41.68it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:06<00:05, 36.59it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:06<00:05, 36.59it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:06<00:05, 36.59it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  46%|████▌     | 155/339 [00:06<00:05, 36.59it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  Loading weights:  46%|████▌     | 155/339 [00:06<00:05, 36.59it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]Loading weights:  46%|████▌     | 156/339 [00:06<00:05, 36.59it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▌     | 156/339 [00:06<00:05, 36.59it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▋     | 157/339 [00:06<00:04, 36.59it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  Loading weights:  46%|████▋     | 157/339 [00:06<00:04, 36.59it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]Loading weights:  47%|████▋     | 158/339 [00:06<00:04, 36.59it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 158/339 [00:06<00:04, 36.59it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 159/339 [00:06<00:04, 36.59it/s, Materializing param=model.layers.13.input_layernorm.weight] Loading weights:  47%|████▋     | 159/339 [00:06<00:04, 36.59it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  47%|████▋     | 160/339 [00:06<00:04, 36.59it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  Loading weights:  47%|████▋     | 160/339 [00:06<00:04, 36.59it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:06<00:04, 40.97it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:06<00:04, 40.97it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:06<00:04, 40.97it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  48%|████▊     | 162/339 [00:06<00:04, 40.97it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  Loading weights:  48%|████▊     | 162/339 [00:06<00:04, 40.97it/s, Materializing param=model.layers.13.mlp.up_proj.weight]Loading weights:  48%|████▊     | 163/339 [00:06<00:04, 40.97it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 163/339 [00:06<00:04, 40.97it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 164/339 [00:06<00:04, 40.97it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          Loading weights:  48%|████▊     | 164/339 [00:06<00:04, 40.97it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]Loading weights:  49%|████▊     | 165/339 [00:06<00:04, 40.97it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▊     | 165/339 [00:06<00:04, 40.97it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 167/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  Loading weights:  49%|████▉     | 167/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]Loading weights:  50%|████▉     | 168/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 168/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 169/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  Loading weights:  50%|████▉     | 169/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]Loading weights:  50%|█████     | 170/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 170/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 171/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.14.input_layernorm.weight] Loading weights:  50%|█████     | 171/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  51%|█████     | 172/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  Loading weights:  51%|█████     | 172/339 [00:06<00:04, 36.15it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  51%|█████     | 173/339 [00:07<00:04, 41.17it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  51%|█████     | 173/339 [00:07<00:04, 41.17it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████     | 173/339 [00:07<00:04, 41.17it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████▏    | 174/339 [00:07<00:04, 41.17it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  Loading weights:  51%|█████▏    | 174/339 [00:07<00:04, 41.17it/s, Materializing param=model.layers.14.mlp.up_proj.weight]Loading weights:  52%|█████▏    | 175/339 [00:07<00:03, 41.17it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 175/339 [00:07<00:03, 41.17it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 176/339 [00:07<00:03, 41.17it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          Loading weights:  52%|█████▏    | 176/339 [00:07<00:03, 41.17it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]Loading weights:  52%|█████▏    | 177/339 [00:07<00:03, 41.17it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  52%|█████▏    | 177/339 [00:07<00:03, 41.17it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 179/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  Loading weights:  53%|█████▎    | 179/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]Loading weights:  53%|█████▎    | 180/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 180/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 181/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  Loading weights:  53%|█████▎    | 181/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]Loading weights:  54%|█████▎    | 182/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▎    | 182/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▍    | 183/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.15.input_layernorm.weight] Loading weights:  54%|█████▍    | 183/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  54%|█████▍    | 184/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  Loading weights:  54%|█████▍    | 184/339 [00:07<00:04, 36.40it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:07<00:03, 41.15it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:07<00:03, 41.15it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:07<00:03, 41.15it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 186/339 [00:07<00:03, 41.15it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  Loading weights:  55%|█████▍    | 186/339 [00:07<00:03, 41.15it/s, Materializing param=model.layers.15.mlp.up_proj.weight]Loading weights:  55%|█████▌    | 187/339 [00:07<00:03, 41.15it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 187/339 [00:07<00:03, 41.15it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 188/339 [00:07<00:03, 41.15it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          Loading weights:  55%|█████▌    | 188/339 [00:07<00:03, 41.15it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]Loading weights:  56%|█████▌    | 189/339 [00:07<00:03, 41.15it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 189/339 [00:07<00:03, 41.15it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:07<00:04, 36.16it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:07<00:04, 36.16it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:07<00:04, 36.16it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▋    | 191/339 [00:07<00:04, 36.16it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  Loading weights:  56%|█████▋    | 191/339 [00:07<00:04, 36.16it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]Loading weights:  57%|█████▋    | 192/339 [00:07<00:04, 36.16it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 192/339 [00:07<00:04, 36.16it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 193/339 [00:07<00:04, 36.16it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  Loading weights:  57%|█████▋    | 193/339 [00:07<00:04, 36.16it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]Loading weights:  57%|█████▋    | 194/339 [00:07<00:04, 36.16it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  57%|█████▋    | 194/339 [00:07<00:04, 36.16it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  58%|█████▊    | 195/339 [00:07<00:03, 36.16it/s, Materializing param=model.layers.16.input_layernorm.weight] Loading weights:  58%|█████▊    | 195/339 [00:07<00:03, 36.16it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  58%|█████▊    | 196/339 [00:07<00:03, 36.16it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  Loading weights:  58%|█████▊    | 196/339 [00:07<00:03, 36.16it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:07<00:03, 40.99it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:07<00:03, 40.99it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:07<00:03, 40.99it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 198/339 [00:07<00:03, 40.99it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  Loading weights:  58%|█████▊    | 198/339 [00:07<00:03, 40.99it/s, Materializing param=model.layers.16.mlp.up_proj.weight]Loading weights:  59%|█████▊    | 199/339 [00:07<00:03, 40.99it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▊    | 199/339 [00:07<00:03, 40.99it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▉    | 200/339 [00:07<00:03, 40.99it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          Loading weights:  59%|█████▉    | 200/339 [00:07<00:03, 40.99it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]Loading weights:  59%|█████▉    | 201/339 [00:07<00:03, 40.99it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  59%|█████▉    | 201/339 [00:07<00:03, 40.99it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 203/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  Loading weights:  60%|█████▉    | 203/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]Loading weights:  60%|██████    | 204/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 204/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 205/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  Loading weights:  60%|██████    | 205/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]Loading weights:  61%|██████    | 206/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 206/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 207/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.17.input_layernorm.weight] Loading weights:  61%|██████    | 207/339 [00:07<00:03, 36.43it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  61%|██████▏   | 208/339 [00:07<00:03, 39.17it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  61%|██████▏   | 208/339 [00:07<00:03, 39.17it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  Loading weights:  61%|██████▏   | 208/339 [00:07<00:03, 39.17it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:07<00:03, 39.17it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:07<00:03, 39.17it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 210/339 [00:08<00:03, 39.17it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  Loading weights:  62%|██████▏   | 210/339 [00:08<00:03, 39.17it/s, Materializing param=model.layers.17.mlp.up_proj.weight]Loading weights:  62%|██████▏   | 211/339 [00:08<00:03, 39.17it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  62%|██████▏   | 211/339 [00:08<00:03, 39.17it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  63%|██████▎   | 212/339 [00:08<00:03, 39.17it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          Loading weights:  63%|██████▎   | 212/339 [00:08<00:03, 39.17it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]Loading weights:  63%|██████▎   | 213/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]Loading weights:  63%|██████▎   | 213/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 213/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 215/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  Loading weights:  63%|██████▎   | 215/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]Loading weights:  64%|██████▎   | 216/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▎   | 216/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▍   | 217/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  Loading weights:  64%|██████▍   | 217/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]Loading weights:  64%|██████▍   | 218/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  64%|██████▍   | 218/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  65%|██████▍   | 219/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.18.input_layernorm.weight] Loading weights:  65%|██████▍   | 219/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.18.input_layernorm.weight]Loading weights:  65%|██████▍   | 220/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  Loading weights:  65%|██████▍   | 220/339 [00:08<00:03, 35.53it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:08<00:02, 42.70it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:08<00:02, 42.70it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:08<00:02, 42.70it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 222/339 [00:08<00:02, 42.70it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  Loading weights:  65%|██████▌   | 222/339 [00:08<00:02, 42.70it/s, Materializing param=model.layers.18.mlp.up_proj.weight]Loading weights:  66%|██████▌   | 223/339 [00:08<00:02, 42.70it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 223/339 [00:08<00:02, 42.70it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 224/339 [00:08<00:02, 42.70it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          Loading weights:  66%|██████▌   | 224/339 [00:08<00:02, 42.70it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]Loading weights:  66%|██████▋   | 225/339 [00:08<00:02, 42.70it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  66%|██████▋   | 225/339 [00:08<00:02, 42.70it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:08<00:03, 37.30it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:08<00:03, 37.30it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:08<00:03, 37.30it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 227/339 [00:08<00:03, 37.30it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  Loading weights:  67%|██████▋   | 227/339 [00:08<00:03, 37.30it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]Loading weights:  67%|██████▋   | 228/339 [00:08<00:02, 37.30it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  67%|██████▋   | 228/339 [00:08<00:02, 37.30it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  68%|██████▊   | 229/339 [00:08<00:02, 37.30it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  Loading weights:  68%|██████▊   | 229/339 [00:08<00:02, 37.30it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]Loading weights:  68%|██████▊   | 230/339 [00:08<00:02, 37.30it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 230/339 [00:08<00:02, 37.30it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 231/339 [00:08<00:02, 37.30it/s, Materializing param=model.layers.19.input_layernorm.weight] Loading weights:  68%|██████▊   | 231/339 [00:08<00:02, 37.30it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  68%|██████▊   | 232/339 [00:08<00:02, 37.30it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  Loading weights:  68%|██████▊   | 232/339 [00:08<00:02, 37.30it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:08<00:02, 42.22it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:08<00:02, 42.22it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:08<00:02, 42.22it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▉   | 234/339 [00:08<00:02, 42.22it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  Loading weights:  69%|██████▉   | 234/339 [00:08<00:02, 42.22it/s, Materializing param=model.layers.19.mlp.up_proj.weight]Loading weights:  69%|██████▉   | 235/339 [00:08<00:02, 42.22it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  69%|██████▉   | 235/339 [00:08<00:02, 42.22it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  70%|██████▉   | 236/339 [00:08<00:02, 42.22it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          Loading weights:  70%|██████▉   | 236/339 [00:08<00:02, 42.22it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]Loading weights:  70%|██████▉   | 237/339 [00:08<00:02, 42.22it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|██████▉   | 237/339 [00:08<00:02, 42.22it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|███████   | 238/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|███████   | 238/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  70%|███████   | 238/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  71%|███████   | 239/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  Loading weights:  71%|███████   | 239/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]Loading weights:  71%|███████   | 240/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 240/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 241/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  Loading weights:  71%|███████   | 241/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]Loading weights:  71%|███████▏  | 242/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  71%|███████▏  | 242/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  72%|███████▏  | 243/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.20.input_layernorm.weight] Loading weights:  72%|███████▏  | 243/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  72%|███████▏  | 244/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  Loading weights:  72%|███████▏  | 244/339 [00:08<00:02, 37.65it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:08<00:02, 42.49it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:08<00:02, 42.49it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:08<00:02, 42.49it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  73%|███████▎  | 246/339 [00:08<00:02, 42.49it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  Loading weights:  73%|███████▎  | 246/339 [00:08<00:02, 42.49it/s, Materializing param=model.layers.20.mlp.up_proj.weight]Loading weights:  73%|███████▎  | 247/339 [00:09<00:02, 42.49it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 247/339 [00:09<00:02, 42.49it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 248/339 [00:09<00:02, 42.49it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          Loading weights:  73%|███████▎  | 248/339 [00:09<00:02, 42.49it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]Loading weights:  73%|███████▎  | 249/339 [00:09<00:02, 42.49it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  73%|███████▎  | 249/339 [00:09<00:02, 42.49it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▍  | 251/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  Loading weights:  74%|███████▍  | 251/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]Loading weights:  74%|███████▍  | 252/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  74%|███████▍  | 252/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  75%|███████▍  | 253/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  Loading weights:  75%|███████▍  | 253/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]Loading weights:  75%|███████▍  | 254/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▍  | 254/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▌  | 255/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.21.input_layernorm.weight] Loading weights:  75%|███████▌  | 255/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  76%|███████▌  | 256/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  Loading weights:  76%|███████▌  | 256/339 [00:09<00:02, 37.81it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 258/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  Loading weights:  76%|███████▌  | 258/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.21.mlp.up_proj.weight]Loading weights:  76%|███████▋  | 259/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  76%|███████▋  | 259/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  77%|███████▋  | 260/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          Loading weights:  77%|███████▋  | 260/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]Loading weights:  77%|███████▋  | 261/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 261/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:09<00:02, 37.48it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:09<00:02, 37.48it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:09<00:02, 37.48it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  78%|███████▊  | 263/339 [00:09<00:02, 37.48it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  Loading weights:  78%|███████▊  | 263/339 [00:09<00:02, 37.48it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]Loading weights:  78%|███████▊  | 264/339 [00:09<00:02, 37.48it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 264/339 [00:09<00:02, 37.48it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 265/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  Loading weights:  78%|███████▊  | 265/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]Loading weights:  78%|███████▊  | 266/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  78%|███████▊  | 266/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  79%|███████▉  | 267/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.input_layernorm.weight] Loading weights:  79%|███████▉  | 267/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.input_layernorm.weight]Loading weights:  79%|███████▉  | 268/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  Loading weights:  79%|███████▉  | 268/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:09<00:01, 42.37it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:09<00:01, 42.37it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:09<00:01, 42.37it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  80%|███████▉  | 270/339 [00:09<00:01, 42.37it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  Loading weights:  80%|███████▉  | 270/339 [00:09<00:01, 42.37it/s, Materializing param=model.layers.22.mlp.up_proj.weight]Loading weights:  80%|███████▉  | 271/339 [00:09<00:01, 42.37it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|███████▉  | 271/339 [00:09<00:01, 42.37it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|████████  | 272/339 [00:09<00:01, 42.37it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          Loading weights:  80%|████████  | 272/339 [00:09<00:01, 42.37it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]Loading weights:  81%|████████  | 273/339 [00:09<00:01, 42.37it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 273/339 [00:09<00:01, 42.37it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 274/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 274/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 274/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 275/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  Loading weights:  81%|████████  | 275/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]Loading weights:  81%|████████▏ | 276/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  81%|████████▏ | 276/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  82%|████████▏ | 277/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  Loading weights:  82%|████████▏ | 277/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]Loading weights:  82%|████████▏ | 278/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 278/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 279/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.23.input_layernorm.weight] Loading weights:  82%|████████▏ | 279/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.23.input_layernorm.weight]Loading weights:  83%|████████▎ | 280/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  Loading weights:  83%|████████▎ | 280/339 [00:09<00:01, 37.48it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:09<00:01, 42.20it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:09<00:01, 42.20it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:09<00:01, 42.20it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 282/339 [00:09<00:01, 42.20it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  Loading weights:  83%|████████▎ | 282/339 [00:09<00:01, 42.20it/s, Materializing param=model.layers.23.mlp.up_proj.weight]Loading weights:  83%|████████▎ | 283/339 [00:09<00:01, 42.20it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  83%|████████▎ | 283/339 [00:09<00:01, 42.20it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  84%|████████▍ | 284/339 [00:09<00:01, 42.20it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          Loading weights:  84%|████████▍ | 284/339 [00:09<00:01, 42.20it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]Loading weights:  84%|████████▍ | 285/339 [00:09<00:01, 42.20it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 285/339 [00:09<00:01, 42.20it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:09<00:01, 37.29it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:09<00:01, 37.29it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:09<00:01, 37.29it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  85%|████████▍ | 287/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  Loading weights:  85%|████████▍ | 287/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]Loading weights:  85%|████████▍ | 288/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▍ | 288/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▌ | 289/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  Loading weights:  85%|████████▌ | 289/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]Loading weights:  86%|████████▌ | 290/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 290/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 291/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.input_layernorm.weight] Loading weights:  86%|████████▌ | 291/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.input_layernorm.weight]Loading weights:  86%|████████▌ | 292/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.mlp.down_proj.weight]  Loading weights:  86%|████████▌ | 292/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:10<00:01, 42.13it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:10<00:01, 42.13it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:10<00:01, 42.13it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  87%|████████▋ | 294/339 [00:10<00:01, 42.13it/s, Materializing param=model.layers.24.mlp.up_proj.weight]  Loading weights:  87%|████████▋ | 294/339 [00:10<00:01, 42.13it/s, Materializing param=model.layers.24.mlp.up_proj.weight]Loading weights:  87%|████████▋ | 295/339 [00:10<00:01, 42.13it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 295/339 [00:10<00:01, 42.13it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 296/339 [00:10<00:01, 42.13it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]          Loading weights:  87%|████████▋ | 296/339 [00:10<00:01, 42.13it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]Loading weights:  88%|████████▊ | 297/339 [00:10<00:00, 42.13it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 297/339 [00:10<00:00, 42.13it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:10<00:01, 37.38it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:10<00:01, 37.38it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:10<00:01, 37.38it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 299/339 [00:10<00:01, 37.38it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]  Loading weights:  88%|████████▊ | 299/339 [00:10<00:01, 37.38it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]Loading weights:  88%|████████▊ | 300/339 [00:10<00:01, 37.38it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  88%|████████▊ | 300/339 [00:10<00:01, 37.38it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  89%|████████▉ | 301/339 [00:10<00:01, 37.38it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]  Loading weights:  89%|████████▉ | 301/339 [00:10<00:01, 37.38it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]Loading weights:  89%|████████▉ | 302/339 [00:10<00:00, 37.38it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 302/339 [00:10<00:00, 37.38it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 303/339 [00:10<00:00, 37.38it/s, Materializing param=model.layers.25.input_layernorm.weight] Loading weights:  89%|████████▉ | 303/339 [00:10<00:00, 37.38it/s, Materializing param=model.layers.25.input_layernorm.weight]Loading weights:  90%|████████▉ | 304/339 [00:10<00:00, 37.38it/s, Materializing param=model.layers.25.mlp.down_proj.weight]  Loading weights:  90%|████████▉ | 304/339 [00:10<00:00, 37.38it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:10<00:00, 42.23it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:10<00:00, 42.23it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:10<00:00, 42.23it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|█████████ | 306/339 [00:10<00:00, 42.23it/s, Materializing param=model.layers.25.mlp.up_proj.weight]  Loading weights:  90%|█████████ | 306/339 [00:10<00:00, 42.23it/s, Materializing param=model.layers.25.mlp.up_proj.weight]Loading weights:  91%|█████████ | 307/339 [00:10<00:00, 42.23it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 307/339 [00:10<00:00, 42.23it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 308/339 [00:10<00:00, 42.23it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]          Loading weights:  91%|█████████ | 308/339 [00:10<00:00, 42.23it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]Loading weights:  91%|█████████ | 309/339 [00:10<00:00, 42.23it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████ | 309/339 [00:10<00:00, 42.23it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  92%|█████████▏| 311/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]  Loading weights:  92%|█████████▏| 311/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]Loading weights:  92%|█████████▏| 312/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 312/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 313/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]  Loading weights:  92%|█████████▏| 313/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]Loading weights:  93%|█████████▎| 314/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 314/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 315/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.26.input_layernorm.weight] Loading weights:  93%|█████████▎| 315/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.26.input_layernorm.weight]Loading weights:  93%|█████████▎| 316/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.26.mlp.down_proj.weight]  Loading weights:  93%|█████████▎| 316/339 [00:10<00:00, 37.04it/s, Materializing param=model.layers.26.mlp.down_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:10<00:00, 42.46it/s, Materializing param=model.layers.26.mlp.down_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:10<00:00, 42.46it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:10<00:00, 42.46it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▍| 318/339 [00:10<00:00, 42.46it/s, Materializing param=model.layers.26.mlp.up_proj.weight]  Loading weights:  94%|█████████▍| 318/339 [00:10<00:00, 42.46it/s, Materializing param=model.layers.26.mlp.up_proj.weight]Loading weights:  94%|█████████▍| 319/339 [00:10<00:00, 42.46it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 319/339 [00:10<00:00, 42.46it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 320/339 [00:10<00:00, 42.46it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]          Loading weights:  94%|█████████▍| 320/339 [00:10<00:00, 42.46it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]Loading weights:  95%|█████████▍| 321/339 [00:10<00:00, 42.46it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 321/339 [00:10<00:00, 42.46it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▌| 323/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]  Loading weights:  95%|█████████▌| 323/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]Loading weights:  96%|█████████▌| 324/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 324/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 325/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]  Loading weights:  96%|█████████▌| 325/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]Loading weights:  96%|█████████▌| 326/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▌| 326/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▋| 327/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.27.input_layernorm.weight] Loading weights:  96%|█████████▋| 327/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.27.input_layernorm.weight]Loading weights:  97%|█████████▋| 328/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.27.mlp.down_proj.weight]  Loading weights:  97%|█████████▋| 328/339 [00:10<00:00, 37.22it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:11<00:00, 42.07it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:11<00:00, 42.07it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:11<00:00, 42.07it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 330/339 [00:11<00:00, 42.07it/s, Materializing param=model.layers.27.mlp.up_proj.weight]  Loading weights:  97%|█████████▋| 330/339 [00:11<00:00, 42.07it/s, Materializing param=model.layers.27.mlp.up_proj.weight]Loading weights:  98%|█████████▊| 331/339 [00:11<00:00, 42.07it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 331/339 [00:11<00:00, 42.07it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 332/339 [00:11<00:00, 42.07it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]          Loading weights:  98%|█████████▊| 332/339 [00:11<00:00, 42.07it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]Loading weights:  98%|█████████▊| 333/339 [00:11<00:00, 42.07it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  98%|█████████▊| 333/339 [00:11<00:00, 42.07it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:11<00:00, 37.21it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:11<00:00, 37.21it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:11<00:00, 37.21it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▉| 335/339 [00:11<00:00, 37.21it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]  Loading weights:  99%|█████████▉| 335/339 [00:11<00:00, 37.21it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]Loading weights:  99%|█████████▉| 336/339 [00:11<00:00, 37.21it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 336/339 [00:11<00:00, 37.21it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 337/339 [00:11<00:00, 37.21it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]  Loading weights:  99%|█████████▉| 337/339 [00:11<00:00, 37.21it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]Loading weights: 100%|█████████▉| 338/339 [00:11<00:00, 37.21it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|█████████▉| 338/339 [00:11<00:00, 37.21it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|██████████| 339/339 [00:11<00:00, 37.21it/s, Materializing param=model.norm.weight]                      Loading weights: 100%|██████████| 339/339 [00:11<00:00, 37.21it/s, Materializing param=model.norm.weight]Loading weights: 100%|██████████| 339/339 [00:11<00:00, 30.17it/s, Materializing param=model.norm.weight]
================================================================================
Send prompt for semantic check:
Generated ASP code:
exam(Exam, Duration, Is_Large).

student(Student, Exam).

period(P, Date, Time, Duration, Is_Late, Penalty).

room(Room, Capacity, Penalty).

order_constrain(E1, E2).

same_time_constraint(E1, E2).

different_time_constraint(E1, E2).

own_room_constraint(Exam).

Intended semantics:
Instance Variables:
- Exams: A set of exams that need to be scheduled.
    - Variables: duration, is_large
- Students: A set of students that are taking a set of exams.
    - Variables: exam
- Periods: A set of periods in which the exams can be scheduled.
    - Variables: date, time, duration, is_late, penalty
- Rooms: A set of rooms in which the exams can be scheduled.
    - Variables: capacity, penalty
- Order_Constrains: A set of constraints that specify the order in which some exams must be scheduled.
    - Variables: exam1, exam2
- Same_Time_Constraints: A set of constraints that specify the exams that must be scheduled at the same time.
    - Variables: exam1, exam2
- Different_Time_Constraints: A set of constraints that specify the exams that must not be scheduled at the same time.
    - Variables: exam1, exam2
- Own_Room_Constraints: A set of constraints that specify the exams that must not be scheduled in a room on their own.
    - Variables: exam


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
exam(Exam, Duration, Is_Large).

student(Student, Exam).

period(P, Date, Time, Duration, Is_Late, Penalty).

room(Room, Capacity, Penalty).

order_constrain(E1, E2).

same_time_constraint(E1, E2).

different_time_constraint(E1, E2).

own_room_constraint(Exam).

####################################################################################
Instance Template:
exam(Exam, Duration, Is_Large).

student(Student, Exam).

period(P, Date, Time, Duration, Is_Late, Penalty).

room(Room, Capacity, Penalty).

order_constrain(E1, E2).

same_time_constraint(E1, E2).

different_time_constraint(E1, E2).

own_room_constraint(Exam).
================================================================================
Send prompt for semantic check:
Generated ASP code:
N_exams { assigned(Exam, Room, Period) : exam(Exam, _, _), room(Room, _, _), period(Period, _, _, _, _, _) }.

Intended semantics:Goal:
An assignment of exams to periods and rooms.
    - Variables: period, room


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
N_exams { assigned(Exam, Room, Period) : exam(Exam, _, _), room(Room, _, _), period(Period, _, _, _, _, _) }.

####################################################################################


Generator
N_exams { assigned(Exam, Room, Period) : exam(Exam, _, _), room(Room, _, _), period(Period, _, _, _, _, _) }.


Hard Constraints

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- student(Student, Exam1), student(Student, Exam2), Exam1 != Exam2, assigned(Exam1, Room, Period), assigned(Exam2, Room, Period).

Intended semantics:- No student sits more than one examination at the same time.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that no student can be assigned to two different exams in the same room and period, which is a stronger constraint than just ensuring no student sits more than one examination at the same time. It could allow different rooms or periods for the two exams."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- student(Student, Exam1), student(Student, Exam2), Exam1 != Exam2, assigned(Exam1, Room, Period), assigned(Exam2, Room, Period).

####################################################################################
- No student sits more than one examination at the same time.
:- student(Student, Exam1), student(Student, Exam2), Exam1 != Exam2, assigned(Exam1, Room, Period), assigned(Exam2, Room, Period).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Exam1, Room, Period), assigned(Exam2, Room, Period), Exam1 != Exam2, student(Student, Exam1), student(Student, Exam2), #count{Exam : assigned(Exam, Room, Period), student(Student, Exam)} > Capacity, room(Room, Capacity, _).

Intended semantics:- The number of students taking exams in a room at the same time should not exceed the capacity of the room.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code incorrectly uses 'Exam1' and 'Exam2' which are not defined, and it does not correctly express the constraint for the number of students in a room at the same time."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Exam1, Room, Period), assigned(Exam2, Room, Period), Exam1 != Exam2, student(Student, Exam1), student(Student, Exam2), #count{Exam : assigned(Exam, Room, Period), student(Student, Exam)} > Capacity, room(Room, Capacity, _).

####################################################################################
- The number of students taking exams in a room at the same time should not exceed the capacity of the room.
 
:- assigned(Exam1, Room, Period), assigned(Exam2, Room, Period), Exam1 != Exam2, student(Student, Exam1), student(Student, Exam2), #count{Exam : assigned(Exam, Room, Period), student(Student, Exam)} > Capacity, room(Room, Capacity, _).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Exam, Room, Period), period(_, _, _, Duration, _, _), period(_, _, _, Duration2, _, _), Duration != Duration2.

Intended semantics:- Period Lengths are not violated.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that periods with the same duration cannot be assigned to the same exam, which is not the same as ensuring period lengths are not violated."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Exam, Room, Period), period(_, _, _, Duration, _, _), period(_, _, _, Duration2, _, _), Duration != Duration2.

####################################################################################
- Period Lengths are not violated.
:- assigned(Exam, Room, Period), period(_, _, _, Duration, _, _), period(_, _, _, Duration2, _, _), Duration != Duration2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- order_constrain(E1, E2), not assigned(E1, _, P1), not assigned(E2, _, P2), P1 < P2.

Intended semantics:- Some exams must be before other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code incorrectly uses 'E1' instead of 'E2' in the negative condition, which changes the intended meaning."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- order_constrain(E1, E2), not assigned(E1, _, P1), not assigned(E2, _, P2), P1 < P2.

####################################################################################
- Some exams must be before other exams.
:- order_constrain(E1, E2), not assigned(E1, _, P1), not assigned(E2, _, P2), P1 < P2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(E1, _, P), assigned(E2, _, P), same_time_constraint(E1, E2).

Intended semantics:- Some exams must be at the same time as other exams.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(E1, _, P), assigned(E2, _, P), same_time_constraint(E1, E2).

####################################################################################
- Some exams must be at the same time as other exams.
:- assigned(E1, _, P), assigned(E2, _, P), same_time_constraint(E1, E2).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(E1, _, P1), assigned(E2, _, P2), different_time_constraint(E1, E2), P1 != P2.

Intended semantics:- Some exams must be at a differen time than other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that exams with a different time constraint must be scheduled in different periods, which is not necessarily the same as just having different times. It could still be the same period as long as it's not the same room."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(E1, _, P1), assigned(E2, _, P2), different_time_constraint(E1, E2), P1 != P2.

####################################################################################
- Some exams must be at a differen time than other exams.
:- assigned(E1, _, P1), assigned(E2, _, P2), different_time_constraint(E1, E2), P1 != P2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(E1, R, P), assigned(E2, R, P), E1 != E2.

Intended semantics:- Some exams must take place in a room with no other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that no two different exams can be assigned to the same room and period simultaneously, which is a stronger constraint than just requiring some exams to be alone in a room."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(E1, R, P), assigned(E2, R, P), E1 != E2.

####################################################################################
- Some exams must take place in a room with no other exams.
:- assigned(E1, R, P), assigned(E2, R, P), E1 != E2.


Soft Constraints:

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("ExamOverlap",student(Student,E1),7) :- student(Student,E1), student(Student,E2), E1 < E2, assigned(E1,_,P1), assigned(E2,_,P2), P1 < P2, P1 + 1 = P2, period(P1,_,_,_,_,_), period(P2,_,_,_,_,_).
penalty("ExamOverlap",student(Student,E1),5) :- student(Student,E1), student(Student,E2), E1 < E2, assigned(E1,_,P1), assigned(E2,_,P2), P1 < P2, P1 + 1 != P2, period(P1,_,_,_,_,_), period(P2,_,_,_,_,_).

Intended semantics:- Students should not have more than one exam in the same day
 Penalty: 7 for two exams in the same day that are in consecutive periods.
 Penalty: 5 for two exams in the same day that are not in consecutive periods.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces penalties for exams in the same day but does not ensure that students do not have more than one exam on the same day. It only penalizes exams that are in consecutive periods."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("ExamOverlap",student(Student,E1),7) :- student(Student,E1), student(Student,E2), E1 < E2, assigned(E1,_,P1), assigned(E2,_,P2), P1 < P2, P1 + 1 = P2, period(P1,_,_,_,_,_), period(P2,_,_,_,_,_).
penalty("ExamOverlap",student(Student,E1),5) :- student(Student,E1), student(Student,E2), E1 < E2, assigned(E1,_,P1), assigned(E2,_,P2), P1 < P2, P1 + 1 != P2, period(P1,_,_,_,_,_), period(P2,_,_,_,_,_).

####################################################################################
- Students should not have more than one exam in the same day
 Penalty: 7 for two exams in the same day that are in consecutive periods.
 Penalty: 5 for two exams in the same day that are not in consecutive periods.
penalty("ExamOverlap",student(Student,E1),7) :- student(Student,E1), student(Student,E2), E1 < E2, assigned(E1,_,P1), assigned(E2,_,P2), P1 < P2, P1 + 1 = P2, period(P1,_,_,_,_,_), period(P2,_,_,_,_,_).
penalty("ExamOverlap",student(Student,E1),5) :- student(Student,E1), student(Student,E2), E1 < E2, assigned(E1,_,P1), assigned(E2,_,P2), P1 < P2, P1 + 1 != P2, period(P1,_,_,_,_,_), period(P2,_,_,_,_,_).

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("DifferentLength",assigned(E1, Room, Period),10) :- assigned(E1, Room, Period), assigned(E2, Room, Period), exam(E1, Dur1, _), exam(E2, Dur2, _), Dur1 != Dur2.

Intended semantics:- Exams of different lengths should not be scheduled in the same room in the same period
 Penalty: 10 for each exam of different length scheduled in the same room in the same period.

Semantic check result:
{
  "match": "false",
  "reason": "The code uses a variable name 'E1' in the penalty rule, which is incorrect. It should be 'E1' and 'E2' as used in the body of the rule."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("DifferentLength",assigned(E1, Room, Period),10) :- assigned(E1, Room, Period), assigned(E2, Room, Period), exam(E1, Dur1, _), exam(E2, Dur2, _), Dur1 != Dur2.

####################################################################################
- Exams of different lengths should not be scheduled in the same room in the same period
 Penalty: 10 for each exam of different length scheduled in the same room in the same period.
penalty("DifferentLength",assigned(E1, Room, Period),10) :- assigned(E1, Room, Period), assigned(E2, Room, Period), exam(E1, Dur1, _), exam(E2, Dur2, _), Dur1 != Dur2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("PeriodPenalty",exam(Exam,_,_),P*Penalty) :- assigned(Exam,Room,Period), period(Period,_,_,_,Penalty,_).

Intended semantics:- Some periods have associated penalties
 Penalty: 1 for each time an exam is scheduled in a period with an associated penalty. Weighted by the penalty value of the period.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("PeriodPenalty",exam(Exam,_,_),P*Penalty) :- assigned(Exam,Room,Period), period(Period,_,_,_,Penalty,_).

####################################################################################
- Some periods have associated penalties
 Penalty: 1 for each time an exam is scheduled in a period with an associated penalty. Weighted by the penalty value of the period.
penalty("PeriodPenalty",exam(Exam,_,_),P*Penalty) :- assigned(Exam,Room,Period), period(Period,_,_,_,Penalty,_).

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("RoomPenalty",Room,Penalty*(N-1)) :- assigned(Exam,Room,Period), room(Room,_,Penalty), N = #count{P : assigned(Exam,Room,P)}.

Intended semantics:- Some rooms have associated penalties
 Penalty: 1 for each time an exam is scheduled in a room with an associated penalty. Weighted by the penalty value of the room.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("RoomPenalty",Room,Penalty*(N-1)) :- assigned(Exam,Room,Period), room(Room,_,Penalty), N = #count{P : assigned(Exam,Room,P)}.

####################################################################################
- Some rooms have associated penalties
 Penalty: 1 for each time an exam is scheduled in a room with an associated penalty. Weighted by the penalty value of the room.
penalty("RoomPenalty",Room,Penalty*(N-1)) :- assigned(Exam,Room,Period), room(Room,_,Penalty), N = #count{P : assigned(Exam,Room,P)}.

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("ExamSpread",student(S),3) :- student(S), N = #count{P1,P2 : assigned(E1,S,P1), assigned(E2,S,P2), P1 < P2, P2 - P1 <= 7}, N > 1.

Intended semantics:- Students should not have more than one exam within a spread of 7 periods.
 Penalty: 3 for each time a student has two exams within a spread of periods.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("ExamSpread",student(S),3) :- student(S), N = #count{P1,P2 : assigned(E1,S,P1), assigned(E2,S,P2), P1 < P2, P2 - P1 <= 7}, N > 1.

####################################################################################
- Students should not have more than one exam within a spread of 7 periods.
 Penalty: 3 for each time a student has two exams within a spread of periods.
penalty("ExamSpread",student(S),3) :- student(S), N = #count{P1,P2 : assigned(E1,S,P1), assigned(E2,S,P2), P1 < P2, P2 - P1 <= 7}, N > 1.

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("LateLargeExam",exam(Exam,_,Is_Large),5) :- assigned(Exam,Room,Period), period(Period,_,_,_,Is_Late,_), exam(Exam,_,_), Is_Large = "large", Is_Late = "late".

Intended semantics:- An exam that is large should not be scheduled in a period that is late.
 Penalty: 5 for each large exam scheduled in a period that is late.

Semantic check result:
{
  "match": "false",
  "reason": "The generated code incorrectly uses the predicate `penalty` with three arguments instead of four, and it does not correctly enforce the penalty for each large exam scheduled in a late period."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("LateLargeExam",exam(Exam,_,Is_Large),5) :- assigned(Exam,Room,Period), period(Period,_,_,_,Is_Late,_), exam(Exam,_,_), Is_Large = "large", Is_Late = "late".

####################################################################################
- An exam that is large should not be scheduled in a period that is late.
 Penalty: 5 for each large exam scheduled in a period that is late.
penalty("LateLargeExam",exam(Exam,_,Is_Large),5) :- assigned(Exam,Room,Period), period(Period,_,_,_,Is_Late,_), exam(Exam,_,_), Is_Large = "large", Is_Late = "late".

Full program saved to Results/experiment/examination_timetabling_ft_qwen_mlx_k=3_n=0_20260110_181357.lp
================================================================================
Send prompt for semantic check:
Generated ASP code:
event(Event).

room(Room, Capacity).

timeslot(Day, Hour).

student(Student, Event).

feature_requirement(FEATURE, EVENT).

room_feature(FEATURE, ROOM).

precedence_constraint(EVENT1, EVENT2).

timeslot_constraint(EVENT, TIMESLOT).

Intended semantics:
- Events: A set of N events.
    - Variables: none
- Rooms: A set of rooms in which the events can be scheduled.
    - Variables: capacity
- Timeslots: A set of timeslots in which the events can be scheduled.
    - Variables: day, hour
- Students: A set of students that are attending a set of events.
    - Variables: event
- Feature Requirements: A set of features which events may have.
    - Variables: feature, event
- Room Features: A set of features which rooms may have.
    - Variables: feature, room
- Precedence Constraints: A set of constraints that specify if some events must be scheduled before other events.
    - Variables: event1, event2
- Timeslot Constraints: A set of constraints for each event that specify the timeslots in which the event can be scheduled.
    - Variables: event, timeslot


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
event(Event).

room(Room, Capacity).

timeslot(Day, Hour).

student(Student, Event).

feature_requirement(FEATURE, EVENT).

room_feature(FEATURE, ROOM).

precedence_constraint(EVENT1, EVENT2).

timeslot_constraint(EVENT, TIMESLOT).

####################################################################################
Instance Template:
event(Event).

room(Room, Capacity).

timeslot(Day, Hour).

student(Student, Event).

feature_requirement(FEATURE, EVENT).

room_feature(FEATURE, ROOM).

precedence_constraint(EVENT1, EVENT2).

timeslot_constraint(EVENT, TIMESLOT).
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).

Intended semantics:
An assignment of events to timeslots and rooms.
    - Variables: timeslot, room


Semantic check result:
{
  "match": "false",
  "reason": "The code requires each event to be assigned to exactly one timeslot and room, but it does not enforce that each timeslot and room combination must be used."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).

####################################################################################


Generator
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).


Hard Constraints

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- student(Student, Event1), student(Student, Event2), Event1 != Event2, timeslot(Timeslot1), timeslot(Timeslot2), assigned(Event1, Timeslot1, _), assigned(Event2, Timeslot2, _), Timeslot1 < Timeslot2, Timeslot1 + 1 = Timeslot2.

Intended semantics:- Students must not have overlapping events.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- student(Student, Event1), student(Student, Event2), Event1 != Event2, timeslot(Timeslot1), timeslot(Timeslot2), assigned(Event1, Timeslot1, _), assigned(Event2, Timeslot2, _), Timeslot1 < Timeslot2, Timeslot1 + 1 = Timeslot2.

####################################################################################
- Students must not have overlapping events.
:- student(Student, Event1), student(Student, Event2), Event1 != Event2, timeslot(Timeslot1), timeslot(Timeslot2), assigned(Event1, Timeslot1, _), assigned(Event2, Timeslot2, _), Timeslot1 < Timeslot2, Timeslot1 + 1 = Timeslot2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- room(Room, Capacity), #count{Event : assigned(Event, Timeslot, Room), timeslot_constraint(Event, Timeslot)} > Capacity.

Intended semantics:- The amount of students in a room at the same time should not exceed the capacity of the room
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- room(Room, Capacity), #count{Event : assigned(Event, Timeslot, Room), timeslot_constraint(Event, Timeslot)} > Capacity.

####################################################################################
- The amount of students in a room at the same time should not exceed the capacity of the room
 
:- room(Room, Capacity), #count{Event : assigned(Event, Timeslot, Room), timeslot_constraint(Event, Timeslot)} > Capacity.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), not feature_requirement(FEATURE, Event).

Intended semantics:- Rooms must have the required features for the event.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), not feature_requirement(FEATURE, Event).

####################################################################################
- Rooms must have the required features for the event.
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), not feature_requirement(FEATURE, Event).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), Event1 != Event2.

Intended semantics:- There must be no more than one event in a room at a time.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), Event1 != Event2.

####################################################################################
- There must be no more than one event in a room at a time.
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), Event1 != Event2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event, Timeslot, Room), not timeslot_constraint(Event, Timeslot).

Intended semantics:- Events may only be scheduled in designated timeslots.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event, Timeslot, Room), not timeslot_constraint(Event, Timeslot).

####################################################################################
- Events may only be scheduled in designated timeslots.
:- assigned(Event, Timeslot, Room), not timeslot_constraint(Event, Timeslot).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event1, _, _, _), assigned(Event2, _, _, _), precedence_constraint(Event1, Event2), not assigned(Event1, Timeslot1, _, _), not assigned(Event2, Timeslot2, _, _), Timeslot1 < Timeslot2.

Intended semantics:- Some events must be scheduled in a specific order.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that if two events have a precedence constraint, they must be assigned to different timeslots, which is not the same as enforcing a specific order."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event1, _, _, _), assigned(Event2, _, _, _), precedence_constraint(Event1, Event2), not assigned(Event1, Timeslot1, _, _), not assigned(Event2, Timeslot2, _, _), Timeslot1 < Timeslot2.

####################################################################################
- Some events must be scheduled in a specific order.
:- assigned(Event1, _, _, _), assigned(Event2, _, _, _), precedence_constraint(Event1, Event2), not assigned(Event1, Timeslot1, _, _), not assigned(Event2, Timeslot2, _, _), Timeslot1 < Timeslot2.


Soft Constraints:

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("LastTimeslot",student(Student,Event),1) :- student(Student,Event), assigned(Event,Timeslot,Room), timeslot(Day,Hour), timeslot_constraint(Event,Timeslot), Hour = 8 + 4.

Intended semantics:- Students should not have events scheduled in the last timeslot of the day
 Penalty: 1 for each student with an event in the last timeslot of the day.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("LastTimeslot",student(Student,Event),1) :- student(Student,Event), assigned(Event,Timeslot,Room), timeslot(Day,Hour), timeslot_constraint(Event,Timeslot), Hour = 8 + 4.

####################################################################################
- Students should not have events scheduled in the last timeslot of the day
 Penalty: 1 for each student with an event in the last timeslot of the day.
penalty("LastTimeslot",student(Student,Event),1) :- student(Student,Event), assigned(Event,Timeslot,Room), timeslot(Day,Hour), timeslot_constraint(Event,Timeslot), Hour = 8 + 4.

================================================================================
Initial response with syntax error:
penalty("ConsecutiveTimeslots",Student,Penalty) :- student(Student,Event), 3 = #count{Timeslot : assigned(Event,Timeslot), Timeslot2 : assigned(Event,Timeslot2), Timeslot3 : assigned(Event,Timeslot3), Timeslot < Timeslot2, Timeslot2 < Timeslot3, Timeslot < Timeslot3 }.

Error: <string>:1:134-135: error: syntax error, unexpected :

 Starting syntax repair attempts...
================================================================================
--------------------------------------------------------------------------------
Correction attempt 1:
penalty("ConsecutiveTimeslots", Student, Penalty) :- student(Student, Event), 3 = #count{ Timeslot : assigned(Event, Timeslot), Timeslot2 : assigned(Event, Timeslot2), Timeslot3 : assigned(Event, Timeslot3), Timeslot < Timeslot2, Timeslot2 < Timeslot3, Timeslot < Timeslot3 }.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:139-140: error: syntax error, unexpected :

--------------------------------------------------------------------------------
Correction attempt 2:
penalty("ConsecutiveTimeslots", Student, Penalty) :- student(Student, Event), 3 = #count{ Timeslot : assigned(Event, Timeslot), Timeslot2 : assigned(Event, Timeslot2), Timeslot3 : assigned(Event, Timeslot3), Timeslot < Timeslot2, Timeslot2 < Timeslot3, Timeslot < Timeslot3 }.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:139-140: error: syntax error, unexpected :

--------------------------------------------------------------------------------
Correction attempt 3:
penalty("ConsecutiveTimeslots", Student, Penalty) :- student(Student, Event), 3 = #count{ Timeslot : assigned(Event, Timeslot), Timeslot2 : assigned(Event, Timeslot2), Timeslot3 : assigned(Event, Timeslot3), Timeslot < Timeslot2, Timeslot2 < Timeslot3, Timeslot < Timeslot3 }.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:139-140: error: syntax error, unexpected :

================================================================================
Syntax errors remain after repair attempts. Cannot proceed to semantic checking.
Total syntax errors: 1

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED , BUT UNSUCCESSFULLY
INITIAL RESPONSE:
penalty("ConsecutiveTimeslots",Student,Penalty) :- student(Student,Event), 3 = #count{Timeslot : assigned(Event,Timeslot), Timeslot2 : assigned(Event,Timeslot2), Timeslot3 : assigned(Event,Timeslot3), Timeslot < Timeslot2, Timeslot2 < Timeslot3, Timeslot < Timeslot3 }.

REPAIRED RESPONSE:
penalty("ConsecutiveTimeslots", Student, Penalty) :- student(Student, Event), 3 = #count{ Timeslot : assigned(Event, Timeslot), Timeslot2 : assigned(Event, Timeslot2), Timeslot3 : assigned(Event, Timeslot3), Timeslot < Timeslot2, Timeslot2 < Timeslot3, Timeslot < Timeslot3 }.

Total syntax errors remaining after repair attempts: 1

Semantics correct (according to LLM): False

####################################################################################
- Students should not have events scheduled in three or more consecutive timeslots
 Penalty: 1 for each student with an event in three or more consecutive timeslots.
penalty("ConsecutiveTimeslots", Student, Penalty) :- student(Student, Event), 3 = #count{ Timeslot : assigned(Event, Timeslot), Timeslot2 : assigned(Event, Timeslot2), Timeslot3 : assigned(Event, Timeslot3), Timeslot < Timeslot2, Timeslot2 < Timeslot3, Timeslot < Timeslot3 }.

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("SingleEventPerDay", student(Student), 1) :- #count{Event : assigned(Event, Timeslot, Room), student(Student), timeslot(Day, _), timeslot_constraint(Event, Timeslot)} = 1.

Intended semantics:- Students should not have exactly one event in a day
 Penalty: 1 for each time a student has exactly one event in a day.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for students having exactly one event in a day, which is the opposite of the intended semantics."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("SingleEventPerDay", student(Student), 1) :- #count{Event : assigned(Event, Timeslot, Room), student(Student), timeslot(Day, _), timeslot_constraint(Event, Timeslot)} = 1.

####################################################################################
- Students should not have exactly one event in a day
 Penalty: 1 for each time a student has exactly one event in a day.
 
penalty("SingleEventPerDay", student(Student), 1) :- #count{Event : assigned(Event, Timeslot, Room), student(Student), timeslot(Day, _), timeslot_constraint(Event, Timeslot)} = 1.

Full program saved to Results/experiment/post_enrollment_based_course_time_tabling_ft_qwen_mlx_k=3_n=0_20260110_181912.lp
================================================================================
Send prompt for semantic check:
Generated ASP code:
nurse(Nurse).

day(Day).

shift_type(Shift_type, Duration).

nurse_requirement(Shift, Min, Max).

work_requirement(Nurse, Min, Max).

shift_requirement(Shift_type, Min, Max, Preferred).

Intended semantics:
Instance Variables:
- Nurses: A set of numbered nurses.
    - Variables: none
- Days: A set of numbered days.
    - Variables: none
- Shift_types: A set of shifts types that the nurses can be assigned to.
    - Variables: shift_type, duration
- nurse_requirements: Min and max number of nurses required for each shift type.
    - Variables: shift, min, max
- work_requirements: Min and max number of hours each nurse must work.
    - Variables: min, max
- shift_requirements: For each shift type, there is a minimum and maximum number of times a nurse must work this shift. There is also a preferred number of each shift type.
    - Variables: shift_type, min, max, preferred


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
nurse(Nurse).

day(Day).

shift_type(Shift_type, Duration).

nurse_requirement(Shift, Min, Max).

work_requirement(Nurse, Min, Max).

shift_requirement(Shift_type, Min, Max, Preferred).

####################################################################################
Instance Template:
nurse(Nurse).

day(Day).

shift_type(Shift_type, Duration).

nurse_requirement(Shift, Min, Max).

work_requirement(Nurse, Min, Max).

shift_requirement(Shift_type, Min, Max, Preferred).
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { works(Nurse, Shift_type, DDay) : day(DDay) } 1 :- nurse(Nurse).

Intended semantics:Goal:
An assignment of nurses to shifts_types and days. There must be one assignment for each nurse and each day.
    - Variables: nurse, shift, day


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
1 { works(Nurse, Shift_type, DDay) : day(DDay) } 1 :- nurse(Nurse).

####################################################################################


Generator
1 { works(Nurse, Shift_type, DDay) : day(DDay) } 1 :- nurse(Nurse).


Hard Constraints

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- shift_type(Shift_type, _), day(DDay), #count{Nurse : works(Nurse, Shift_type, DDay)} < Min, nurse_requirement(Shift_type, Min, _).
:- shift_type(Shift_type, _), day(DDay), #count{Nurse : works(Nurse, Shift_type, DDay)} > Max, nurse_requirement(Shift_type, _, Max).

Intended semantics:- Every day, the number of nurses assigned to each shift must be between the specified minimum and maximum
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- shift_type(Shift_type, _), day(DDay), #count{Nurse : works(Nurse, Shift_type, DDay)} < Min, nurse_requirement(Shift_type, Min, _).
:- shift_type(Shift_type, _), day(DDay), #count{Nurse : works(Nurse, Shift_type, DDay)} > Max, nurse_requirement(Shift_type, _, Max).

####################################################################################
- Every day, the number of nurses assigned to each shift must be between the specified minimum and maximum
 
:- shift_type(Shift_type, _), day(DDay), #count{Nurse : works(Nurse, Shift_type, DDay)} < Min, nurse_requirement(Shift_type, Min, _).
:- shift_type(Shift_type, _), day(DDay), #count{Nurse : works(Nurse, Shift_type, DDay)} > Max, nurse_requirement(Shift_type, _, Max).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #sum{Duration, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} > Max, work_requirement(Nurse, _, Max).

Intended semantics:- Each nurse must work at least the specified minimum and at most the specified maximum number of hours
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces global minimum and maximum constraints for all nurses combined, rather than individual constraints for each nurse."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- nurse(Nurse), #sum{Duration, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} > Max, work_requirement(Nurse, _, Max).

####################################################################################
- Each nurse must work at least the specified minimum and at most the specified maximum number of hours
 
:- nurse(Nurse), #sum{Duration, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} > Max, work_requirement(Nurse, _, Max).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

Intended semantics:- Nurses must have exactly 30 days of vacation.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each nurse must have exactly 30 days of vacation, not exactly 31 days."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

####################################################################################
- Nurses must have exactly 30 days of vacation.
 
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- works(Nurse, night, DDay), works(Nurse, morning, DDay+1).
:- works(Nurse, night, DDay), works(Nurse, afternoon, DDay+1).

Intended semantics:- If a nurse works a night shift, they can not work a morning or afternoon shift the next day

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- works(Nurse, night, DDay), works(Nurse, morning, DDay+1).
:- works(Nurse, night, DDay), works(Nurse, afternoon, DDay+1).

####################################################################################
- If a nurse works a night shift, they can not work a morning or afternoon shift the next day
:- works(Nurse, night, DDay), works(Nurse, morning, DDay+1).
:- works(Nurse, night, DDay), works(Nurse, afternoon, DDay+1).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- works(Nurse, "afternoon", DDay), works(Nurse, "morning", DDay + 1).

Intended semantics:- If a nurse works an afternoon shift, they cannot work a morning shift the next day.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- works(Nurse, "afternoon", DDay), works(Nurse, "morning", DDay + 1).

####################################################################################
- If a nurse works an afternoon shift, they cannot work a morning shift the next day.
:- works(Nurse, "afternoon", DDay), works(Nurse, "morning", DDay + 1).

================================================================================
Initial response with syntax error:
:- nurse(Nurse), N = #count{DDay : day(DDay), works(Nurse, "rest", DDay)}, N < 2, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay < #max{DDay : day(DDay)}}, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay > #min{DDay : day(DDay)}}.

Error: <string>:1:147-151: error: syntax error, unexpected #max

 Starting syntax repair attempts...
================================================================================
--------------------------------------------------------------------------------
Correction attempt 1:
:- nurse(Nurse), N = #count{DDay : day(DDay), works(Nurse, "rest", DDay)}, N < 2, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay < #max{DDay : day(DDay)}}, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay > #min{DDay : day(DDay)}}.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:147-151: error: syntax error, unexpected #max

--------------------------------------------------------------------------------
Correction attempt 2:
:- nurse(Nurse), N = #count{DDay : day(DDay), works(Nurse, "rest", DDay)}, N < 2, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay < #max{DDay : day(DDay)}}, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay > #min{DDay : day(DDay)}}.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:147-151: error: syntax error, unexpected #max

--------------------------------------------------------------------------------
Correction attempt 3:
:- nurse(Nurse), N = #count{DDay : day(DDay), works(Nurse, "rest", DDay)}, N < 2, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay < #max{DDay : day(DDay)}}, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay > #min{DDay : day(DDay)}}.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:147-151: error: syntax error, unexpected #max

================================================================================
Syntax errors remain after repair attempts. Cannot proceed to semantic checking.
Total syntax errors: 1

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- nurse(Nurse), N = #count{DDay : day(DDay), works(Nurse, "rest", DDay)}, N < 2, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay < #max{DDay : day(DDay)}}, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay > #min{DDay : day(DDay)}}.

####################################################################################
- Each nurse has at least two ordinary rest days for every window of fourteen days
 
:- nurse(Nurse), N = #count{DDay : day(DDay), works(Nurse, "rest", DDay)}, N < 2, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay < #max{DDay : day(DDay)}}, N = #count{DDay : day(DDay), works(Nurse, "rest", DDay), DDay > #min{DDay : day(DDay)}}.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), works(Nurse, night, DDay+2), not works(Nurse, special_rest, DDay+1).

Intended semantics:- Nurses working on two consecutive nights deserve one special_rest day in addition to the ordinary rest days

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a specific sequence of shifts without considering the general pattern. It requires a special_rest day after the second consecutive night shift, which is not the intended semantics of rewarding nurses for any two consecutive nights with a special_rest day."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), works(Nurse, night, DDay+2), not works(Nurse, special_rest, DDay+1).

####################################################################################
- Nurses working on two consecutive nights deserve one special_rest day in addition to the ordinary rest days
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), works(Nurse, night, DDay+2), not works(Nurse, special_rest, DDay+1).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #sum{Hours, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} < Min, work_requirement(Nurse, Min, Max).
:- nurse(Nurse), #sum{Hours, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} > Max, work_requirement(Nurse, Min, Max).

Intended semantics:- The total number of hours worked by each nurse must be between the specified minimum and maximum
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- nurse(Nurse), #sum{Hours, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} < Min, work_requirement(Nurse, Min, Max).
:- nurse(Nurse), #sum{Hours, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} > Max, work_requirement(Nurse, Min, Max).

####################################################################################
- The total number of hours worked by each nurse must be between the specified minimum and maximum
 
:- nurse(Nurse), #sum{Hours, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} < Min, work_requirement(Nurse, Min, Max).
:- nurse(Nurse), #sum{Hours, Shift_type, DDay: works(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} > Max, work_requirement(Nurse, Min, Max).


Soft Constraints:

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("shift_preference", works(Nurse, Shift_type, DDay), 1) :- works(Nurse, Shift_type, DDay), shift_requirement(Shift_type, Min, Max, Preferred), Preferred != 0, Preferred != Min + Max.
penalty("shift_preference", works(Nurse, Shift_type, DDay), 1) :- works(Nurse, Shift_type, DDay), shift_requirement(Shift_type, Min, Max, Preferred), Preferred != 0, Preferred != Min + Max.

Intended semantics:- Nurses work the preferred number of shifts for each shift type
 Penalty: 1 for each shift below the preferred number
 Penalty: 1 for each shift above the preferred number
 

Semantic check result:
{
  "match": "false",
  "reason": "The code incorrectly uses 'Preferred !=  t' which likely means 'Preferred != Min + Max', but the syntax is incorrect. It should be 'Preferred != Min + Max'. Additionally, the penalty is defined twice with the same condition, which is redundant and might cause unexpected behavior."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("shift_preference", works(Nurse, Shift_type, DDay), 1) :- works(Nurse, Shift_type, DDay), shift_requirement(Shift_type, Min, Max, Preferred), Preferred != 0, Preferred != Min + Max.
penalty("shift_preference", works(Nurse, Shift_type, DDay), 1) :- works(Nurse, Shift_type, DDay), shift_requirement(Shift_type, Min, Max, Preferred), Preferred != 0, Preferred != Min + Max.

####################################################################################
- Nurses work the preferred number of shifts for each shift type
 Penalty: 1 for each shift below the preferred number
 Penalty: 1 for each shift above the preferred number
 
penalty("shift_preference", works(Nurse, Shift_type, DDay), 1) :- works(Nurse, Shift_type, DDay), shift_requirement(Shift_type, Min, Max, Preferred), Preferred != 0, Preferred != Min + Max.
penalty("shift_preference", works(Nurse, Shift_type, DDay), 1) :- works(Nurse, Shift_type, DDay), shift_requirement(Shift_type, Min, Max, Preferred), Preferred != 0, Preferred != Min + Max.

Full program saved to Results/experiment/nurse_scheduling_ft_qwen_mlx_k=3_n=0_20260110_182321.lp
================================================================================
Send prompt for semantic check:
Generated ASP code:
team(Team).

venue(Venue).

availability(Venue, Date).

Intended semantics:
- Teams: a set of teams that plays in the league
    - Variables: team
- Venues: a set of venues that is available
    - Variables: venues
- Availabilities: A set of dates at which a given venue is available 
    - Variables: venues, dates
    

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
team(Team).

venue(Venue).

availability(Venue, Date).

####################################################################################
Instance Template:
team(Team).

venue(Venue).

availability(Venue, Date).
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { game(Team1, Team2, Venue, Gameday) : team(Team1), team(Team2), venue(Venue), gameday(Gameday) }.

Intended semantics:
An assignment of games to pairs of teams, venues and game days.
    - Variables: team1, team2, venue, gameday

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces exactly one game for each combination of teams, venue, and gameday, whereas the intended semantics only requires an assignment without specifying uniqueness."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
1 { game(Team1, Team2, Venue, Gameday) : team(Team1), team(Team2), venue(Venue), gameday(Gameday) }.

####################################################################################


Generator
1 { game(Team1, Team2, Venue, Gameday) : team(Team1), team(Team2), venue(Venue), gameday(Gameday) }.


Hard Constraints

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- game(Team1, Team2, _, Gameday), game(Team1, Team2, _, Gameday2), Gameday != Gameday2.

Intended semantics:- No team plays each other more than once at different gamedays.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- game(Team1, Team2, _, Gameday), game(Team1, Team2, _, Gameday2), Gameday != Gameday2.

####################################################################################
- No team plays each other more than once at different gamedays.
:- game(Team1, Team2, _, Gameday), game(Team1, Team2, _, Gameday2), Gameday != Gameday2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }.

Intended semantics:- Every team plays every other team at least once.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that for any two different teams, there must be at least one game where each team plays against the other, but it does not ensure that every team plays every other team at least once globally."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }.

####################################################################################
- Every team plays every other team at least once.
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- game(Team, Team, _, _).

Intended semantics:- No team plays itself.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- game(Team, Team, _, _).

####################################################################################
- No team plays itself.
:- game(Team, Team, _, _).


Soft Constraints:

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

Intended semantics:- Teams should play at the same venue as little as possible.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for any game where two different teams play against each other at the same venue on the same day, which is not aligned with the intended semantics of minimizing venue changes for each team."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

####################################################################################
- Teams should play at the same venue as little as possible.
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

Full program saved to Results/experiment/sports scheduling_ft_qwen_mlx_k=3_n=0_20260110_182849.lp
