CUDA not detected. Using MPS/CPU configuration (Quantization disabled).
Using Huggingface Hub Token from .env
Using dtype: torch.bfloat16
loading local model and tokenizer...
Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]Loading weights:   0%|          | 1/339 [00:00<00:00, 4999.17it/s, Materializing param=lm_head.weight]Loading weights:   0%|          | 1/339 [00:00<00:00, 4017.53it/s, Materializing param=lm_head.weight]Loading weights:   1%|          | 2/339 [00:00<01:58,  2.84it/s, Materializing param=lm_head.weight]  Loading weights:   1%|          | 2/339 [00:00<01:58,  2.84it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 2/339 [00:00<01:58,  2.84it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 3/339 [00:01<02:12,  2.53it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 3/339 [00:01<02:12,  2.53it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 3/339 [00:01<02:12,  2.53it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 4/339 [00:01<02:12,  2.53it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  Loading weights:   1%|          | 4/339 [00:01<02:12,  2.53it/s, Materializing param=model.layers.0.mlp.down_proj.weight]Loading weights:   1%|▏         | 5/339 [00:01<02:11,  2.53it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   1%|▏         | 5/339 [00:01<02:11,  2.53it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|▏         | 6/339 [00:01<00:54,  6.09it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|▏         | 6/339 [00:01<00:54,  6.09it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  Loading weights:   2%|▏         | 6/339 [00:01<00:54,  6.09it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   2%|▏         | 7/339 [00:01<00:54,  6.09it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 7/339 [00:01<00:54,  6.09it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 8/339 [00:01<00:54,  6.09it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          Loading weights:   2%|▏         | 8/339 [00:01<00:54,  6.09it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]Loading weights:   3%|▎         | 9/339 [00:01<00:54,  6.09it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 9/339 [00:01<00:54,  6.09it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 10/339 [00:01<00:54,  6.09it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 10/339 [00:01<00:54,  6.09it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 11/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]  Loading weights:   3%|▎         | 11/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]Loading weights:   4%|▎         | 12/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▎         | 12/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▍         | 13/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  Loading weights:   4%|▍         | 13/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]Loading weights:   4%|▍         | 14/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 14/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 15/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.1.input_layernorm.weight] Loading weights:   4%|▍         | 15/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   5%|▍         | 16/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  Loading weights:   5%|▍         | 16/339 [00:01<00:53,  6.09it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   5%|▌         | 17/339 [00:01<00:15, 21.33it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   5%|▌         | 17/339 [00:01<00:15, 21.33it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 17/339 [00:01<00:15, 21.33it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 18/339 [00:01<00:15, 21.33it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  Loading weights:   5%|▌         | 18/339 [00:01<00:15, 21.33it/s, Materializing param=model.layers.1.mlp.up_proj.weight]Loading weights:   6%|▌         | 19/339 [00:01<00:15, 21.33it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 19/339 [00:01<00:15, 21.33it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 20/339 [00:01<00:14, 21.33it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          Loading weights:   6%|▌         | 20/339 [00:01<00:14, 21.33it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]Loading weights:   6%|▌         | 21/339 [00:01<00:14, 21.33it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▌         | 21/339 [00:01<00:14, 21.33it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▋         | 22/339 [00:01<00:12, 26.09it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▋         | 22/339 [00:01<00:12, 26.09it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   6%|▋         | 22/339 [00:01<00:12, 26.09it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   7%|▋         | 23/339 [00:01<00:12, 26.09it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  Loading weights:   7%|▋         | 23/339 [00:01<00:12, 26.09it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]Loading weights:   7%|▋         | 24/339 [00:01<00:12, 26.09it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 24/339 [00:01<00:12, 26.09it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 25/339 [00:01<00:12, 26.09it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  Loading weights:   7%|▋         | 25/339 [00:01<00:12, 26.09it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]Loading weights:   8%|▊         | 26/339 [00:01<00:11, 26.09it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 26/339 [00:01<00:11, 26.09it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 27/339 [00:01<00:11, 26.09it/s, Materializing param=model.layers.2.input_layernorm.weight] Loading weights:   8%|▊         | 27/339 [00:01<00:11, 26.09it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:   8%|▊         | 28/339 [00:01<00:11, 26.09it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  Loading weights:   8%|▊         | 28/339 [00:01<00:11, 26.09it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:   9%|▊         | 29/339 [00:01<00:11, 26.09it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▊         | 29/339 [00:01<00:11, 26.09it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▉         | 30/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▉         | 30/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  Loading weights:   9%|▉         | 30/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.mlp.up_proj.weight]Loading weights:   9%|▉         | 31/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 31/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 32/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          Loading weights:   9%|▉         | 32/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]Loading weights:  10%|▉         | 33/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|▉         | 33/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|█         | 34/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 34/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 35/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  Loading weights:  10%|█         | 35/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]Loading weights:  11%|█         | 36/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 36/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 37/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  Loading weights:  11%|█         | 37/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]Loading weights:  11%|█         | 38/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  11%|█         | 38/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  12%|█▏        | 39/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.3.input_layernorm.weight] Loading weights:  12%|█▏        | 39/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  12%|█▏        | 40/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  Loading weights:  12%|█▏        | 40/339 [00:01<00:08, 34.79it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 42/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  Loading weights:  12%|█▏        | 42/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.mlp.up_proj.weight]Loading weights:  13%|█▎        | 43/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 43/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 44/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          Loading weights:  13%|█▎        | 44/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]Loading weights:  13%|█▎        | 45/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  13%|█▎        | 45/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▍        | 47/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  Loading weights:  14%|█▍        | 47/339 [00:01<00:06, 47.57it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]Loading weights:  14%|█▍        | 48/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]Loading weights:  14%|█▍        | 48/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 48/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 49/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  Loading weights:  14%|█▍        | 49/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]Loading weights:  15%|█▍        | 50/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▍        | 50/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▌        | 51/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.4.input_layernorm.weight] Loading weights:  15%|█▌        | 51/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  15%|█▌        | 52/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  Loading weights:  15%|█▌        | 52/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:01<00:05, 50.15it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 54/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 54/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  Loading weights:  16%|█▌        | 54/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.mlp.up_proj.weight]Loading weights:  16%|█▌        | 55/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  16%|█▌        | 55/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  17%|█▋        | 56/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          Loading weights:  17%|█▋        | 56/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]Loading weights:  17%|█▋        | 57/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 57/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 59/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  Loading weights:  17%|█▋        | 59/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]Loading weights:  18%|█▊        | 60/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 60/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 61/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  Loading weights:  18%|█▊        | 61/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]Loading weights:  18%|█▊        | 62/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  18%|█▊        | 62/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  19%|█▊        | 63/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.5.input_layernorm.weight] Loading weights:  19%|█▊        | 63/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  19%|█▉        | 64/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  Loading weights:  19%|█▉        | 64/339 [00:02<00:05, 50.50it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 66/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  Loading weights:  19%|█▉        | 66/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.mlp.up_proj.weight]Loading weights:  20%|█▉        | 67/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|█▉        | 67/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|██        | 68/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          Loading weights:  20%|██        | 68/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]Loading weights:  20%|██        | 69/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  20%|██        | 69/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  21%|██        | 70/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 70/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 71/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  Loading weights:  21%|██        | 71/339 [00:02<00:04, 59.97it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]Loading weights:  21%|██        | 72/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]Loading weights:  21%|██        | 72/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  21%|██        | 72/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  22%|██▏       | 73/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  Loading weights:  22%|██▏       | 73/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]Loading weights:  22%|██▏       | 74/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 74/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 75/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.6.input_layernorm.weight] Loading weights:  22%|██▏       | 75/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  22%|██▏       | 76/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  Loading weights:  22%|██▏       | 76/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 78/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  Loading weights:  23%|██▎       | 78/339 [00:02<00:04, 59.32it/s, Materializing param=model.layers.6.mlp.up_proj.weight]Loading weights:  23%|██▎       | 79/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.mlp.up_proj.weight]Loading weights:  23%|██▎       | 79/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  23%|██▎       | 79/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  24%|██▎       | 80/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          Loading weights:  24%|██▎       | 80/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]Loading weights:  24%|██▍       | 81/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 81/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 83/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  Loading weights:  24%|██▍       | 83/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]Loading weights:  25%|██▍       | 84/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▍       | 84/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▌       | 85/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  Loading weights:  25%|██▌       | 85/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]Loading weights:  25%|██▌       | 86/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  25%|██▌       | 86/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  26%|██▌       | 87/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.7.input_layernorm.weight] Loading weights:  26%|██▌       | 87/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  26%|██▌       | 88/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  Loading weights:  26%|██▌       | 88/339 [00:02<00:04, 52.66it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  27%|██▋       | 90/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  Loading weights:  27%|██▋       | 90/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.mlp.up_proj.weight]Loading weights:  27%|██▋       | 91/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 91/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 92/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          Loading weights:  27%|██▋       | 92/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]Loading weights:  27%|██▋       | 93/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  27%|██▋       | 93/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 95/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  Loading weights:  28%|██▊       | 95/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]Loading weights:  28%|██▊       | 96/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  28%|██▊       | 96/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  29%|██▊       | 97/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  Loading weights:  29%|██▊       | 97/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]Loading weights:  29%|██▉       | 98/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 98/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 99/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.8.input_layernorm.weight] Loading weights:  29%|██▉       | 99/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  29%|██▉       | 100/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.8.mlp.down_proj.weight] Loading weights:  29%|██▉       | 100/339 [00:02<00:04, 58.13it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|███       | 102/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  Loading weights:  30%|███       | 102/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.mlp.up_proj.weight]Loading weights:  30%|███       | 103/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  30%|███       | 103/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  31%|███       | 104/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          Loading weights:  31%|███       | 104/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]Loading weights:  31%|███       | 105/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███       | 105/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  32%|███▏      | 107/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  Loading weights:  32%|███▏      | 107/339 [00:02<00:03, 66.36it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]Loading weights:  32%|███▏      | 108/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]Loading weights:  32%|███▏      | 108/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 108/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 109/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  Loading weights:  32%|███▏      | 109/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]Loading weights:  32%|███▏      | 110/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  32%|███▏      | 110/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  33%|███▎      | 111/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.9.input_layernorm.weight] Loading weights:  33%|███▎      | 111/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  33%|███▎      | 112/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  Loading weights:  33%|███▎      | 112/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  34%|███▎      | 114/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  Loading weights:  34%|███▎      | 114/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.9.mlp.up_proj.weight]Loading weights:  34%|███▍      | 115/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.mlp.up_proj.weight]Loading weights:  34%|███▍      | 115/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 115/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 116/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          Loading weights:  34%|███▍      | 116/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]Loading weights:  35%|███▍      | 117/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 117/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▌      | 119/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  Loading weights:  35%|███▌      | 119/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]Loading weights:  35%|███▌      | 120/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  35%|███▌      | 120/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  36%|███▌      | 121/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  Loading weights:  36%|███▌      | 121/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]Loading weights:  36%|███▌      | 122/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▌      | 122/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▋      | 123/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  36%|███▋      | 123/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  37%|███▋      | 124/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  Loading weights:  37%|███▋      | 124/339 [00:03<00:04, 52.14it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 126/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  Loading weights:  37%|███▋      | 126/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.mlp.up_proj.weight]Loading weights:  37%|███▋      | 127/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  37%|███▋      | 127/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  38%|███▊      | 128/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          Loading weights:  38%|███▊      | 128/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]Loading weights:  38%|███▊      | 129/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 129/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  39%|███▊      | 131/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  Loading weights:  39%|███▊      | 131/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]Loading weights:  39%|███▉      | 132/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 132/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 133/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  Loading weights:  39%|███▉      | 133/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]Loading weights:  40%|███▉      | 134/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 134/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 135/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.11.input_layernorm.weight] Loading weights:  40%|███▉      | 135/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  40%|████      | 136/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  Loading weights:  40%|████      | 136/339 [00:03<00:03, 56.89it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  40%|████      | 137/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  40%|████      | 137/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  40%|████      | 137/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  41%|████      | 138/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  Loading weights:  41%|████      | 138/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.mlp.up_proj.weight]Loading weights:  41%|████      | 139/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████      | 139/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████▏     | 140/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          Loading weights:  41%|████▏     | 140/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]Loading weights:  42%|████▏     | 141/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 141/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 143/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  Loading weights:  42%|████▏     | 143/339 [00:03<00:03, 65.14it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]Loading weights:  42%|████▏     | 144/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]Loading weights:  42%|████▏     | 144/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  42%|████▏     | 144/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  43%|████▎     | 145/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  Loading weights:  43%|████▎     | 145/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]Loading weights:  43%|████▎     | 146/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 146/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 147/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.12.input_layernorm.weight] Loading weights:  43%|████▎     | 147/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  44%|████▎     | 148/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  Loading weights:  44%|████▎     | 148/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 150/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  Loading weights:  44%|████▍     | 150/339 [00:03<00:03, 62.96it/s, Materializing param=model.layers.12.mlp.up_proj.weight]Loading weights:  45%|████▍     | 151/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.mlp.up_proj.weight]Loading weights:  45%|████▍     | 151/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 151/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 152/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          Loading weights:  45%|████▍     | 152/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]Loading weights:  45%|████▌     | 153/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 153/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  46%|████▌     | 155/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  Loading weights:  46%|████▌     | 155/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]Loading weights:  46%|████▌     | 156/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▌     | 156/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▋     | 157/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  Loading weights:  46%|████▋     | 157/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]Loading weights:  47%|████▋     | 158/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 158/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 159/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.13.input_layernorm.weight] Loading weights:  47%|████▋     | 159/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  47%|████▋     | 160/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  Loading weights:  47%|████▋     | 160/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:03<00:03, 54.21it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  48%|████▊     | 162/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  48%|████▊     | 162/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  Loading weights:  48%|████▊     | 162/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.mlp.up_proj.weight]Loading weights:  48%|████▊     | 163/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 163/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 164/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          Loading weights:  48%|████▊     | 164/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]Loading weights:  49%|████▊     | 165/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▊     | 165/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 167/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  Loading weights:  49%|████▉     | 167/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]Loading weights:  50%|████▉     | 168/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 168/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 169/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  Loading weights:  50%|████▉     | 169/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]Loading weights:  50%|█████     | 170/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 170/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 171/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.14.input_layernorm.weight] Loading weights:  50%|█████     | 171/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  51%|█████     | 172/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  Loading weights:  51%|█████     | 172/339 [00:03<00:02, 61.14it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  51%|█████     | 173/339 [00:03<00:02, 65.87it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  51%|█████     | 173/339 [00:03<00:02, 65.87it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████     | 173/339 [00:03<00:02, 65.87it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████▏    | 174/339 [00:03<00:02, 65.87it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  Loading weights:  51%|█████▏    | 174/339 [00:03<00:02, 65.87it/s, Materializing param=model.layers.14.mlp.up_proj.weight]Loading weights:  52%|█████▏    | 175/339 [00:04<00:02, 65.87it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 175/339 [00:04<00:02, 65.87it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 176/339 [00:04<00:02, 65.87it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          Loading weights:  52%|█████▏    | 176/339 [00:04<00:02, 65.87it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]Loading weights:  52%|█████▏    | 177/339 [00:04<00:02, 65.87it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  52%|█████▏    | 177/339 [00:04<00:02, 65.87it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:04<00:02, 65.87it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:04<00:02, 65.87it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 179/339 [00:04<00:02, 65.87it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  Loading weights:  53%|█████▎    | 179/339 [00:04<00:02, 65.87it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]Loading weights:  53%|█████▎    | 180/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]Loading weights:  53%|█████▎    | 180/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 180/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 181/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  Loading weights:  53%|█████▎    | 181/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]Loading weights:  54%|█████▎    | 182/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▎    | 182/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▍    | 183/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.15.input_layernorm.weight] Loading weights:  54%|█████▍    | 183/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  54%|█████▍    | 184/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  Loading weights:  54%|█████▍    | 184/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 186/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  Loading weights:  55%|█████▍    | 186/339 [00:04<00:02, 63.02it/s, Materializing param=model.layers.15.mlp.up_proj.weight]Loading weights:  55%|█████▌    | 187/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.mlp.up_proj.weight]Loading weights:  55%|█████▌    | 187/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 187/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 188/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          Loading weights:  55%|█████▌    | 188/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]Loading weights:  56%|█████▌    | 189/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 189/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▋    | 191/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  Loading weights:  56%|█████▋    | 191/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]Loading weights:  57%|█████▋    | 192/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 192/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 193/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  Loading weights:  57%|█████▋    | 193/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]Loading weights:  57%|█████▋    | 194/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  57%|█████▋    | 194/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  58%|█████▊    | 195/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.16.input_layernorm.weight] Loading weights:  58%|█████▊    | 195/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  58%|█████▊    | 196/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  Loading weights:  58%|█████▊    | 196/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:04<00:02, 54.00it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 198/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 198/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  Loading weights:  58%|█████▊    | 198/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.mlp.up_proj.weight]Loading weights:  59%|█████▊    | 199/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▊    | 199/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▉    | 200/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          Loading weights:  59%|█████▉    | 200/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]Loading weights:  59%|█████▉    | 201/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  59%|█████▉    | 201/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 203/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  Loading weights:  60%|█████▉    | 203/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]Loading weights:  60%|██████    | 204/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 204/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 205/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  Loading weights:  60%|██████    | 205/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]Loading weights:  61%|██████    | 206/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 206/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 207/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.17.input_layernorm.weight] Loading weights:  61%|██████    | 207/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  61%|██████▏   | 208/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  Loading weights:  61%|██████▏   | 208/339 [00:04<00:02, 60.41it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 210/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  Loading weights:  62%|██████▏   | 210/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.mlp.up_proj.weight]Loading weights:  62%|██████▏   | 211/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  62%|██████▏   | 211/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  63%|██████▎   | 212/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          Loading weights:  63%|██████▎   | 212/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]Loading weights:  63%|██████▎   | 213/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 213/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 215/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  Loading weights:  63%|██████▎   | 215/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]Loading weights:  64%|██████▎   | 216/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▎   | 216/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▍   | 217/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  Loading weights:  64%|██████▍   | 217/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]Loading weights:  64%|██████▍   | 218/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  64%|██████▍   | 218/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  65%|██████▍   | 219/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.18.input_layernorm.weight] Loading weights:  65%|██████▍   | 219/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.18.input_layernorm.weight]Loading weights:  65%|██████▍   | 220/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  Loading weights:  65%|██████▍   | 220/339 [00:04<00:02, 57.07it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 222/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  Loading weights:  65%|██████▌   | 222/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.mlp.up_proj.weight]Loading weights:  66%|██████▌   | 223/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 223/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 224/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          Loading weights:  66%|██████▌   | 224/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]Loading weights:  66%|██████▋   | 225/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  66%|██████▋   | 225/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 227/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  Loading weights:  67%|██████▋   | 227/339 [00:04<00:01, 62.72it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]Loading weights:  67%|██████▋   | 228/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]Loading weights:  67%|██████▋   | 228/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  67%|██████▋   | 228/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  68%|██████▊   | 229/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  Loading weights:  68%|██████▊   | 229/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]Loading weights:  68%|██████▊   | 230/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 230/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 231/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.19.input_layernorm.weight] Loading weights:  68%|██████▊   | 231/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  68%|██████▊   | 232/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  Loading weights:  68%|██████▊   | 232/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:04<00:01, 58.97it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▉   | 234/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▉   | 234/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  Loading weights:  69%|██████▉   | 234/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.mlp.up_proj.weight]Loading weights:  69%|██████▉   | 235/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  69%|██████▉   | 235/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  70%|██████▉   | 236/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          Loading weights:  70%|██████▉   | 236/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]Loading weights:  70%|██████▉   | 237/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|██████▉   | 237/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|███████   | 238/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  70%|███████   | 238/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  71%|███████   | 239/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  Loading weights:  71%|███████   | 239/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]Loading weights:  71%|███████   | 240/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 240/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 241/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  Loading weights:  71%|███████   | 241/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]Loading weights:  71%|███████▏  | 242/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  71%|███████▏  | 242/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  72%|███████▏  | 243/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.20.input_layernorm.weight] Loading weights:  72%|███████▏  | 243/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  72%|███████▏  | 244/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  Loading weights:  72%|███████▏  | 244/339 [00:05<00:01, 54.48it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  73%|███████▎  | 246/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  Loading weights:  73%|███████▎  | 246/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.mlp.up_proj.weight]Loading weights:  73%|███████▎  | 247/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 247/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 248/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          Loading weights:  73%|███████▎  | 248/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]Loading weights:  73%|███████▎  | 249/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  73%|███████▎  | 249/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:05<00:01, 59.61it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▍  | 251/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▍  | 251/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  Loading weights:  74%|███████▍  | 251/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]Loading weights:  74%|███████▍  | 252/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  74%|███████▍  | 252/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  75%|███████▍  | 253/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  Loading weights:  75%|███████▍  | 253/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]Loading weights:  75%|███████▍  | 254/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▍  | 254/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▌  | 255/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.21.input_layernorm.weight] Loading weights:  75%|███████▌  | 255/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  76%|███████▌  | 256/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  Loading weights:  76%|███████▌  | 256/339 [00:05<00:01, 49.80it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:05<00:01, 48.46it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:05<00:01, 48.46it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:05<00:01, 48.46it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 258/339 [00:05<00:01, 48.46it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  Loading weights:  76%|███████▌  | 258/339 [00:05<00:01, 48.46it/s, Materializing param=model.layers.21.mlp.up_proj.weight]Loading weights:  76%|███████▋  | 259/339 [00:05<00:01, 48.46it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  76%|███████▋  | 259/339 [00:05<00:01, 48.46it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  77%|███████▋  | 260/339 [00:05<00:01, 48.46it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          Loading weights:  77%|███████▋  | 260/339 [00:05<00:01, 48.46it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]Loading weights:  77%|███████▋  | 261/339 [00:05<00:01, 48.46it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 261/339 [00:05<00:01, 48.46it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  78%|███████▊  | 263/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  Loading weights:  78%|███████▊  | 263/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]Loading weights:  78%|███████▊  | 264/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 264/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 265/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  Loading weights:  78%|███████▊  | 265/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]Loading weights:  78%|███████▊  | 266/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  78%|███████▊  | 266/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  79%|███████▉  | 267/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.22.input_layernorm.weight] Loading weights:  79%|███████▉  | 267/339 [00:05<00:02, 35.85it/s, Materializing param=model.layers.22.input_layernorm.weight]Loading weights:  79%|███████▉  | 268/339 [00:05<00:01, 35.85it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  Loading weights:  79%|███████▉  | 268/339 [00:05<00:01, 35.85it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:05<00:01, 37.52it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:05<00:01, 37.52it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:05<00:01, 37.52it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  80%|███████▉  | 270/339 [00:06<00:01, 37.52it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  Loading weights:  80%|███████▉  | 270/339 [00:06<00:01, 37.52it/s, Materializing param=model.layers.22.mlp.up_proj.weight]Loading weights:  80%|███████▉  | 271/339 [00:06<00:01, 37.52it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|███████▉  | 271/339 [00:06<00:01, 37.52it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|████████  | 272/339 [00:06<00:01, 37.52it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          Loading weights:  80%|████████  | 272/339 [00:06<00:01, 37.52it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]Loading weights:  81%|████████  | 273/339 [00:06<00:01, 37.52it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 273/339 [00:06<00:01, 37.52it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 274/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 274/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 274/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 275/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  Loading weights:  81%|████████  | 275/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]Loading weights:  81%|████████▏ | 276/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  81%|████████▏ | 276/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  82%|████████▏ | 277/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  Loading weights:  82%|████████▏ | 277/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]Loading weights:  82%|████████▏ | 278/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 278/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 279/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.23.input_layernorm.weight] Loading weights:  82%|████████▏ | 279/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.23.input_layernorm.weight]Loading weights:  83%|████████▎ | 280/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  Loading weights:  83%|████████▎ | 280/339 [00:06<00:02, 26.85it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:06<00:02, 26.69it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:06<00:02, 26.69it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:06<00:02, 26.69it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 282/339 [00:06<00:02, 26.69it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  Loading weights:  83%|████████▎ | 282/339 [00:06<00:02, 26.69it/s, Materializing param=model.layers.23.mlp.up_proj.weight]Loading weights:  83%|████████▎ | 283/339 [00:06<00:02, 26.69it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  83%|████████▎ | 283/339 [00:06<00:02, 26.69it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  84%|████████▍ | 284/339 [00:06<00:02, 26.69it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          Loading weights:  84%|████████▍ | 284/339 [00:06<00:02, 26.69it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]Loading weights:  84%|████████▍ | 285/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]Loading weights:  84%|████████▍ | 285/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 285/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  85%|████████▍ | 287/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  Loading weights:  85%|████████▍ | 287/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]Loading weights:  85%|████████▍ | 288/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▍ | 288/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▌ | 289/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  Loading weights:  85%|████████▌ | 289/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]Loading weights:  86%|████████▌ | 290/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 290/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 291/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.24.input_layernorm.weight] Loading weights:  86%|████████▌ | 291/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.24.input_layernorm.weight]Loading weights:  86%|████████▌ | 292/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.24.mlp.down_proj.weight]  Loading weights:  86%|████████▌ | 292/339 [00:06<00:02, 21.81it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:07<00:02, 21.01it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:07<00:02, 21.01it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:07<00:02, 21.01it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  87%|████████▋ | 294/339 [00:07<00:02, 21.01it/s, Materializing param=model.layers.24.mlp.up_proj.weight]  Loading weights:  87%|████████▋ | 294/339 [00:07<00:02, 21.01it/s, Materializing param=model.layers.24.mlp.up_proj.weight]Loading weights:  87%|████████▋ | 295/339 [00:07<00:02, 21.01it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 295/339 [00:07<00:02, 21.01it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 296/339 [00:07<00:02, 21.08it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 296/339 [00:07<00:02, 21.08it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]          Loading weights:  87%|████████▋ | 296/339 [00:07<00:02, 21.08it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]Loading weights:  88%|████████▊ | 297/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 297/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 299/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]  Loading weights:  88%|████████▊ | 299/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]Loading weights:  88%|████████▊ | 300/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  88%|████████▊ | 300/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  89%|████████▉ | 301/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]  Loading weights:  89%|████████▉ | 301/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]Loading weights:  89%|████████▉ | 302/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 302/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 303/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.25.input_layernorm.weight] Loading weights:  89%|████████▉ | 303/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.25.input_layernorm.weight]Loading weights:  90%|████████▉ | 304/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.25.mlp.down_proj.weight]  Loading weights:  90%|████████▉ | 304/339 [00:07<00:01, 21.08it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:07<00:01, 27.56it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:07<00:01, 27.56it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:07<00:01, 27.56it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|█████████ | 306/339 [00:07<00:01, 27.56it/s, Materializing param=model.layers.25.mlp.up_proj.weight]  Loading weights:  90%|█████████ | 306/339 [00:07<00:01, 27.56it/s, Materializing param=model.layers.25.mlp.up_proj.weight]Loading weights:  91%|█████████ | 307/339 [00:07<00:01, 27.56it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 307/339 [00:07<00:01, 27.56it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 308/339 [00:07<00:01, 27.56it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]          Loading weights:  91%|█████████ | 308/339 [00:07<00:01, 27.56it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]Loading weights:  91%|█████████ | 309/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]Loading weights:  91%|█████████ | 309/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████ | 309/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  92%|█████████▏| 311/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]  Loading weights:  92%|█████████▏| 311/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]Loading weights:  92%|█████████▏| 312/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 312/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 313/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]  Loading weights:  92%|█████████▏| 313/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]Loading weights:  93%|█████████▎| 314/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 314/339 [00:07<00:01, 24.70it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 315/339 [00:07<00:00, 24.70it/s, Materializing param=model.layers.26.input_layernorm.weight] Loading weights:  93%|█████████▎| 315/339 [00:07<00:00, 24.70it/s, Materializing param=model.layers.26.input_layernorm.weight]Loading weights:  93%|█████████▎| 316/339 [00:07<00:00, 28.64it/s, Materializing param=model.layers.26.input_layernorm.weight]Loading weights:  93%|█████████▎| 316/339 [00:07<00:00, 28.64it/s, Materializing param=model.layers.26.mlp.down_proj.weight]  Loading weights:  93%|█████████▎| 316/339 [00:07<00:00, 28.64it/s, Materializing param=model.layers.26.mlp.down_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:07<00:00, 28.64it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:07<00:00, 28.64it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▍| 318/339 [00:08<00:00, 28.64it/s, Materializing param=model.layers.26.mlp.up_proj.weight]  Loading weights:  94%|█████████▍| 318/339 [00:08<00:00, 28.64it/s, Materializing param=model.layers.26.mlp.up_proj.weight]Loading weights:  94%|█████████▍| 319/339 [00:08<00:00, 28.64it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 319/339 [00:08<00:00, 28.64it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 320/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 320/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]          Loading weights:  94%|█████████▍| 320/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]Loading weights:  95%|█████████▍| 321/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 321/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▌| 323/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]  Loading weights:  95%|█████████▌| 323/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]Loading weights:  96%|█████████▌| 324/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 324/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 325/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]  Loading weights:  96%|█████████▌| 325/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]Loading weights:  96%|█████████▌| 326/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▌| 326/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▋| 327/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.27.input_layernorm.weight] Loading weights:  96%|█████████▋| 327/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.27.input_layernorm.weight]Loading weights:  97%|█████████▋| 328/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.27.mlp.down_proj.weight]  Loading weights:  97%|█████████▋| 328/339 [00:08<00:00, 23.27it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:08<00:00, 25.00it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:08<00:00, 25.00it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:08<00:00, 25.00it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 330/339 [00:08<00:00, 25.00it/s, Materializing param=model.layers.27.mlp.up_proj.weight]  Loading weights:  97%|█████████▋| 330/339 [00:08<00:00, 25.00it/s, Materializing param=model.layers.27.mlp.up_proj.weight]Loading weights:  98%|█████████▊| 331/339 [00:08<00:00, 25.00it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 331/339 [00:08<00:00, 25.00it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 332/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 332/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]          Loading weights:  98%|█████████▊| 332/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]Loading weights:  98%|█████████▊| 333/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  98%|█████████▊| 333/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▉| 335/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]  Loading weights:  99%|█████████▉| 335/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]Loading weights:  99%|█████████▉| 336/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 336/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 337/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]  Loading weights:  99%|█████████▉| 337/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]Loading weights: 100%|█████████▉| 338/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|█████████▉| 338/339 [00:08<00:00, 24.10it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|██████████| 339/339 [00:08<00:00, 24.10it/s, Materializing param=model.norm.weight]                      Loading weights: 100%|██████████| 339/339 [00:08<00:00, 24.10it/s, Materializing param=model.norm.weight]Loading weights: 100%|██████████| 339/339 [00:08<00:00, 38.44it/s, Materializing param=model.norm.weight]
The tokenizer you are loading from './local_models/Qwen/Qwen2.5-7B-Instruct' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
Using Huggingface Hub Token from .env
Using dtype: torch.bfloat16
loading local model and tokenizer...
Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]Loading weights:   0%|          | 1/339 [00:00<00:00, 123361.88it/s, Materializing param=lm_head.weight]Loading weights:   0%|          | 1/339 [00:00<00:00, 28728.11it/s, Materializing param=lm_head.weight] Loading weights:   1%|          | 2/339 [00:02<05:48,  1.03s/it, Materializing param=lm_head.weight]   Loading weights:   1%|          | 2/339 [00:02<05:48,  1.03s/it, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 2/339 [00:02<05:48,  1.03s/it, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 3/339 [00:02<05:01,  1.11it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 3/339 [00:02<05:01,  1.11it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 3/339 [00:02<05:01,  1.11it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 4/339 [00:02<05:00,  1.11it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  Loading weights:   1%|          | 4/339 [00:02<05:00,  1.11it/s, Materializing param=model.layers.0.mlp.down_proj.weight]Loading weights:   1%|▏         | 5/339 [00:02<04:59,  1.11it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   1%|▏         | 5/339 [00:02<04:59,  1.11it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|▏         | 6/339 [00:02<02:00,  2.77it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|▏         | 6/339 [00:02<02:00,  2.77it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  Loading weights:   2%|▏         | 6/339 [00:02<02:00,  2.77it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   2%|▏         | 7/339 [00:03<01:59,  2.77it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 7/339 [00:03<01:59,  2.77it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 8/339 [00:03<01:59,  2.77it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          Loading weights:   2%|▏         | 8/339 [00:03<01:59,  2.77it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]Loading weights:   3%|▎         | 9/339 [00:03<01:59,  2.77it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 9/339 [00:03<01:59,  2.77it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 10/339 [00:03<01:58,  2.77it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 10/339 [00:03<01:58,  2.77it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 11/339 [00:03<00:51,  6.38it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 11/339 [00:03<00:51,  6.38it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]  Loading weights:   3%|▎         | 11/339 [00:03<00:51,  6.38it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]Loading weights:   4%|▎         | 12/339 [00:03<00:51,  6.38it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▎         | 12/339 [00:03<00:51,  6.38it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▍         | 13/339 [00:03<00:51,  6.38it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  Loading weights:   4%|▍         | 13/339 [00:03<00:51,  6.38it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]Loading weights:   4%|▍         | 14/339 [00:03<00:50,  6.38it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 14/339 [00:03<00:50,  6.38it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 15/339 [00:03<00:50,  6.38it/s, Materializing param=model.layers.1.input_layernorm.weight] Loading weights:   4%|▍         | 15/339 [00:03<00:50,  6.38it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   5%|▍         | 16/339 [00:03<00:50,  6.38it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  Loading weights:   5%|▍         | 16/339 [00:03<00:50,  6.38it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   5%|▌         | 17/339 [00:03<00:27, 11.57it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   5%|▌         | 17/339 [00:03<00:27, 11.57it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 17/339 [00:03<00:27, 11.57it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 18/339 [00:03<00:27, 11.57it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  Loading weights:   5%|▌         | 18/339 [00:03<00:27, 11.57it/s, Materializing param=model.layers.1.mlp.up_proj.weight]Loading weights:   6%|▌         | 19/339 [00:03<00:27, 11.57it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 19/339 [00:03<00:27, 11.57it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 20/339 [00:03<00:27, 11.57it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          Loading weights:   6%|▌         | 20/339 [00:03<00:27, 11.57it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]Loading weights:   6%|▌         | 21/339 [00:03<00:23, 13.63it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]Loading weights:   6%|▌         | 21/339 [00:03<00:23, 13.63it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▌         | 21/339 [00:03<00:23, 13.63it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▋         | 22/339 [00:03<00:23, 13.63it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   6%|▋         | 22/339 [00:03<00:23, 13.63it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   7%|▋         | 23/339 [00:03<00:23, 13.63it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  Loading weights:   7%|▋         | 23/339 [00:03<00:23, 13.63it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]Loading weights:   7%|▋         | 24/339 [00:03<00:23, 13.63it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 24/339 [00:03<00:23, 13.63it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 25/339 [00:03<00:23, 13.63it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  Loading weights:   7%|▋         | 25/339 [00:03<00:23, 13.63it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]Loading weights:   8%|▊         | 26/339 [00:03<00:22, 13.63it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 26/339 [00:03<00:22, 13.63it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 27/339 [00:03<00:22, 13.63it/s, Materializing param=model.layers.2.input_layernorm.weight] Loading weights:   8%|▊         | 27/339 [00:03<00:22, 13.63it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:   8%|▊         | 28/339 [00:03<00:22, 13.63it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  Loading weights:   8%|▊         | 28/339 [00:03<00:22, 13.63it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:   9%|▊         | 29/339 [00:03<00:14, 21.93it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:   9%|▊         | 29/339 [00:03<00:14, 21.93it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▊         | 29/339 [00:03<00:14, 21.93it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▉         | 30/339 [00:03<00:14, 21.93it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  Loading weights:   9%|▉         | 30/339 [00:03<00:14, 21.93it/s, Materializing param=model.layers.2.mlp.up_proj.weight]Loading weights:   9%|▉         | 31/339 [00:03<00:14, 21.93it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 31/339 [00:03<00:14, 21.93it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 32/339 [00:03<00:13, 21.93it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          Loading weights:   9%|▉         | 32/339 [00:03<00:13, 21.93it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]Loading weights:  10%|▉         | 33/339 [00:03<00:13, 21.93it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|▉         | 33/339 [00:03<00:13, 21.93it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|█         | 34/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|█         | 34/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 34/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 35/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  Loading weights:  10%|█         | 35/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]Loading weights:  11%|█         | 36/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 36/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 37/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  Loading weights:  11%|█         | 37/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]Loading weights:  11%|█         | 38/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  11%|█         | 38/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  12%|█▏        | 39/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.3.input_layernorm.weight] Loading weights:  12%|█▏        | 39/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  12%|█▏        | 40/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  Loading weights:  12%|█▏        | 40/339 [00:03<00:12, 23.46it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:03<00:09, 30.08it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:03<00:09, 30.08it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:03<00:09, 30.08it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 42/339 [00:03<00:09, 30.08it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  Loading weights:  12%|█▏        | 42/339 [00:03<00:09, 30.08it/s, Materializing param=model.layers.3.mlp.up_proj.weight]Loading weights:  13%|█▎        | 43/339 [00:03<00:09, 30.08it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 43/339 [00:03<00:09, 30.08it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 44/339 [00:03<00:09, 30.08it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          Loading weights:  13%|█▎        | 44/339 [00:03<00:09, 30.08it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]Loading weights:  13%|█▎        | 45/339 [00:03<00:09, 30.08it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  13%|█▎        | 45/339 [00:03<00:09, 30.08it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:03<00:10, 29.28it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:03<00:10, 29.28it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:03<00:10, 29.28it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▍        | 47/339 [00:03<00:09, 29.28it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  Loading weights:  14%|█▍        | 47/339 [00:03<00:09, 29.28it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]Loading weights:  14%|█▍        | 48/339 [00:03<00:09, 29.28it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 48/339 [00:03<00:09, 29.28it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 49/339 [00:04<00:09, 29.28it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  Loading weights:  14%|█▍        | 49/339 [00:04<00:09, 29.28it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]Loading weights:  15%|█▍        | 50/339 [00:04<00:09, 29.28it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▍        | 50/339 [00:04<00:09, 29.28it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▌        | 51/339 [00:04<00:09, 29.28it/s, Materializing param=model.layers.4.input_layernorm.weight] Loading weights:  15%|█▌        | 51/339 [00:04<00:09, 29.28it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  15%|█▌        | 52/339 [00:04<00:09, 29.28it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  Loading weights:  15%|█▌        | 52/339 [00:04<00:09, 29.28it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 54/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  Loading weights:  16%|█▌        | 54/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.mlp.up_proj.weight]Loading weights:  16%|█▌        | 55/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  16%|█▌        | 55/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  17%|█▋        | 56/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          Loading weights:  17%|█▋        | 56/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]Loading weights:  17%|█▋        | 57/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 57/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:04<00:09, 30.63it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 59/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 59/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  Loading weights:  17%|█▋        | 59/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]Loading weights:  18%|█▊        | 60/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 60/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 61/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  Loading weights:  18%|█▊        | 61/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]Loading weights:  18%|█▊        | 62/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  18%|█▊        | 62/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  19%|█▊        | 63/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.5.input_layernorm.weight] Loading weights:  19%|█▊        | 63/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  19%|█▉        | 64/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  Loading weights:  19%|█▉        | 64/339 [00:04<00:07, 35.42it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:04<00:06, 39.79it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:04<00:06, 39.79it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:04<00:06, 39.79it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 66/339 [00:04<00:06, 39.79it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  Loading weights:  19%|█▉        | 66/339 [00:04<00:06, 39.79it/s, Materializing param=model.layers.5.mlp.up_proj.weight]Loading weights:  20%|█▉        | 67/339 [00:04<00:06, 39.79it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|█▉        | 67/339 [00:04<00:06, 39.79it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|██        | 68/339 [00:04<00:06, 39.79it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          Loading weights:  20%|██        | 68/339 [00:04<00:06, 39.79it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]Loading weights:  20%|██        | 69/339 [00:04<00:06, 39.79it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  20%|██        | 69/339 [00:04<00:06, 39.79it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  21%|██        | 70/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  21%|██        | 70/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 70/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 71/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  Loading weights:  21%|██        | 71/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]Loading weights:  21%|██        | 72/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  21%|██        | 72/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  22%|██▏       | 73/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  Loading weights:  22%|██▏       | 73/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]Loading weights:  22%|██▏       | 74/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 74/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 75/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.6.input_layernorm.weight] Loading weights:  22%|██▏       | 75/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  22%|██▏       | 76/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  Loading weights:  22%|██▏       | 76/339 [00:04<00:07, 35.90it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:04<00:06, 40.76it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:04<00:06, 40.76it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:04<00:06, 40.76it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 78/339 [00:04<00:06, 40.76it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  Loading weights:  23%|██▎       | 78/339 [00:04<00:06, 40.76it/s, Materializing param=model.layers.6.mlp.up_proj.weight]Loading weights:  23%|██▎       | 79/339 [00:04<00:06, 40.76it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  23%|██▎       | 79/339 [00:04<00:06, 40.76it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  24%|██▎       | 80/339 [00:04<00:06, 40.76it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          Loading weights:  24%|██▎       | 80/339 [00:04<00:06, 40.76it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]Loading weights:  24%|██▍       | 81/339 [00:04<00:06, 40.76it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 81/339 [00:04<00:06, 40.76it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:04<00:07, 36.26it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:04<00:07, 36.26it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:04<00:07, 36.26it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 83/339 [00:04<00:07, 36.26it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  Loading weights:  24%|██▍       | 83/339 [00:04<00:07, 36.26it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]Loading weights:  25%|██▍       | 84/339 [00:04<00:07, 36.26it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▍       | 84/339 [00:04<00:07, 36.26it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▌       | 85/339 [00:04<00:07, 36.26it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  Loading weights:  25%|██▌       | 85/339 [00:04<00:07, 36.26it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]Loading weights:  25%|██▌       | 86/339 [00:04<00:06, 36.26it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  25%|██▌       | 86/339 [00:04<00:06, 36.26it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  26%|██▌       | 87/339 [00:04<00:06, 36.26it/s, Materializing param=model.layers.7.input_layernorm.weight] Loading weights:  26%|██▌       | 87/339 [00:04<00:06, 36.26it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  26%|██▌       | 88/339 [00:04<00:06, 36.26it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  Loading weights:  26%|██▌       | 88/339 [00:04<00:06, 36.26it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:05<00:06, 41.28it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:05<00:06, 41.28it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:05<00:06, 41.28it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  27%|██▋       | 90/339 [00:05<00:06, 41.28it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  Loading weights:  27%|██▋       | 90/339 [00:05<00:06, 41.28it/s, Materializing param=model.layers.7.mlp.up_proj.weight]Loading weights:  27%|██▋       | 91/339 [00:05<00:06, 41.28it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 91/339 [00:05<00:06, 41.28it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 92/339 [00:05<00:05, 41.28it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          Loading weights:  27%|██▋       | 92/339 [00:05<00:05, 41.28it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]Loading weights:  27%|██▋       | 93/339 [00:05<00:05, 41.28it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  27%|██▋       | 93/339 [00:05<00:05, 41.28it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 95/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  Loading weights:  28%|██▊       | 95/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]Loading weights:  28%|██▊       | 96/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  28%|██▊       | 96/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  29%|██▊       | 97/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  Loading weights:  29%|██▊       | 97/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]Loading weights:  29%|██▉       | 98/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 98/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 99/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.8.input_layernorm.weight] Loading weights:  29%|██▉       | 99/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  29%|██▉       | 100/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.8.mlp.down_proj.weight] Loading weights:  29%|██▉       | 100/339 [00:05<00:06, 36.66it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:05<00:05, 41.88it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:05<00:05, 41.88it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:05<00:05, 41.88it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|███       | 102/339 [00:05<00:05, 41.88it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  Loading weights:  30%|███       | 102/339 [00:05<00:05, 41.88it/s, Materializing param=model.layers.8.mlp.up_proj.weight]Loading weights:  30%|███       | 103/339 [00:05<00:05, 41.88it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  30%|███       | 103/339 [00:05<00:05, 41.88it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  31%|███       | 104/339 [00:05<00:05, 41.88it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          Loading weights:  31%|███       | 104/339 [00:05<00:05, 41.88it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]Loading weights:  31%|███       | 105/339 [00:05<00:05, 41.88it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███       | 105/339 [00:05<00:05, 41.88it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  32%|███▏      | 107/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  Loading weights:  32%|███▏      | 107/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]Loading weights:  32%|███▏      | 108/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 108/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 109/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  Loading weights:  32%|███▏      | 109/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]Loading weights:  32%|███▏      | 110/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  32%|███▏      | 110/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  33%|███▎      | 111/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.9.input_layernorm.weight] Loading weights:  33%|███▎      | 111/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  33%|███▎      | 112/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  Loading weights:  33%|███▎      | 112/339 [00:05<00:06, 37.45it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:05<00:05, 42.07it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:05<00:05, 42.07it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:05<00:05, 42.07it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  34%|███▎      | 114/339 [00:05<00:05, 42.07it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  Loading weights:  34%|███▎      | 114/339 [00:05<00:05, 42.07it/s, Materializing param=model.layers.9.mlp.up_proj.weight]Loading weights:  34%|███▍      | 115/339 [00:05<00:05, 42.07it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 115/339 [00:05<00:05, 42.07it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 116/339 [00:05<00:05, 42.07it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          Loading weights:  34%|███▍      | 116/339 [00:05<00:05, 42.07it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]Loading weights:  35%|███▍      | 117/339 [00:05<00:05, 42.07it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 117/339 [00:05<00:05, 42.07it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▌      | 119/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  Loading weights:  35%|███▌      | 119/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]Loading weights:  35%|███▌      | 120/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  35%|███▌      | 120/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  36%|███▌      | 121/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  Loading weights:  36%|███▌      | 121/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]Loading weights:  36%|███▌      | 122/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▌      | 122/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▋      | 123/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  36%|███▋      | 123/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  37%|███▋      | 124/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  Loading weights:  37%|███▋      | 124/339 [00:05<00:05, 37.53it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:05<00:05, 42.35it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:05<00:05, 42.35it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:05<00:05, 42.35it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 126/339 [00:06<00:05, 42.35it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  Loading weights:  37%|███▋      | 126/339 [00:06<00:05, 42.35it/s, Materializing param=model.layers.10.mlp.up_proj.weight]Loading weights:  37%|███▋      | 127/339 [00:06<00:05, 42.35it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  37%|███▋      | 127/339 [00:06<00:05, 42.35it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  38%|███▊      | 128/339 [00:06<00:04, 42.35it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          Loading weights:  38%|███▊      | 128/339 [00:06<00:04, 42.35it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]Loading weights:  38%|███▊      | 129/339 [00:06<00:04, 42.35it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 129/339 [00:06<00:04, 42.35it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  39%|███▊      | 131/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  Loading weights:  39%|███▊      | 131/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]Loading weights:  39%|███▉      | 132/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 132/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 133/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  Loading weights:  39%|███▉      | 133/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]Loading weights:  40%|███▉      | 134/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 134/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 135/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.11.input_layernorm.weight] Loading weights:  40%|███▉      | 135/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  40%|████      | 136/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  Loading weights:  40%|████      | 136/339 [00:06<00:05, 37.35it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  40%|████      | 137/339 [00:06<00:04, 42.11it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  40%|████      | 137/339 [00:06<00:04, 42.11it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  40%|████      | 137/339 [00:06<00:04, 42.11it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  41%|████      | 138/339 [00:06<00:04, 42.11it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  Loading weights:  41%|████      | 138/339 [00:06<00:04, 42.11it/s, Materializing param=model.layers.11.mlp.up_proj.weight]Loading weights:  41%|████      | 139/339 [00:06<00:04, 42.11it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████      | 139/339 [00:06<00:04, 42.11it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████▏     | 140/339 [00:06<00:04, 42.11it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          Loading weights:  41%|████▏     | 140/339 [00:06<00:04, 42.11it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]Loading weights:  42%|████▏     | 141/339 [00:06<00:04, 42.11it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 141/339 [00:06<00:04, 42.11it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 143/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  Loading weights:  42%|████▏     | 143/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]Loading weights:  42%|████▏     | 144/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  42%|████▏     | 144/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  43%|████▎     | 145/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  Loading weights:  43%|████▎     | 145/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]Loading weights:  43%|████▎     | 146/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 146/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 147/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.12.input_layernorm.weight] Loading weights:  43%|████▎     | 147/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  44%|████▎     | 148/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  Loading weights:  44%|████▎     | 148/339 [00:06<00:05, 36.63it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:06<00:04, 41.45it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:06<00:04, 41.45it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:06<00:04, 41.45it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 150/339 [00:06<00:04, 41.45it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  Loading weights:  44%|████▍     | 150/339 [00:06<00:04, 41.45it/s, Materializing param=model.layers.12.mlp.up_proj.weight]Loading weights:  45%|████▍     | 151/339 [00:06<00:04, 41.45it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 151/339 [00:06<00:04, 41.45it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 152/339 [00:06<00:04, 41.45it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          Loading weights:  45%|████▍     | 152/339 [00:06<00:04, 41.45it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]Loading weights:  45%|████▌     | 153/339 [00:06<00:04, 41.45it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 153/339 [00:06<00:04, 41.45it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:06<00:05, 36.69it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:06<00:05, 36.69it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:06<00:05, 36.69it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  46%|████▌     | 155/339 [00:06<00:05, 36.69it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  Loading weights:  46%|████▌     | 155/339 [00:06<00:05, 36.69it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]Loading weights:  46%|████▌     | 156/339 [00:06<00:04, 36.69it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▌     | 156/339 [00:06<00:04, 36.69it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▋     | 157/339 [00:06<00:04, 36.69it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  Loading weights:  46%|████▋     | 157/339 [00:06<00:04, 36.69it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]Loading weights:  47%|████▋     | 158/339 [00:06<00:04, 36.69it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 158/339 [00:06<00:04, 36.69it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 159/339 [00:06<00:04, 36.69it/s, Materializing param=model.layers.13.input_layernorm.weight] Loading weights:  47%|████▋     | 159/339 [00:06<00:04, 36.69it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  47%|████▋     | 160/339 [00:06<00:04, 36.69it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  Loading weights:  47%|████▋     | 160/339 [00:06<00:04, 36.69it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:06<00:04, 41.37it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:06<00:04, 41.37it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:06<00:04, 41.37it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  48%|████▊     | 162/339 [00:06<00:04, 41.37it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  Loading weights:  48%|████▊     | 162/339 [00:06<00:04, 41.37it/s, Materializing param=model.layers.13.mlp.up_proj.weight]Loading weights:  48%|████▊     | 163/339 [00:07<00:04, 41.37it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 163/339 [00:07<00:04, 41.37it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 164/339 [00:07<00:04, 41.37it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          Loading weights:  48%|████▊     | 164/339 [00:07<00:04, 41.37it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]Loading weights:  49%|████▊     | 165/339 [00:07<00:04, 41.37it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▊     | 165/339 [00:07<00:04, 41.37it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 167/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  Loading weights:  49%|████▉     | 167/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]Loading weights:  50%|████▉     | 168/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 168/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 169/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  Loading weights:  50%|████▉     | 169/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]Loading weights:  50%|█████     | 170/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 170/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 171/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.14.input_layernorm.weight] Loading weights:  50%|█████     | 171/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  51%|█████     | 172/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  Loading weights:  51%|█████     | 172/339 [00:07<00:04, 36.68it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  51%|█████     | 173/339 [00:07<00:03, 41.54it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  51%|█████     | 173/339 [00:07<00:03, 41.54it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████     | 173/339 [00:07<00:03, 41.54it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████▏    | 174/339 [00:07<00:03, 41.54it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  Loading weights:  51%|█████▏    | 174/339 [00:07<00:03, 41.54it/s, Materializing param=model.layers.14.mlp.up_proj.weight]Loading weights:  52%|█████▏    | 175/339 [00:07<00:03, 41.54it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 175/339 [00:07<00:03, 41.54it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 176/339 [00:07<00:03, 41.54it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          Loading weights:  52%|█████▏    | 176/339 [00:07<00:03, 41.54it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]Loading weights:  52%|█████▏    | 177/339 [00:07<00:03, 41.54it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  52%|█████▏    | 177/339 [00:07<00:03, 41.54it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 179/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  Loading weights:  53%|█████▎    | 179/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]Loading weights:  53%|█████▎    | 180/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 180/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 181/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  Loading weights:  53%|█████▎    | 181/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]Loading weights:  54%|█████▎    | 182/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▎    | 182/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▍    | 183/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.15.input_layernorm.weight] Loading weights:  54%|█████▍    | 183/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  54%|█████▍    | 184/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  Loading weights:  54%|█████▍    | 184/339 [00:07<00:04, 36.44it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:07<00:03, 41.39it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:07<00:03, 41.39it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:07<00:03, 41.39it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 186/339 [00:07<00:03, 41.39it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  Loading weights:  55%|█████▍    | 186/339 [00:07<00:03, 41.39it/s, Materializing param=model.layers.15.mlp.up_proj.weight]Loading weights:  55%|█████▌    | 187/339 [00:07<00:03, 41.39it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 187/339 [00:07<00:03, 41.39it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 188/339 [00:07<00:03, 41.39it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          Loading weights:  55%|█████▌    | 188/339 [00:07<00:03, 41.39it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]Loading weights:  56%|█████▌    | 189/339 [00:07<00:03, 41.39it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 189/339 [00:07<00:03, 41.39it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:07<00:04, 36.78it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:07<00:04, 36.78it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:07<00:04, 36.78it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▋    | 191/339 [00:07<00:04, 36.78it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  Loading weights:  56%|█████▋    | 191/339 [00:07<00:04, 36.78it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]Loading weights:  57%|█████▋    | 192/339 [00:07<00:03, 36.78it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 192/339 [00:07<00:03, 36.78it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 193/339 [00:07<00:03, 36.78it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  Loading weights:  57%|█████▋    | 193/339 [00:07<00:03, 36.78it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]Loading weights:  57%|█████▋    | 194/339 [00:07<00:03, 36.78it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  57%|█████▋    | 194/339 [00:07<00:03, 36.78it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  58%|█████▊    | 195/339 [00:07<00:03, 36.78it/s, Materializing param=model.layers.16.input_layernorm.weight] Loading weights:  58%|█████▊    | 195/339 [00:07<00:03, 36.78it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  58%|█████▊    | 196/339 [00:07<00:03, 36.78it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  Loading weights:  58%|█████▊    | 196/339 [00:07<00:03, 36.78it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:07<00:03, 40.71it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:07<00:03, 40.71it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:07<00:03, 40.71it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 198/339 [00:07<00:03, 40.71it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  Loading weights:  58%|█████▊    | 198/339 [00:07<00:03, 40.71it/s, Materializing param=model.layers.16.mlp.up_proj.weight]Loading weights:  59%|█████▊    | 199/339 [00:07<00:03, 40.71it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▊    | 199/339 [00:07<00:03, 40.71it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▉    | 200/339 [00:07<00:03, 40.71it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          Loading weights:  59%|█████▉    | 200/339 [00:07<00:03, 40.71it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]Loading weights:  59%|█████▉    | 201/339 [00:07<00:03, 40.71it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  59%|█████▉    | 201/339 [00:07<00:03, 40.71it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 203/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  Loading weights:  60%|█████▉    | 203/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]Loading weights:  60%|██████    | 204/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 204/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 205/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  Loading weights:  60%|██████    | 205/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]Loading weights:  61%|██████    | 206/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 206/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 207/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.17.input_layernorm.weight] Loading weights:  61%|██████    | 207/339 [00:07<00:03, 36.20it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  61%|██████▏   | 208/339 [00:08<00:03, 38.72it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  61%|██████▏   | 208/339 [00:08<00:03, 38.72it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  Loading weights:  61%|██████▏   | 208/339 [00:08<00:03, 38.72it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:08<00:03, 38.72it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:08<00:03, 38.72it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 210/339 [00:08<00:03, 38.72it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  Loading weights:  62%|██████▏   | 210/339 [00:08<00:03, 38.72it/s, Materializing param=model.layers.17.mlp.up_proj.weight]Loading weights:  62%|██████▏   | 211/339 [00:08<00:03, 38.72it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  62%|██████▏   | 211/339 [00:08<00:03, 38.72it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  63%|██████▎   | 212/339 [00:08<00:03, 38.72it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          Loading weights:  63%|██████▎   | 212/339 [00:08<00:03, 38.72it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]Loading weights:  63%|██████▎   | 213/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]Loading weights:  63%|██████▎   | 213/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 213/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 215/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  Loading weights:  63%|██████▎   | 215/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]Loading weights:  64%|██████▎   | 216/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▎   | 216/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▍   | 217/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  Loading weights:  64%|██████▍   | 217/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]Loading weights:  64%|██████▍   | 218/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  64%|██████▍   | 218/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  65%|██████▍   | 219/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.18.input_layernorm.weight] Loading weights:  65%|██████▍   | 219/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.18.input_layernorm.weight]Loading weights:  65%|██████▍   | 220/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  Loading weights:  65%|██████▍   | 220/339 [00:08<00:03, 35.51it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:08<00:02, 42.80it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:08<00:02, 42.80it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:08<00:02, 42.80it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 222/339 [00:08<00:02, 42.80it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  Loading weights:  65%|██████▌   | 222/339 [00:08<00:02, 42.80it/s, Materializing param=model.layers.18.mlp.up_proj.weight]Loading weights:  66%|██████▌   | 223/339 [00:08<00:02, 42.80it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 223/339 [00:08<00:02, 42.80it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 224/339 [00:08<00:02, 42.80it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          Loading weights:  66%|██████▌   | 224/339 [00:08<00:02, 42.80it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]Loading weights:  66%|██████▋   | 225/339 [00:08<00:02, 42.80it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  66%|██████▋   | 225/339 [00:08<00:02, 42.80it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 227/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  Loading weights:  67%|██████▋   | 227/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]Loading weights:  67%|██████▋   | 228/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  67%|██████▋   | 228/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  68%|██████▊   | 229/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  Loading weights:  68%|██████▊   | 229/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]Loading weights:  68%|██████▊   | 230/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 230/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 231/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.19.input_layernorm.weight] Loading weights:  68%|██████▊   | 231/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  68%|██████▊   | 232/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  Loading weights:  68%|██████▊   | 232/339 [00:08<00:02, 37.81it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:08<00:02, 42.61it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:08<00:02, 42.61it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:08<00:02, 42.61it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▉   | 234/339 [00:08<00:02, 42.61it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  Loading weights:  69%|██████▉   | 234/339 [00:08<00:02, 42.61it/s, Materializing param=model.layers.19.mlp.up_proj.weight]Loading weights:  69%|██████▉   | 235/339 [00:08<00:02, 42.61it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  69%|██████▉   | 235/339 [00:08<00:02, 42.61it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  70%|██████▉   | 236/339 [00:08<00:02, 42.61it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          Loading weights:  70%|██████▉   | 236/339 [00:08<00:02, 42.61it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]Loading weights:  70%|██████▉   | 237/339 [00:08<00:02, 42.61it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|██████▉   | 237/339 [00:08<00:02, 42.61it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|███████   | 238/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|███████   | 238/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  70%|███████   | 238/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  71%|███████   | 239/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  Loading weights:  71%|███████   | 239/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]Loading weights:  71%|███████   | 240/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 240/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 241/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  Loading weights:  71%|███████   | 241/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]Loading weights:  71%|███████▏  | 242/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  71%|███████▏  | 242/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  72%|███████▏  | 243/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.20.input_layernorm.weight] Loading weights:  72%|███████▏  | 243/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  72%|███████▏  | 244/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  Loading weights:  72%|███████▏  | 244/339 [00:08<00:02, 37.53it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:08<00:02, 42.47it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:08<00:02, 42.47it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:08<00:02, 42.47it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  73%|███████▎  | 246/339 [00:09<00:02, 42.47it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  Loading weights:  73%|███████▎  | 246/339 [00:09<00:02, 42.47it/s, Materializing param=model.layers.20.mlp.up_proj.weight]Loading weights:  73%|███████▎  | 247/339 [00:09<00:02, 42.47it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 247/339 [00:09<00:02, 42.47it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 248/339 [00:09<00:02, 42.47it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          Loading weights:  73%|███████▎  | 248/339 [00:09<00:02, 42.47it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]Loading weights:  73%|███████▎  | 249/339 [00:09<00:02, 42.47it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  73%|███████▎  | 249/339 [00:09<00:02, 42.47it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▍  | 251/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  Loading weights:  74%|███████▍  | 251/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]Loading weights:  74%|███████▍  | 252/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  74%|███████▍  | 252/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  75%|███████▍  | 253/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  Loading weights:  75%|███████▍  | 253/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]Loading weights:  75%|███████▍  | 254/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▍  | 254/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▌  | 255/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.21.input_layernorm.weight] Loading weights:  75%|███████▌  | 255/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  76%|███████▌  | 256/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  Loading weights:  76%|███████▌  | 256/339 [00:09<00:02, 37.71it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:09<00:01, 42.64it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:09<00:01, 42.64it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:09<00:01, 42.64it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 258/339 [00:09<00:01, 42.64it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  Loading weights:  76%|███████▌  | 258/339 [00:09<00:01, 42.64it/s, Materializing param=model.layers.21.mlp.up_proj.weight]Loading weights:  76%|███████▋  | 259/339 [00:09<00:01, 42.64it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  76%|███████▋  | 259/339 [00:09<00:01, 42.64it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  77%|███████▋  | 260/339 [00:09<00:01, 42.64it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          Loading weights:  77%|███████▋  | 260/339 [00:09<00:01, 42.64it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]Loading weights:  77%|███████▋  | 261/339 [00:09<00:01, 42.64it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 261/339 [00:09<00:01, 42.64it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:09<00:02, 37.73it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:09<00:02, 37.73it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:09<00:02, 37.73it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  78%|███████▊  | 263/339 [00:09<00:02, 37.73it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  Loading weights:  78%|███████▊  | 263/339 [00:09<00:02, 37.73it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]Loading weights:  78%|███████▊  | 264/339 [00:09<00:01, 37.73it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 264/339 [00:09<00:01, 37.73it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 265/339 [00:09<00:01, 37.73it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  Loading weights:  78%|███████▊  | 265/339 [00:09<00:01, 37.73it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]Loading weights:  78%|███████▊  | 266/339 [00:09<00:01, 37.73it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  78%|███████▊  | 266/339 [00:09<00:01, 37.73it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  79%|███████▉  | 267/339 [00:09<00:01, 37.73it/s, Materializing param=model.layers.22.input_layernorm.weight] Loading weights:  79%|███████▉  | 267/339 [00:09<00:01, 37.73it/s, Materializing param=model.layers.22.input_layernorm.weight]Loading weights:  79%|███████▉  | 268/339 [00:09<00:01, 37.73it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  Loading weights:  79%|███████▉  | 268/339 [00:09<00:01, 37.73it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  80%|███████▉  | 270/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  Loading weights:  80%|███████▉  | 270/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.22.mlp.up_proj.weight]Loading weights:  80%|███████▉  | 271/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|███████▉  | 271/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|████████  | 272/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          Loading weights:  80%|████████  | 272/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]Loading weights:  81%|████████  | 273/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 273/339 [00:09<00:01, 42.33it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 274/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 274/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 274/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 275/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  Loading weights:  81%|████████  | 275/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]Loading weights:  81%|████████▏ | 276/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  81%|████████▏ | 276/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  82%|████████▏ | 277/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  Loading weights:  82%|████████▏ | 277/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]Loading weights:  82%|████████▏ | 278/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 278/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 279/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.23.input_layernorm.weight] Loading weights:  82%|████████▏ | 279/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.23.input_layernorm.weight]Loading weights:  83%|████████▎ | 280/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  Loading weights:  83%|████████▎ | 280/339 [00:09<00:01, 37.27it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:09<00:01, 42.26it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:09<00:01, 42.26it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:09<00:01, 42.26it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 282/339 [00:09<00:01, 42.26it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  Loading weights:  83%|████████▎ | 282/339 [00:09<00:01, 42.26it/s, Materializing param=model.layers.23.mlp.up_proj.weight]Loading weights:  83%|████████▎ | 283/339 [00:10<00:01, 42.26it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  83%|████████▎ | 283/339 [00:10<00:01, 42.26it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  84%|████████▍ | 284/339 [00:10<00:01, 42.26it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          Loading weights:  84%|████████▍ | 284/339 [00:10<00:01, 42.26it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]Loading weights:  84%|████████▍ | 285/339 [00:10<00:01, 42.26it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 285/339 [00:10<00:01, 42.26it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  85%|████████▍ | 287/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  Loading weights:  85%|████████▍ | 287/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]Loading weights:  85%|████████▍ | 288/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▍ | 288/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▌ | 289/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  Loading weights:  85%|████████▌ | 289/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]Loading weights:  86%|████████▌ | 290/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 290/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 291/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.24.input_layernorm.weight] Loading weights:  86%|████████▌ | 291/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.24.input_layernorm.weight]Loading weights:  86%|████████▌ | 292/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.24.mlp.down_proj.weight]  Loading weights:  86%|████████▌ | 292/339 [00:10<00:01, 37.70it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:10<00:01, 42.63it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:10<00:01, 42.63it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:10<00:01, 42.63it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  87%|████████▋ | 294/339 [00:10<00:01, 42.63it/s, Materializing param=model.layers.24.mlp.up_proj.weight]  Loading weights:  87%|████████▋ | 294/339 [00:10<00:01, 42.63it/s, Materializing param=model.layers.24.mlp.up_proj.weight]Loading weights:  87%|████████▋ | 295/339 [00:10<00:01, 42.63it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 295/339 [00:10<00:01, 42.63it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 296/339 [00:10<00:01, 42.63it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]          Loading weights:  87%|████████▋ | 296/339 [00:10<00:01, 42.63it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]Loading weights:  88%|████████▊ | 297/339 [00:10<00:00, 42.63it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 297/339 [00:10<00:00, 42.63it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:10<00:01, 37.74it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:10<00:01, 37.74it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:10<00:01, 37.74it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 299/339 [00:10<00:01, 37.74it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]  Loading weights:  88%|████████▊ | 299/339 [00:10<00:01, 37.74it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]Loading weights:  88%|████████▊ | 300/339 [00:10<00:01, 37.74it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  88%|████████▊ | 300/339 [00:10<00:01, 37.74it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  89%|████████▉ | 301/339 [00:10<00:01, 37.74it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]  Loading weights:  89%|████████▉ | 301/339 [00:10<00:01, 37.74it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]Loading weights:  89%|████████▉ | 302/339 [00:10<00:00, 37.74it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 302/339 [00:10<00:00, 37.74it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 303/339 [00:10<00:00, 37.74it/s, Materializing param=model.layers.25.input_layernorm.weight] Loading weights:  89%|████████▉ | 303/339 [00:10<00:00, 37.74it/s, Materializing param=model.layers.25.input_layernorm.weight]Loading weights:  90%|████████▉ | 304/339 [00:10<00:00, 37.74it/s, Materializing param=model.layers.25.mlp.down_proj.weight]  Loading weights:  90%|████████▉ | 304/339 [00:10<00:00, 37.74it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:10<00:00, 42.12it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:10<00:00, 42.12it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:10<00:00, 42.12it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|█████████ | 306/339 [00:10<00:00, 42.12it/s, Materializing param=model.layers.25.mlp.up_proj.weight]  Loading weights:  90%|█████████ | 306/339 [00:10<00:00, 42.12it/s, Materializing param=model.layers.25.mlp.up_proj.weight]Loading weights:  91%|█████████ | 307/339 [00:10<00:00, 42.12it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 307/339 [00:10<00:00, 42.12it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 308/339 [00:10<00:00, 42.12it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]          Loading weights:  91%|█████████ | 308/339 [00:10<00:00, 42.12it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]Loading weights:  91%|█████████ | 309/339 [00:10<00:00, 42.12it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████ | 309/339 [00:10<00:00, 42.12it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  92%|█████████▏| 311/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]  Loading weights:  92%|█████████▏| 311/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]Loading weights:  92%|█████████▏| 312/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 312/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 313/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]  Loading weights:  92%|█████████▏| 313/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]Loading weights:  93%|█████████▎| 314/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 314/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 315/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.26.input_layernorm.weight] Loading weights:  93%|█████████▎| 315/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.26.input_layernorm.weight]Loading weights:  93%|█████████▎| 316/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.26.mlp.down_proj.weight]  Loading weights:  93%|█████████▎| 316/339 [00:10<00:00, 37.27it/s, Materializing param=model.layers.26.mlp.down_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:10<00:00, 42.29it/s, Materializing param=model.layers.26.mlp.down_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:10<00:00, 42.29it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:10<00:00, 42.29it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▍| 318/339 [00:10<00:00, 42.29it/s, Materializing param=model.layers.26.mlp.up_proj.weight]  Loading weights:  94%|█████████▍| 318/339 [00:10<00:00, 42.29it/s, Materializing param=model.layers.26.mlp.up_proj.weight]Loading weights:  94%|█████████▍| 319/339 [00:10<00:00, 42.29it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 319/339 [00:10<00:00, 42.29it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 320/339 [00:10<00:00, 42.29it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]          Loading weights:  94%|█████████▍| 320/339 [00:10<00:00, 42.29it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]Loading weights:  95%|█████████▍| 321/339 [00:10<00:00, 42.29it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 321/339 [00:10<00:00, 42.29it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:10<00:00, 37.53it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:10<00:00, 37.53it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:10<00:00, 37.53it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▌| 323/339 [00:10<00:00, 37.53it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]  Loading weights:  95%|█████████▌| 323/339 [00:10<00:00, 37.53it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]Loading weights:  96%|█████████▌| 324/339 [00:10<00:00, 37.53it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 324/339 [00:10<00:00, 37.53it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 325/339 [00:11<00:00, 37.53it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]  Loading weights:  96%|█████████▌| 325/339 [00:11<00:00, 37.53it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]Loading weights:  96%|█████████▌| 326/339 [00:11<00:00, 37.53it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▌| 326/339 [00:11<00:00, 37.53it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▋| 327/339 [00:11<00:00, 37.53it/s, Materializing param=model.layers.27.input_layernorm.weight] Loading weights:  96%|█████████▋| 327/339 [00:11<00:00, 37.53it/s, Materializing param=model.layers.27.input_layernorm.weight]Loading weights:  97%|█████████▋| 328/339 [00:11<00:00, 37.53it/s, Materializing param=model.layers.27.mlp.down_proj.weight]  Loading weights:  97%|█████████▋| 328/339 [00:11<00:00, 37.53it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:11<00:00, 42.26it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:11<00:00, 42.26it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:11<00:00, 42.26it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 330/339 [00:11<00:00, 42.26it/s, Materializing param=model.layers.27.mlp.up_proj.weight]  Loading weights:  97%|█████████▋| 330/339 [00:11<00:00, 42.26it/s, Materializing param=model.layers.27.mlp.up_proj.weight]Loading weights:  98%|█████████▊| 331/339 [00:11<00:00, 42.26it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 331/339 [00:11<00:00, 42.26it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 332/339 [00:11<00:00, 42.26it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]          Loading weights:  98%|█████████▊| 332/339 [00:11<00:00, 42.26it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]Loading weights:  98%|█████████▊| 333/339 [00:11<00:00, 42.26it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  98%|█████████▊| 333/339 [00:11<00:00, 42.26it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:11<00:00, 36.88it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:11<00:00, 36.88it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:11<00:00, 36.88it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▉| 335/339 [00:11<00:00, 36.88it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]  Loading weights:  99%|█████████▉| 335/339 [00:11<00:00, 36.88it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]Loading weights:  99%|█████████▉| 336/339 [00:11<00:00, 36.88it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 336/339 [00:11<00:00, 36.88it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 337/339 [00:11<00:00, 36.88it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]  Loading weights:  99%|█████████▉| 337/339 [00:11<00:00, 36.88it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]Loading weights: 100%|█████████▉| 338/339 [00:11<00:00, 36.88it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|█████████▉| 338/339 [00:11<00:00, 36.88it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|██████████| 339/339 [00:11<00:00, 36.88it/s, Materializing param=model.norm.weight]                      Loading weights: 100%|██████████| 339/339 [00:11<00:00, 36.88it/s, Materializing param=model.norm.weight]Loading weights: 100%|██████████| 339/339 [00:11<00:00, 29.95it/s, Materializing param=model.norm.weight]
================================================================================
Send prompt for semantic check:
Generated ASP code:
exam(Exam, Duration, Is_Large).

student(Student, Exam).

period(P, Date, Time, Duration, Is_Late, Penalty).

room(Room, Capacity, Penalty).

order_constrain(Exam1, Exam2).

same_time_constraint(Exam1, Exam2).

different_time_constraint(Exam1, Exam2).

own_room_constraint(Exam).

Intended semantics:
Instance Variables:
- Exams: A set of exams that need to be scheduled.
    - Variables: duration, is_large
- Students: A set of students that are taking a set of exams.
    - Variables: exam
- Periods: A set of periods in which the exams can be scheduled.
    - Variables: date, time, duration, is_late, penalty
- Rooms: A set of rooms in which the exams can be scheduled.
    - Variables: capacity, penalty
- Order_Constrains: A set of constraints that specify the order in which some exams must be scheduled.
    - Variables: exam1, exam2
- Same_Time_Constraints: A set of constraints that specify the exams that must be scheduled at the same time.
    - Variables: exam1, exam2
- Different_Time_Constraints: A set of constraints that specify the exams that must not be scheduled at the same time.
    - Variables: exam1, exam2
- Own_Room_Constraints: A set of constraints that specify the exams that must not be scheduled in a room on their own.
    - Variables: exam


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
exam(Exam, Duration, Is_Large).

student(Student, Exam).

period(P, Date, Time, Duration, Is_Late, Penalty).

room(Room, Capacity, Penalty).

order_constrain(Exam1, Exam2).

same_time_constraint(Exam1, Exam2).

different_time_constraint(Exam1, Exam2).

own_room_constraint(Exam).

####################################################################################
Instance Template:
exam(Exam, Duration, Is_Large).

student(Student, Exam).

period(P, Date, Time, Duration, Is_Late, Penalty).

room(Room, Capacity, Penalty).

order_constrain(Exam1, Exam2).

same_time_constraint(Exam1, Exam2).

different_time_constraint(Exam1, Exam2).

own_room_constraint(Exam).
================================================================================
Send prompt for semantic check:
Generated ASP code:
{ scheduled(Exam, P, R) : exam(Exam, _, _, _, _), period(P, _, _, _, _, _), room(R, _, _) }.

Intended semantics:Goal:
An assignment of exams to periods and rooms.
    - Variables: period, room


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
{ scheduled(Exam, P, R) : exam(Exam, _, _, _, _), period(P, _, _, _, _, _), room(R, _, _) }.

####################################################################################


Generator
{ scheduled(Exam, P, R) : exam(Exam, _, _, _, _), period(P, _, _, _, _, _), room(R, _, _) }.


Hard Constraints

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), student(Student, Exam1), student(Student, Exam2), Exam1 != Exam2.

Intended semantics:- No student sits more than one examination at the same time.

Semantic check result:
{
  "match": "false",
  "reason": "The code incorrectly requires that no two different exams can be scheduled in the same room at the same period, which is not the intended semantics. The intended semantics should only restrict students from sitting more than one exam simultaneously."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), student(Student, Exam1), student(Student, Exam2), Exam1 != Exam2.

Intended semantics:- No student sits more than one examination at the same time.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
================================================================================
Semantics are now correct according to LLM. Exiting semantic repair loop.

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), student(Student, Exam1), student(Student, Exam2), Exam1 != Exam2.

####################################################################################
- No student sits more than one examination at the same time.
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), student(Student, Exam1), student(Student, Exam2), Exam1 != Exam2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, student(Student, Exam1), student(Student, Exam2), #count{Exam : scheduled(Exam, P, R), student(Student, Exam)} > room(R, Capacity).

Intended semantics:- The number of students taking exams in a room at the same time should not exceed the capacity of the room.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code incorrectly counts the number of exams a student is taking across all periods and rooms, instead of counting the number of students taking exams in the same room at the same time."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, student(Student, Exam1), student(Student, Exam2), #count{Exam : scheduled(Exam, P, R), student(Student, Exam)} > room(R, Capacity).

Intended semantics:- The number of students taking exams in a room at the same time should not exceed the capacity of the room.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code incorrectly uses 'room(R, Capacity)' instead of 'room(R, Capacity)' in the count predicate, leading to an incorrect count of exams."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, student(Student, Exam1), student(Student, Exam2), #count{Exam : scheduled(Exam, P, R), student(Student, Exam)} > room(R, Capacity).

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, student(Student, Exam1), student(Student, Exam2), #count{Exam : scheduled(Exam, P, R), student(Student, Exam)} > room(R, Capacity).

Intended semantics:- The number of students taking exams in a room at the same time should not exceed the capacity of the room.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code incorrectly uses 'room(R, Capacity)' instead of 'room(R, Capacity)' in the count predicate, leading to an incorrect count of exams."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, student(Student, Exam1), student(Student, Exam2), #count{Exam : scheduled(Exam, P, R), student(Student, Exam)} > room(R, Capacity).

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, student(Student, Exam1), student(Student, Exam2), #count{Exam : scheduled(Exam, P, R), student(Student, Exam)} > room(R, Capacity).

Intended semantics:- The number of students taking exams in a room at the same time should not exceed the capacity of the room.
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
================================================================================
Semantics are now correct according to LLM. Exiting semantic repair loop.

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, student(Student, Exam1), student(Student, Exam2), #count{Exam : scheduled(Exam, P, R), student(Student, Exam)} > room(R, Capacity).

####################################################################################
- The number of students taking exams in a room at the same time should not exceed the capacity of the room.
 
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, student(Student, Exam1), student(Student, Exam2), #count{Exam : scheduled(Exam, P, R), student(Student, Exam)} > room(R, Capacity).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam, P, _), period(P, _, _, Duration, _, _), scheduled(Exam, P2, _), period(P2, _, _, Duration2, _, _), P < P2, Duration != Duration2.

Intended semantics:- Period Lengths are not violated.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that no two exams can be scheduled in the same period, but it does not ensure that the duration of the periods is respected. It only checks that different periods have different start times and durations are not equal, which is redundant."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam, P, _), period(P, _, _, Duration, _, _), scheduled(Exam, P2, _), period(P2, _, _, Duration2, _, _), P < P2, Duration != Duration2.

Intended semantics:- Period Lengths are not violated.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that no two exams can be scheduled in the same period by checking both the start time and the duration, which is unnecessary for ensuring period lengths are not violated. It should only check the start times to ensure no overlap."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
:- scheduled(Exam, P, _), period(P, _, _, Duration, _, _), scheduled(Exam, P2, _), period(P2, _, _, Duration2, _, _), P < P2, Duration != Duration2.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam, P, _), period(P, _, _, Duration, _, _), scheduled(Exam, P2, _), period(P2, _, _, _, _, _), P < P2.

Intended semantics:- Period Lengths are not violated.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that no two exams are scheduled in the same period by comparing their start times but does not ensure that the durations of overlapping periods are consistent. It should only check the start times to ensure no overlap."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
:- scheduled(Exam, P, _), period(P, _, _, Duration, _, _), scheduled(Exam, P2, _), period(P2, _, _, _, _, _), P < P2.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam, P, _), period(P, _, _, Duration, _, _), scheduled(Exam, P2, _), period(P2, _, _, Duration2, _, _), P < P2, Duration != Duration2.

Intended semantics:- Period Lengths are not violated.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that no two exams are scheduled in the same period by comparing both the start times and the durations, which is unnecessary for ensuring period lengths are not violated. It should only check the start times to ensure no overlap."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 3/3:
:- scheduled(Exam, P, _), period(P, _, _, Duration, _, _), scheduled(Exam, P2, _), period(P2, _, _, Duration2, _, _), P < P2, Duration != Duration2.

--------------------------------------------------------------------------------
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
:- scheduled(Exam, P, _), period(P, _, _, Duration, _, _), scheduled(Exam, P2, _), period(P2, _, _, Duration2, _, _), P < P2, Duration != Duration2.

REPAIRED RESPONSE:
:- scheduled(Exam, P, _), period(P, _, _, Duration, _, _), scheduled(Exam, P2, _), period(P2, _, _, _, _, _), P < P2.

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): False

####################################################################################
- Period Lengths are not violated.
:- scheduled(Exam, P, _), period(P, _, _, Duration, _, _), scheduled(Exam, P2, _), period(P2, _, _, _, _, _), P < P2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- order_constrain(Exam1, Exam2), not scheduled(Exam1, P1, _), scheduled(Exam2, P2, _), P1 < P2.

Intended semantics:- Some exams must be before other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code uses 'Exama' which is a typo and should be 'Exam2'. Additionally, it should check if P1 < P2 to ensure the correct order constraint."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- order_constrain(Exam1, Exam2), not scheduled(Exam1, P1, _), scheduled(Exam2, P2, _), P1 < P2.

Intended semantics:- Some exams must be before other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code uses 'Exama' which is a typo and should be 'Exam2'. Additionally, it should check P1 < P2 to ensure the correct order constraint."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
:- order_constrain(Exam1, Exam2), not scheduled(Exam1, P1, _), scheduled(Exam2, P2, _), P1 < P2.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- order_constrain(Exam1, Exam2), not scheduled(Exam1, P1, _), scheduled(Exam2, P2, _), P1 < P2.

Intended semantics:- Some exams must be before other exams.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
================================================================================
Semantics are now correct according to LLM. Exiting semantic repair loop.

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- order_constrain(Exam1, Exam2), not scheduled(Exam1, P1, _), scheduled(Exam2, P2, _), P1 < P2.

####################################################################################
- Some exams must be before other exams.
:- order_constrain(Exam1, Exam2), not scheduled(Exam1, P1, _), scheduled(Exam2, P2, _), P1 < P2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P1, R1), scheduled(Exam2, P2, R2), same_time_constraint(Exam1, Exam2), P1 != P2.

Intended semantics:- Some exams must be at the same time as other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that exams must be scheduled at different periods if they have the same_time_constraint, which is the opposite of the intended semantics."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P1, R1), scheduled(Exam2, P2, R2), same_time_constraint(Exam1, Exam2), P1 != P2.

Intended semantics:- Some exams must be at the same time as other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that exams with the same_time_constraint must be scheduled at different periods, which is the opposite of the intended semantics."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
:- scheduled(Exam1, P1, R1), scheduled(Exam2, P2, R2), same_time_constraint(Exam1, Exam2), P1 != P2.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P1, R1), scheduled(Exam2, P2, R2), same_time_constraint(Exam1, Exam2), P1 != P2.

Intended semantics:- Some exams must be at the same time as other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that exams with the same_time_constraint must be scheduled at different periods, which is the opposite of the intended semantics."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
:- scheduled(Exam1, P1, R1), scheduled(Exam2, P2, R2), same_time_constraint(Exam1, Exam2), P1 != P2.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P1, R1), scheduled(Exam2, P2, R2), same_time_constraint(Exam1, Exam2), P1 != P2.

Intended semantics:- Some exams must be at the same time as other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that exams with the same_time_constraint must be scheduled at different periods, which is the opposite of the intended semantics."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 3/3:
:- scheduled(Exam1, P1, R1), scheduled(Exam2, P2, R2), same_time_constraint(Exam1, Exam2), P1 != P2.

--------------------------------------------------------------------------------
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- scheduled(Exam1, P1, R1), scheduled(Exam2, P2, R2), same_time_constraint(Exam1, Exam2), P1 != P2.

####################################################################################
- Some exams must be at the same time as other exams.
:- scheduled(Exam1, P1, R1), scheduled(Exam2, P2, R2), same_time_constraint(Exam1, Exam2), P1 != P2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P1, _), scheduled(Exam2, P2, _), different_time_constraint(Exam1, Exam2), P1 != P2.

Intended semantics:- Some exams must be at a differen time than other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that exams with a different time constraint must be scheduled in different periods, but it does not ensure that they are actually scheduled at different times."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P1, _), scheduled(Exam2, P2, _), different_time_constraint(Exam1, Exam2), P1 != P2.

Intended semantics:- Some exams must be at a differen time than other exams.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
================================================================================
Semantics are now correct according to LLM. Exiting semantic repair loop.

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- scheduled(Exam1, P1, _), scheduled(Exam2, P2, _), different_time_constraint(Exam1, Exam2), P1 != P2.

####################################################################################
- Some exams must be at a differen time than other exams.
:- scheduled(Exam1, P1, _), scheduled(Exam2, P2, _), different_time_constraint(Exam1, Exam2), P1 != P2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, own_room_constraint(Exam1).

Intended semantics:- Some exams must take place in a room with no other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that if two different exams are scheduled in the same room, then one of them must have the 'own_room_constraint' predicate, which is not necessarily the same as ensuring some exams take place in a room with no other exams."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, own_room_constraint(Exam1).

Intended semantics:- Some exams must take place in a room with no other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that if two different exams are scheduled in the same room, then one of them must have the 'own_room_constraint' predicate, which is not the same as ensuring some exams take place in a room with no other exams."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, own_room_constraint(Exam1).

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, not own_room_constraint(Exam1).

Intended semantics:- Some exams must take place in a room with no other exams.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
================================================================================
Semantics are now correct according to LLM. Exiting semantic repair loop.

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, own_room_constraint(Exam1).

REPAIRED RESPONSE:
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, not own_room_constraint(Exam1).

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): True

####################################################################################
- Some exams must take place in a room with no other exams.
:- scheduled(Exam1, P, R), scheduled(Exam2, P, R), Exam1 != Exam2, not own_room_constraint(Exam1).


Soft Constraints:

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("ExamOverlap",student(Student,E1),7) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 = 1.
penalty("ExamOverlap",student(Student,E1),5) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 > 1.

Intended semantics:- Students should not have more than one exam in the same day
 Penalty: 7 for two exams in the same day that are in consecutive periods.
 Penalty: 5 for two exams in the same day that are not in consecutive periods.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces penalties for overlapping exams within the same room, but it does not ensure that students do not have more than one exam on the same day. The penalties are applied based on the periods and rooms, not the days."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("ExamOverlap",student(Student,E1),7) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 = 1.
penalty("ExamOverlap",student(Student,E1),5) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 > 1.

Intended semantics:- Students should not have more than one exam in the same day
 Penalty: 7 for two exams in the same day that are in consecutive periods.
 Penalty: 5 for two exams in the same day that are not in consecutive periods.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces penalties for overlapping exams within the same room, but it does not ensure that students do not have more than one exam on the same day. The penalties are applied based on the periods and rooms, not the days."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
penalty("ExamOverlap",student(Student,E1),7) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 = 1.
penalty("ExamOverlap",student(Student,E1),5) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 > 1.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("ExamOverlap",student(Student,E1),7) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 = 1, day_of_period(P1,D1), day_of_period(P2,D2), D1 = D2.
penalty("ExamOverlap",student(Student,E1),5) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 > 1, day_of_period(P1,D1), day_of_period(P2,D2), D1 = D2.

Intended semantics:- Students should not have more than one exam in the same day
 Penalty: 7 for two exams in the same day that are in consecutive periods.
 Penalty: 5 for two exams in the same day that are not in consecutive periods.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
================================================================================
Semantics are now correct according to LLM. Exiting semantic repair loop.

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
penalty("ExamOverlap",student(Student,E1),7) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 = 1.
penalty("ExamOverlap",student(Student,E1),5) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 > 1.

REPAIRED RESPONSE:
penalty("ExamOverlap",student(Student,E1),7) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 = 1, day_of_period(P1,D1), day_of_period(P2,D2), D1 = D2.
penalty("ExamOverlap",student(Student,E1),5) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 > 1, day_of_period(P1,D1), day_of_period(P2,D2), D1 = D2.

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): True

####################################################################################
- Students should not have more than one exam in the same day
 Penalty: 7 for two exams in the same day that are in consecutive periods.
 Penalty: 5 for two exams in the same day that are not in consecutive periods.
penalty("ExamOverlap",student(Student,E1),7) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 = 1, day_of_period(P1,D1), day_of_period(P2,D2), D1 = D2.
penalty("ExamOverlap",student(Student,E1),5) :- student(Student,E1), student(Student,E2), E1 < E2, scheduled(E1,P1,R), scheduled(E2,P2,R), P1 < P2, P2 - P1 > 1, day_of_period(P1,D1), day_of_period(P2,D2), D1 = D2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("DifferentLength", scheduled(Exam1, P, R), 10) :- scheduled(Exam1, P, R), scheduled(Exam2, P, R), exam(Exam1, D1, _, _, _), exam(Exam2, D2, _, _, _), D1 != D2.

Intended semantics:- Exams of different lengths should not be scheduled in the same room in the same period
 Penalty: 10 for each exam of different length scheduled in the same room in the same period.

Semantic check result:
{
  "match": "false",
  "reason": "The generated ASP code has syntax errors and incorrectly references itself without proper quantification and predicate names. The intended semantics cannot be accurately captured with the current code."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("DifferentLength", scheduled(Exam1, P, R), 10) :- scheduled(Exam1, P, R), scheduled(Exam2, P, R), exam(Exam1, D1, _, _, _), exam(Exam2, D2, _, _, _), D1 != D2.

Intended semantics:- Exams of different lengths should not be scheduled in the same room in the same period
 Penalty: 10 for each exam of different length scheduled in the same room in the same period.

Semantic check result:
{
  "match": "false",
  "reason": "The generated ASP code does not enforce the constraint that exams of different lengths should not be scheduled in the same room in the same period. It only checks for exams of different lengths being scheduled in the same room, but does not ensure they are in the same period."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
penalty("DifferentLength", scheduled(Exam1, P, R), 10) :- scheduled(Exam1, P, R), scheduled(Exam2, P, R), exam(Exam1, D1, _, _, _), exam(Exam2, D2, _, _, _), D1 != D2.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("DifferentLength", scheduled(Exam1, P, R), 10) :- scheduled(Exam1, P, R), scheduled(Exam2, P, R), exam(Exam1, D1, _, _, _), exam(Exam2, D2, _, _, _), D1 != D2.

Intended semantics:- Exams of different lengths should not be scheduled in the same room in the same period
 Penalty: 10 for each exam of different length scheduled in the same room in the same period.

Semantic check result:
{
  "match": "false",
  "reason": "The generated ASP code does not correctly enforce the constraint that exams of different lengths should not be scheduled in the same room in the same period. It only checks for exams of different lengths being scheduled in the same room, but does not ensure they are in the same period."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
penalty("DifferentLength", scheduled(Exam1, P, R), 10) :- scheduled(Exam1, P, R), scheduled(Exam2, P, R), exam(Exam1, D1, _, _, _), exam(Exam2, D2, _, _, _), D1 != D2.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("DifferentLength", scheduled(Exam1, P, R), 10) :- scheduled(Exam1, P, R), scheduled(Exam2, P, R), exam(Exam1, D1, _, _, _), exam(Exam2, D2, _, _, _), D1 != D2.

Intended semantics:- Exams of different lengths should not be scheduled in the same room in the same period
 Penalty: 10 for each exam of different length scheduled in the same room in the same period.

Semantic check result:
{
  "match": "false",
  "reason": "The generated ASP code does not correctly enforce the constraint that exams of different lengths should not be scheduled in the same room in the same period. It only checks for exams of different lengths being scheduled in the same room, but does not ensure they are in the same period."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 3/3:
penalty("DifferentLength", scheduled(Exam1, P, R), 10) :- scheduled(Exam1, P, R), scheduled(Exam2, P, R), exam(Exam1, D1, _, _, _), exam(Exam2, D2, _, _, _), D1 != D2.

--------------------------------------------------------------------------------
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("DifferentLength", scheduled(Exam1, P, R), 10) :- scheduled(Exam1, P, R), scheduled(Exam2, P, R), exam(Exam1, D1, _, _, _), exam(Exam2, D2, _, _, _), D1 != D2.

####################################################################################
- Exams of different lengths should not be scheduled in the same room in the same period
 Penalty: 10 for each exam of different length scheduled in the same room in the same period.
penalty("DifferentLength", scheduled(Exam1, P, R), 10) :- scheduled(Exam1, P, R), scheduled(Exam2, P, R), exam(Exam1, D1, _, _, _), exam(Exam2, D2, _, _, _), D1 != D2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("PeriodPenalty", Exam, P, Penalty) :- scheduled(Exam, P, _), period(P, _, _, _, _, Penalty).

Intended semantics:- Some periods have associated penalties
 Penalty: 1 for each time an exam is scheduled in a period with an associated penalty. Weighted by the penalty value of the period.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("PeriodPenalty", Exam, P, Penalty) :- scheduled(Exam, P, _), period(P, _, _, _, _, Penalty).

####################################################################################
- Some periods have associated penalties
 Penalty: 1 for each time an exam is scheduled in a period with an associated penalty. Weighted by the penalty value of the period.
penalty("PeriodPenalty", Exam, P, Penalty) :- scheduled(Exam, P, _), period(P, _, _, _, _, Penalty).

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("RoomPenalty",exam(Exam,P,R),PEN) :- scheduled(Exam,P,R), room(R,_,PEN).

Intended semantics:- Some rooms have associated penalties
 Penalty: 1 for each time an exam is scheduled in a room with an associated penalty. Weighted by the penalty value of the room.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("RoomPenalty",exam(Exam,P,R),PEN) :- scheduled(Exam,P,R), room(R,_,PEN).

####################################################################################
- Some rooms have associated penalties
 Penalty: 1 for each time an exam is scheduled in a room with an associated penalty. Weighted by the penalty value of the room.
penalty("RoomPenalty",exam(Exam,P,R),PEN) :- scheduled(Exam,P,R), room(R,_,PEN).

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("ExamSpread",student(S,E),3) :- student(S,E), M = #max { P : scheduled(E,P,R) }, m = #min { P : scheduled(E,P,R) }, M - m > 7.

Intended semantics:- Students should not have more than one exam within a spread of 7 periods.
 Penalty: 3 for each time a student has two exams within a spread of periods.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("ExamSpread",student(S,E),3) :- student(S,E), M = #max { P : scheduled(E,P,R) }, m = #min { P : scheduled(E,P,R) }, M - m > 7.

####################################################################################
- Students should not have more than one exam within a spread of 7 periods.
 Penalty: 3 for each time a student has two exams within a spread of periods.
penalty("ExamSpread",student(S,E),3) :- student(S,E), M = #max { P : scheduled(E,P,R) }, m = #min { P : scheduled(E,P,R) }, M - m > 7.

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("LateLargeExam", scheduled(Exam, P, R), 5) :- exam(Exam, _, _, _, Is_Large), period(P, _, _, _, Is_Late, _), Is_Large = 1, Is_Late = 1.

Intended semantics:- An exam that is large should not be scheduled in a period that is late.
 Penalty: 5 for each large exam scheduled in a period that is late.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for each large exam scheduled in a period that is late, whereas the intended semantics only state that such scheduling should not occur without specifying a penalty."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("LateLargeExam", scheduled(Exam, P, R), 5) :- exam(Exam, _, _, _, Is_Large), period(P, _, _, _, Is_Late, _), Is_Large = 1, Is_Late = 1.

Intended semantics:- An exam that is large should not be scheduled in a period that is late.
 Penalty: 5 for each large exam scheduled in a period that is late.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for each large exam scheduled in a period that is late, whereas the intended semantics only state that such scheduling should not occur without specifying a penalty."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
penalty("LateLargeExam", scheduled(Exam, P, R), 5) :- exam(Exam, _, _, _, Is_Large), period(P, _, _, _, Is_Late, _), Is_Large = 1, Is_Late = 1.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- exam(Exam, _, _, _, Is_Large), period(P, _, _, _, Is_Late, _), Is_Large = 1, Is_Late = 1.

Intended semantics:- An exam that is large should not be scheduled in a period that is late.
 Penalty: 5 for each large exam scheduled in a period that is late.

Semantic check result:
{
  "match": "false",
  "reason": "The code forbids any large exam from being scheduled in a period that is late, whereas the intended semantics only state that a penalty should be applied, not a strict prohibition."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
:- exam(Exam, _, _, _, Is_Large), period(P, _, _, _, Is_Late, _), Is_Large = 1, Is_Late = 1.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("LateLargeExam", scheduled(Exam, P, R), 5) :- exam(Exam, _, _, _, Is_Large), period(P, _, _, _, Is_Late, _), Is_Large = 1, Is_Late = 1.

Intended semantics:- An exam that is large should not be scheduled in a period that is late.
 Penalty: 5 for each large exam scheduled in a period that is late.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for each large exam scheduled in a period that is late, whereas the intended semantics only state that such scheduling should not occur without specifying a penalty."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 3/3:
penalty("LateLargeExam", scheduled(Exam, P, R), 5) :- exam(Exam, _, _, _, Is_Large), period(P, _, _, _, Is_Late, _), Is_Large = 1, Is_Late = 1.

--------------------------------------------------------------------------------
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
penalty("LateLargeExam", scheduled(Exam, P, R), 5) :- exam(Exam, _, _, _, Is_Large), period(P, _, _, _, Is_Late, _), Is_Large = 1, Is_Late = 1.

REPAIRED RESPONSE:
:- exam(Exam, _, _, _, Is_Large), period(P, _, _, _, Is_Late, _), Is_Large = 1, Is_Late = 1.

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): False

####################################################################################
- An exam that is large should not be scheduled in a period that is late.
 Penalty: 5 for each large exam scheduled in a period that is late.
:- exam(Exam, _, _, _, Is_Large), period(P, _, _, _, Is_Late, _), Is_Large = 1, Is_Late = 1.

Full program saved to Results/experiments_2/examination_timetabling_ft_qwen_mlx_k=3_n=3_20260111_102824.lp
================================================================================
Send prompt for semantic check:
Generated ASP code:
event(Event).

room(Room, Capacity).

timeslot(Day, Hour).

student(Student, Event).

feature_requirement(FEATURE, EVENT).

room_feature(FEATURE, ROOM).

precedence_constraint(EVENT1, EVENT2).

timeslot_constraint(EVENT, TIMESLOT).

Intended semantics:
- Events: A set of N events.
    - Variables: none
- Rooms: A set of rooms in which the events can be scheduled.
    - Variables: capacity
- Timeslots: A set of timeslots in which the events can be scheduled.
    - Variables: day, hour
- Students: A set of students that are attending a set of events.
    - Variables: event
- Feature Requirements: A set of features which events may have.
    - Variables: feature, event
- Room Features: A set of features which rooms may have.
    - Variables: feature, room
- Precedence Constraints: A set of constraints that specify if some events must be scheduled before other events.
    - Variables: event1, event2
- Timeslot Constraints: A set of constraints for each event that specify the timeslots in which the event can be scheduled.
    - Variables: event, timeslot


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
event(Event).

room(Room, Capacity).

timeslot(Day, Hour).

student(Student, Event).

feature_requirement(FEATURE, EVENT).

room_feature(FEATURE, ROOM).

precedence_constraint(EVENT1, EVENT2).

timeslot_constraint(EVENT, TIMESLOT).

####################################################################################
Instance Template:
event(Event).

room(Room, Capacity).

timeslot(Day, Hour).

student(Student, Event).

feature_requirement(FEATURE, EVENT).

room_feature(FEATURE, ROOM).

precedence_constraint(EVENT1, EVENT2).

timeslot_constraint(EVENT, TIMESLOT).
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).

Intended semantics:
An assignment of events to timeslots and rooms.
    - Variables: timeslot, room


Semantic check result:
{
  "match": "false",
  "reason": "The code requires each event to be assigned to exactly one timeslot and room, but it does not enforce that each timeslot and room combination must be used."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).

Intended semantics:
An assignment of events to timeslots and rooms.
    - Variables: timeslot, room


Semantic check result:
{
  "match": "false",
  "reason": "The code requires each event to be assigned to exactly one timeslot and room, but it does not enforce that each timeslot and room combination must be used."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).

Intended semantics:
An assignment of events to timeslots and rooms.
    - Variables: timeslot, room


Semantic check result:
{
  "match": "false",
  "reason": "The code requires each event to be assigned to exactly one timeslot and room, but it does not enforce that each timeslot and room combination must be used."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).

Intended semantics:
An assignment of events to timeslots and rooms.
    - Variables: timeslot, room


Semantic check result:
{
  "match": "false",
  "reason": "The code requires each event to be assigned to exactly one timeslot and room, but it does not enforce that each timeslot and room combination must be used."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 3/3:
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).

--------------------------------------------------------------------------------
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).

####################################################################################


Generator
1 { assigned(Event, Timeslot, Room) : event(Event) } 1 :- room(Room, _), timeslot(Timeslot).


Hard Constraints

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- student(Student, Event1), student(Student, Event2), Event1 != Event2, timeslot(Timeslot1), timeslot(Timeslot2), assigned(Event1, Timeslot1, _), assigned(Event2, Timeslot2, _), Timeslot1 < Timeslot2, Timeslot2 - Timeslot1 = 1.

Intended semantics:- Students must not have overlapping events.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- student(Student, Event1), student(Student, Event2), Event1 != Event2, timeslot(Timeslot1), timeslot(Timeslot2), assigned(Event1, Timeslot1, _), assigned(Event2, Timeslot2, _), Timeslot1 < Timeslot2, Timeslot2 - Timeslot1 = 1.

####################################################################################
- Students must not have overlapping events.
:- student(Student, Event1), student(Student, Event2), Event1 != Event2, timeslot(Timeslot1), timeslot(Timeslot2), assigned(Event1, Timeslot1, _), assigned(Event2, Timeslot2, _), Timeslot1 < Timeslot2, Timeslot2 - Timeslot1 = 1.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), student(Student, Event1), student(Student, Event2), Event1 != Event2, room(Room, Capacity), #count{Student : student(Student, Event1), student(Student, Event2)} > Capacity.

Intended semantics:- The amount of students in a room at the same time should not exceed the capacity of the room
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), student(Student, Event1), student(Student, Event2), Event1 != Event2, room(Room, Capacity), #count{Student : student(Student, Event1), student(Student, Event2)} > Capacity.

####################################################################################
- The amount of students in a room at the same time should not exceed the capacity of the room
 
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), student(Student, Event1), student(Student, Event2), Event1 != Event2, room(Room, Capacity), #count{Student : student(Student, Event1), student(Student, Event2)} > Capacity.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), not feature_requirement(FEATURE, Event).

Intended semantics:- Rooms must have the required features for the event.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), not feature_requirement(FEATURE, Event).

####################################################################################
- Rooms must have the required features for the event.
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), not feature_requirement(FEATURE, Event).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), Event1 != Event2.

Intended semantics:- There must be no more than one event in a room at a time.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), Event1 != Event2.

####################################################################################
- There must be no more than one event in a room at a time.
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), Event1 != Event2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event, Timeslot, Room), not timeslot_constraint(Event, Timeslot).

Intended semantics:- Events may only be scheduled in designated timeslots.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event, Timeslot, Room), not timeslot_constraint(Event, Timeslot).

####################################################################################
- Events may only be scheduled in designated timeslots.
:- assigned(Event, Timeslot, Room), not timeslot_constraint(Event, Timeslot).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event1, Timeslot1, Room), assigned(Event2, Timeslot2, Room), Timeslot1 < Timeslot2, precedence_constraint(Event1, Event2).

Intended semantics:- Some events must be scheduled in a specific order.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that if two events are scheduled in the same room and one event precedes another in time, then the first event must be scheduled before the second event. However, it does not ensure that the events are scheduled in the correct order if they are in different timeslots or rooms."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event1, Timeslot1, Room), assigned(Event2, Timeslot2, Room), Timeslot1 < Timeslot2, precedence_constraint(Event1, Event2).

Intended semantics:- Some events must be scheduled in a specific order.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that if two events are scheduled in the same room and one event precedes another in time, then the first event must be scheduled before the second event. However, it does not ensure that the events are scheduled in the correct order if they are in different rooms."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
:- assigned(Event1, Timeslot1, Room), assigned(Event2, Timeslot2, Room), Timeslot1 < Timeslot2, precedence_constraint(Event1, Event2).

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event1, Timeslot1, Room1), assigned(Event2, Timeslot2, Room2), Timeslot1 < Timeslot2, precedence_constraint(Event1, Event2).

Intended semantics:- Some events must be scheduled in a specific order.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that if two events are scheduled in different rooms and one event precedes another in time, then the first event must be scheduled before the second event. However, it does not ensure that the events are scheduled in the correct order if they are in the same room."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
:- assigned(Event1, Timeslot1, Room1), assigned(Event2, Timeslot2, Room2), Timeslot1 < Timeslot2, precedence_constraint(Event1, Event2).

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event1, Timeslot1, Room), assigned(Event2, Timeslot2, Room), Timeslot1 < Timeslot2, precedence_constraint(Event1, Event2).

Intended semantics:- Some events must be scheduled in a specific order.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
================================================================================
Semantics are now correct according to LLM. Exiting semantic repair loop.

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event1, Timeslot1, Room), assigned(Event2, Timeslot2, Room), Timeslot1 < Timeslot2, precedence_constraint(Event1, Event2).

####################################################################################
- Some events must be scheduled in a specific order.
:- assigned(Event1, Timeslot1, Room), assigned(Event2, Timeslot2, Room), Timeslot1 < Timeslot2, precedence_constraint(Event1, Event2).


Soft Constraints:

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("LastTimeslot",student(Student,Event),1) :- student(Student,Event), assigned(Event,Timeslot,Room), timeslot(Day,Hour), last_timeslot(Day,Hour).

Intended semantics:- Students should not have events scheduled in the last timeslot of the day
 Penalty: 1 for each student with an event in the last timeslot of the day.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("LastTimeslot",student(Student,Event),1) :- student(Student,Event), assigned(Event,Timeslot,Room), timeslot(Day,Hour), last_timeslot(Day,Hour).

####################################################################################
- Students should not have events scheduled in the last timeslot of the day
 Penalty: 1 for each student with an event in the last timeslot of the day.
penalty("LastTimeslot",student(Student,Event),1) :- student(Student,Event), assigned(Event,Timeslot,Room), timeslot(Day,Hour), last_timeslot(Day,Hour).

================================================================================
Initial response with syntax error:
penalty("ConsecutiveTimeslots",Student,3) :- student(Student,Event), assigned(Event,Timeslot,_,_), N = { Timeslot' : assigned(Event,Timeslot',_,_,_) }, N >= 3.

Error: <string>:1:116-117: error: syntax error, unexpected :

 Starting syntax repair attempts...
================================================================================
--------------------------------------------------------------------------------
Correction attempt 1:
penalty("ConsecutiveTimeslots", Student, 3) :- student(Student, Event), assigned(Event, Timeslot, _, _), N = #count{ Timeslot' : assigned(Event, Timeslot', _, _, _) }, N >= 3.

--------------------------------------------------------------------------------
Syntax corrected successfully!

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("ConsecutiveTimeslots", Student, 3) :- student(Student, Event), assigned(Event, Timeslot, _, _), N = #count{ Timeslot' : assigned(Event, Timeslot', _, _, _) }, N >= 3.

Intended semantics:- Students should not have events scheduled in three or more consecutive timeslots
 Penalty: 1 for each student with an event in three or more consecutive timeslots.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
penalty("ConsecutiveTimeslots",Student,3) :- student(Student,Event), assigned(Event,Timeslot,_,_), N = { Timeslot' : assigned(Event,Timeslot',_,_,_) }, N >= 3.

REPAIRED RESPONSE:
penalty("ConsecutiveTimeslots", Student, 3) :- student(Student, Event), assigned(Event, Timeslot, _, _), N = #count{ Timeslot' : assigned(Event, Timeslot', _, _, _) }, N >= 3.

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): True

####################################################################################
- Students should not have events scheduled in three or more consecutive timeslots
 Penalty: 1 for each student with an event in three or more consecutive timeslots.
penalty("ConsecutiveTimeslots", Student, 3) :- student(Student, Event), assigned(Event, Timeslot, _, _), N = #count{ Timeslot' : assigned(Event, Timeslot', _, _, _) }, N >= 3.

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("SingleEventPerDay", student(Student), 1) :- #count{Event : assigned(Event, Timeslot, Room), student(Student, Event)} = 1.

Intended semantics:- Students should not have exactly one event in a day
 Penalty: 1 for each time a student has exactly one event in a day.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for students having exactly one event in a day, whereas the intended semantics is to penalize cases where students do not have exactly one event in a day."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("SingleEventPerDay", student(Student), 1) :- #count{Event : assigned(Event, Timeslot, Room), student(Student, Event)} = 1.

Intended semantics:- Students should not have exactly one event in a day
 Penalty: 1 for each time a student has exactly one event in a day.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for students having exactly one event in a day, whereas the intended semantics is to penalize cases where students do not have exactly one event in a day."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
penalty("SingleEventPerDay", student(Student), 1) :- #count{Event : assigned(Event, Timeslot, Room), student(Student, Event)} = 1.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("SingleEventPerDay", student(Student), 1) :- #count{Event : assigned(Event, Timeslot, Room), student(Student, Event)} != 1.

Intended semantics:- Students should not have exactly one event in a day
 Penalty: 1 for each time a student has exactly one event in a day.
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
================================================================================
Semantics are now correct according to LLM. Exiting semantic repair loop.

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
penalty("SingleEventPerDay", student(Student), 1) :- #count{Event : assigned(Event, Timeslot, Room), student(Student, Event)} = 1.

REPAIRED RESPONSE:
penalty("SingleEventPerDay", student(Student), 1) :- #count{Event : assigned(Event, Timeslot, Room), student(Student, Event)} != 1.

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): True

####################################################################################
- Students should not have exactly one event in a day
 Penalty: 1 for each time a student has exactly one event in a day.
 
penalty("SingleEventPerDay", student(Student), 1) :- #count{Event : assigned(Event, Timeslot, Room), student(Student, Event)} != 1.

Full program saved to Results/experiments_2/post_enrollment_based_course_time_tabling_ft_qwen_mlx_k=3_n=3_20260111_104623.lp
================================================================================
Send prompt for semantic check:
Generated ASP code:
nurse(Nurse).

day(Day).

shift_type(Shift_Type, Duration).

nurse_requirement(Shift, Min, Max).

work_requirement(Nurse, Min, Max).

shift_requirement(Shift_Type, Min, Max, Preferred).

Intended semantics:
Instance Variables:
- Nurses: A set of numbered nurses.
    - Variables: none
- Days: A set of numbered days.
    - Variables: none
- Shift_types: A set of shifts types that the nurses can be assigned to.
    - Variables: shift_type, duration
- nurse_requirements: Min and max number of nurses required for each shift type.
    - Variables: shift, min, max
- work_requirements: Min and max number of hours each nurse must work.
    - Variables: min, max
- shift_requirements: For each shift type, there is a minimum and maximum number of times a nurse must work this shift. There is also a preferred number of each shift type.
    - Variables: shift_type, min, max, preferred


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
nurse(Nurse).

day(Day).

shift_type(Shift_Type, Duration).

nurse_requirement(Shift, Min, Max).

work_requirement(Nurse, Min, Max).

shift_requirement(Shift_Type, Min, Max, Preferred).

####################################################################################
Instance Template:
nurse(Nurse).

day(Day).

shift_type(Shift_Type, Duration).

nurse_requirement(Shift, Min, Max).

work_requirement(Nurse, Min, Max).

shift_requirement(Shift_Type, Min, Max, Preferred).
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { works(Nurse, Shift_Type, DDay) : day(DDay) } 1 :- nurse(Nurse).

Intended semantics:Goal:
An assignment of nurses to shifts_types and days. There must be one assignment for each nurse and each day.
    - Variables: nurse, shift, day


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
1 { works(Nurse, Shift_Type, DDay) : day(DDay) } 1 :- nurse(Nurse).

####################################################################################


Generator
1 { works(Nurse, Shift_Type, DDay) : day(DDay) } 1 :- nurse(Nurse).


Hard Constraints

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- shift_type(Shift_Type, _), day(DDay), #count{Nurse : works(Nurse, Shift_Type, DDay)} < Min, nurse_requirement(Shift_Type, Min, _).
:- shift_type(Shift_Type, _), day(DDay), #count{Nurse : works(Nurse, Shift_Type, DDay)} > Max, nurse_requirement(Shift_Type, _, Max).

Intended semantics:- Every day, the number of nurses assigned to each shift must be between the specified minimum and maximum
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- shift_type(Shift_Type, _), day(DDay), #count{Nurse : works(Nurse, Shift_Type, DDay)} < Min, nurse_requirement(Shift_Type, Min, _).
:- shift_type(Shift_Type, _), day(DDay), #count{Nurse : works(Nurse, Shift_Type, DDay)} > Max, nurse_requirement(Shift_Type, _, Max).

####################################################################################
- Every day, the number of nurses assigned to each shift must be between the specified minimum and maximum
 
:- shift_type(Shift_Type, _), day(DDay), #count{Nurse : works(Nurse, Shift_Type, DDay)} < Min, nurse_requirement(Shift_Type, Min, _).
:- shift_type(Shift_Type, _), day(DDay), #count{Nurse : works(Nurse, Shift_Type, DDay)} > Max, nurse_requirement(Shift_Type, _, Max).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).

Intended semantics:- Each nurse must work at least the specified minimum and at most the specified maximum number of hours
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces global minimum and maximum constraints for all nurses combined, rather than individual constraints for each nurse."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).

Intended semantics:- Each nurse must work at least the specified minimum and at most the specified maximum number of hours
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces global minimum and maximum constraints for all nurses combined, rather than individual constraints for each nurse."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).

Intended semantics:- Each nurse must work at least the specified minimum and at most the specified maximum number of hours
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces global minimum and maximum constraints for all nurses combined, rather than individual constraints for each nurse."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).

Intended semantics:- Each nurse must work at least the specified minimum and at most the specified maximum number of hours
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces global minimum and maximum constraints for all nurses combined, rather than individual constraints for each nurse."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 3/3:
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).

--------------------------------------------------------------------------------
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).

####################################################################################
- Each nurse must work at least the specified minimum and at most the specified maximum number of hours
 
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Duration, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

Intended semantics:- Nurses must have exactly 30 days of vacation.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each nurse must have exactly 30 days of vacation, not exactly 31 days."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

Intended semantics:- Nurses must have exactly 30 days of vacation.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each nurse must have exactly 31 days of vacation, not exactly 30 days."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

Intended semantics:- Nurses must have exactly 30 days of vacation.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each nurse must have exactly 31 days of vacation, not exactly t31 days."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

Intended semantics:- Nurses must have exactly 30 days of vacation.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each nurse must have exactly 31 days of vacation, not exactly 30 days."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 3/3:
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

--------------------------------------------------------------------------------
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

####################################################################################
- Nurses must have exactly 30 days of vacation.
 
:- nurse(Nurse), #count{DDay : works(Nurse, vacation, DDay)} != 30.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- works(Nurse, night, DDay), works(Nurse, morning, DDay+1).
:- works(Nurse, night, DDay), works(Nurse, afternoon, DDay+1).

Intended semantics:- If a nurse works a night shift, they can not work a morning or afternoon shift the next day

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- works(Nurse, night, DDay), works(Nurse, morning, DDay+1).
:- works(Nurse, night, DDay), works(Nurse, afternoon, DDay+1).

####################################################################################
- If a nurse works a night shift, they can not work a morning or afternoon shift the next day
:- works(Nurse, night, DDay), works(Nurse, morning, DDay+1).
:- works(Nurse, night, DDay), works(Nurse, afternoon, DDay+1).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- works(Nurse, "afternoon", DDay), works(Nurse, "morning", DDay+1).

Intended semantics:- If a nurse works an afternoon shift, they cannot work a morning shift the next day.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- works(Nurse, "afternoon", DDay), works(Nurse, "morning", DDay+1).

####################################################################################
- If a nurse works an afternoon shift, they cannot work a morning shift the next day.
:- works(Nurse, "afternoon", DDay), works(Nurse, "morning", DDay+1).

================================================================================
Initial response with syntax error:
:- nurse(Nurse), day(DDay), day(DDay2), DDay < DDay2, DDay2 - DDay = #const{days_in_window}, #count{DDay3 : day(DDay3), DDay3 < DDay, DDay3 + 1 < DDay2, works(Nurse, "rest", DDay3), works(Nurse, "rest", DDay3 + 1)} < 2.

Error: <string>:1:70-76: error: syntax error, unexpected #const

 Starting syntax repair attempts...
================================================================================
--------------------------------------------------------------------------------
Correction attempt 1:
:- nurse(Nurse), day(DDay), day(DDay2), DDay < DDay2, DDay2 - DDay = #const{days_in_window}, #count{DDay2 : day(DDay2), DDay2 < DDay, DDay2 + 1 < DDay2, works(Nurse, "rest", DDay2), works(Nurse, "rest", DDay2 + 1)} < 2.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:70-76: error: syntax error, unexpected #const

--------------------------------------------------------------------------------
Correction attempt 2:
:- nurse(Nurse), day(DDay), day(DDay2), DDay < DDay2, DDay2 - DDay = #const{days_in_window}, #count{DDay2 : day(DDay2), DDay2 < DDay, DDay2 + 1 < DDay2, works(Nurse, "rest", DDay2), works(Nurse, "rest", DDay2 + 1)} < 2.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:70-76: error: syntax error, unexpected #const

--------------------------------------------------------------------------------
Correction attempt 3:
:- nurse(Nurse), day(DDay), day(DDay2), DDay < DDay2, DDay2 - DDay = #const{days_in_window}, #count{DDay2 : day(DDay2), DDay2 < DDay, DDay2 + 1 < DDay2, works(Nurse, "rest", DDay2), works(Nurse, "rest", DDay2 + 1)} < 2.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:70-76: error: syntax error, unexpected #const

================================================================================
Syntax errors remain after repair attempts. Cannot proceed to semantic checking.
Total syntax errors: 1

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED , BUT UNSUCCESSFULLY
INITIAL RESPONSE:
:- nurse(Nurse), day(DDay), day(DDay2), DDay < DDay2, DDay2 - DDay = #const{days_in_window}, #count{DDay3 : day(DDay3), DDay3 < DDay, DDay3 + 1 < DDay2, works(Nurse, "rest", DDay3), works(Nurse, "rest", DDay3 + 1)} < 2.

REPAIRED RESPONSE:
:- nurse(Nurse), day(DDay), day(DDay2), DDay < DDay2, DDay2 - DDay = #const{days_in_window}, #count{DDay2 : day(DDay2), DDay2 < DDay, DDay2 + 1 < DDay2, works(Nurse, "rest", DDay2), works(Nurse, "rest", DDay2 + 1)} < 2.

Total syntax errors remaining after repair attempts: 1

Semantics correct (according to LLM): False

####################################################################################
- Each nurse has at least two ordinary rest days for every window of fourteen days
 
:- nurse(Nurse), day(DDay), day(DDay2), DDay < DDay2, DDay2 - DDay = #const{days_in_window}, #count{DDay2 : day(DDay2), DDay2 < DDay, DDay2 + 1 < DDay2, works(Nurse, "rest", DDay2), works(Nurse, "rest", DDay2 + 1)} < 2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), not works(Nurse, special_rest, DDay+1), not works(Nurse, rest, DDay+1).

Intended semantics:- Nurses working on two consecutive nights deserve one special_rest day in addition to the ordinary rest days

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that if a nurse works two consecutive nights, they cannot work a special_rest or rest day immediately after the second night, which does not match the intended semantics of deserving an additional special_rest day."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), not works(Nurse, special_rest, DDay+1), not works(Nurse, rest, DDay+1).

Intended semantics:- Nurses working on two consecutive nights deserve one special_rest day in addition to the ordinary rest days

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that if a nurse works two consecutive nights, they cannot work either a special_rest or rest day immediately after the second night. This does not match the intended semantics of deserving an additional special_rest day."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), not works(Nurse, special_rest, DDay+1), not works(Nurse, rest, DDay+1).

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), not works(Nurse, special_rest, DDay+1), not works(Nurse, rest, DDay+1).

Intended semantics:- Nurses working on two consecutive nights deserve one special_rest day in addition to the ordinary rest days

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that if a nurse works two consecutive nights, they cannot work either a special_rest or rest day immediately after the second night. This does not match the intended semantics of deserving an additional special_rest day."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), not works(Nurse, special_rest, DDay+1), not works(Nurse, rest, DDay+1).

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), not works(Nurse, special_rest, DDay+1), not works(Nurse, rest, DDay+1).

Intended semantics:- Nurses working on two consecutive nights deserve one special_rest day in addition to the ordinary rest days

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that if a nurse works two consecutive nights, they cannot work either a special_rest or rest day immediately after the second night. This does not match the intended semantics of deserving an additional special_rest day."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 3/3:
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), not works(Nurse, special_rest, DDay+1), not works(Nurse, rest, DDay+1).

--------------------------------------------------------------------------------
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), not works(Nurse, special_rest, DDay+1), not works(Nurse, rest, DDay+1).

####################################################################################
- Nurses working on two consecutive nights deserve one special_rest day in addition to the ordinary rest days
:- works(Nurse, night, DDay), works(Nurse, night, DDay+1), not works(Nurse, special_rest, DDay+1), not works(Nurse, rest, DDay+1).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #sum{Hours, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Hours, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).

Intended semantics:- The total number of hours worked by each nurse must be between the specified minimum and maximum
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- nurse(Nurse), #sum{Hours, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Hours, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).

####################################################################################
- The total number of hours worked by each nurse must be between the specified minimum and maximum
 
:- nurse(Nurse), #sum{Hours, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} < Min, work_requirement(Nurse, Min, _).
:- nurse(Nurse), #sum{Hours, Shift_Type, DDay: works(Nurse, Shift_Type, DDay), shift_type(Shift_Type, Duration)} > Max, work_requirement(Nurse, _, Max).


Soft Constraints:

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("shift_preference", works(Nurse, Shift_Type, DDay), 1) :- works(Nurse, Shift_Type, DDay), shift_requirement(Shift_Type, _, Preferred, _), works(Nurse, Shift_Type, DDay) < Preferred.
penalty("shift_preference", works(Nurse, Shift_Type, DDay), 1) :- works(Nurse, Shift_Type, DDay), shift_requirement(Shift_Type, _, Preferred, _), works(Nurse, Shift_Type, DDay) > Preferred.

Intended semantics:- Nurses work the preferred number of shifts for each shift type
 Penalty: 1 for each shift below the preferred number
 Penalty: 1 for each shift above the preferred number
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("shift_preference", works(Nurse, Shift_Type, DDay), 1) :- works(Nurse, Shift_Type, DDay), shift_requirement(Shift_Type, _, Preferred, _), works(Nurse, Shift_Type, DDay) < Preferred.
penalty("shift_preference", works(Nurse, Shift_Type, DDay), 1) :- works(Nurse, Shift_Type, DDay), shift_requirement(Shift_Type, _, Preferred, _), works(Nurse, Shift_Type, DDay) > Preferred.

####################################################################################
- Nurses work the preferred number of shifts for each shift type
 Penalty: 1 for each shift below the preferred number
 Penalty: 1 for each shift above the preferred number
 
penalty("shift_preference", works(Nurse, Shift_Type, DDay), 1) :- works(Nurse, Shift_Type, DDay), shift_requirement(Shift_Type, _, Preferred, _), works(Nurse, Shift_Type, DDay) < Preferred.
penalty("shift_preference", works(Nurse, Shift_Type, DDay), 1) :- works(Nurse, Shift_Type, DDay), shift_requirement(Shift_Type, _, Preferred, _), works(Nurse, Shift_Type, DDay) > Preferred.

Full program saved to Results/experiments_2/nurse_scheduling_ft_qwen_mlx_k=3_n=3_20260111_105512.lp
================================================================================
Send prompt for semantic check:
Generated ASP code:
team(Team).

venue(Venue).

availability(Venue, Date).

Intended semantics:
- Teams: a set of teams that plays in the league
    - Variables: team
- Venues: a set of venues that is available
    - Variables: venues
- Availabilities: A set of dates at which a given venue is available 
    - Variables: venues, dates
    

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
team(Team).

venue(Venue).

availability(Venue, Date).

####################################################################################
Instance Template:
team(Team).

venue(Venue).

availability(Venue, Date).
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { game(Team1, Team2, Venue, Gameday) : team(Team1), team(Team2), venue(Venue), gameday(Gameday) }.

Intended semantics:
An assignment of games to pairs of teams, venues and game days.
    - Variables: team1, team2, venue, gameday

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces exactly one game for each combination of teams, venue, and gameday, whereas the intended semantics only requires an assignment without specifying uniqueness."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { game(Team1, Team2, Venue, Gameday) : team(Team1), team(Team2), venue(Venue), gameday(Gameday) }.

Intended semantics:
An assignment of games to pairs of teams, venues and game days.
    - Variables: team1, team2, venue, gameday

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
================================================================================
Semantics are now correct according to LLM. Exiting semantic repair loop.

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
1 { game(Team1, Team2, Venue, Gameday) : team(Team1), team(Team2), venue(Venue), gameday(Gameday) }.

####################################################################################


Generator
1 { game(Team1, Team2, Venue, Gameday) : team(Team1), team(Team2), venue(Venue), gameday(Gameday) }.


Hard Constraints

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- game(Team1, Team2, _, Gameday), game(Team1, Team2, _, Gameday2), Gameday != Gameday2.

Intended semantics:- No team plays each other more than once at different gamedays.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- game(Team1, Team2, _, Gameday), game(Team1, Team2, _, Gameday2), Gameday != Gameday2.

####################################################################################
- No team plays each other more than once at different gamedays.
:- game(Team1, Team2, _, Gameday), game(Team1, Team2, _, Gameday2), Gameday != Gameday2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }, N < 1.

Intended semantics:- Every team plays every other team at least once.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each pair of teams plays against each other at most once, not at least once."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }, N < 1.

Intended semantics:- Every team plays every other team at least once.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each pair of teams plays against each other at most once, not at least once."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }, N < 1.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }, N = 0.

Intended semantics:- Every team plays every other team at least once.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each pair of teams plays against each other exactly zero times, which means no games are played, not at least once."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }, N = 0.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }, N = 0.

Intended semantics:- Every team plays every other team at least once.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each pair of teams plays against each other exactly zero times, which means no games are played, not at least once."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 3/3:
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }, N = 0.

--------------------------------------------------------------------------------
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }, N < 1.

REPAIRED RESPONSE:
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }, N = 0.

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): False

####################################################################################
- Every team plays every other team at least once.
:- team(Team1), team(Team2), Team1 != Team2, N = #count { game(Team1, Team2, _, _), game(Team2, Team1, _, _) }, N = 0.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- game(Team, Team, _, _).

Intended semantics:- No team plays itself.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- game(Team, Team, _, _).

####################################################################################
- No team plays itself.
:- game(Team, Team, _, _).


Soft Constraints:

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

Intended semantics:- Teams should play at the same venue as little as possible.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for any game where two different teams play against each other at the same venue on the same day, which is not aligned with the intended semantics of minimizing venue changes for each team."
}

================================================================================
================================================================================
Semantics are incorrect. Starting semantic repair attempts (n=3 left)...
================================================================================
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

Intended semantics:- Teams should play at the same venue as little as possible.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for the same team playing at different venues on the same day, which is not aligned with the intended semantics of minimizing venue changes for each team."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 1/3:
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

Intended semantics:- Teams should play at the same venue as little as possible.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for the same team playing at different venues on the same day, which is not aligned with the intended semantics of minimizing venue changes for each team."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 2/3:
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

--------------------------------------------------------------------------------
================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

Intended semantics:- Teams should play at the same venue as little as possible.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for the same team playing at different venues on the same day, which is not aligned with the intended semantics of minimizing venue changes for each team."
}

================================================================================
--------------------------------------------------------------------------------
Semantic repair attempt 3/3:
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

--------------------------------------------------------------------------------
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

####################################################################################
- Teams should play at the same venue as little as possible.
penalty("VenueChange",game(Team1,Team2,Venue1,Gameday),1) :- game(Team1,Team2,Venue1,Gameday), game(Team1,Team2,Venue2,Gameday), Venue1 != Venue2.

Full program saved to Results/experiments_2/sports scheduling_ft_qwen_mlx_k=3_n=3_20260111_110659.lp
