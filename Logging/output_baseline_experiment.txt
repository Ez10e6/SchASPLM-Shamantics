The tokenizer you are loading from './local_models/Qwen/Qwen2.5-7B-Instruct' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
CUDA not detected. Using MPS/CPU configuration (Quantization disabled).
Using Huggingface Hub Token from .env
Using dtype: torch.bfloat16
loading local model and tokenizer...
Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]Loading weights:   0%|          | 1/339 [00:00<00:00, 6944.21it/s, Materializing param=lm_head.weight]Loading weights:   0%|          | 1/339 [00:00<00:00, 5433.04it/s, Materializing param=lm_head.weight]Loading weights:   1%|          | 2/339 [00:01<03:22,  1.66it/s, Materializing param=lm_head.weight]  Loading weights:   1%|          | 2/339 [00:01<03:22,  1.66it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 2/339 [00:01<03:22,  1.66it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 3/339 [00:01<03:21,  1.66it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 3/339 [00:01<03:21,  1.66it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 4/339 [00:01<03:21,  1.66it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  Loading weights:   1%|          | 4/339 [00:01<03:21,  1.66it/s, Materializing param=model.layers.0.mlp.down_proj.weight]Loading weights:   1%|▏         | 5/339 [00:01<03:20,  1.66it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   1%|▏         | 5/339 [00:01<03:20,  1.66it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|▏         | 6/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|▏         | 6/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  Loading weights:   2%|▏         | 6/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   2%|▏         | 7/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 7/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 8/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          Loading weights:   2%|▏         | 8/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]Loading weights:   3%|▎         | 9/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 9/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 10/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 10/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 11/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]  Loading weights:   3%|▎         | 11/339 [00:01<00:58,  5.65it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]Loading weights:   4%|▎         | 12/339 [00:01<00:57,  5.65it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▎         | 12/339 [00:01<00:57,  5.65it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▍         | 13/339 [00:01<00:57,  5.65it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  Loading weights:   4%|▍         | 13/339 [00:01<00:57,  5.65it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]Loading weights:   4%|▍         | 14/339 [00:01<00:57,  5.65it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 14/339 [00:01<00:57,  5.65it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 15/339 [00:01<00:57,  5.65it/s, Materializing param=model.layers.1.input_layernorm.weight] Loading weights:   4%|▍         | 15/339 [00:01<00:57,  5.65it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   5%|▍         | 16/339 [00:01<00:57,  5.65it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  Loading weights:   5%|▍         | 16/339 [00:01<00:57,  5.65it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   5%|▌         | 17/339 [00:01<00:17, 18.20it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   5%|▌         | 17/339 [00:01<00:17, 18.20it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 17/339 [00:01<00:17, 18.20it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 18/339 [00:01<00:17, 18.20it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  Loading weights:   5%|▌         | 18/339 [00:01<00:17, 18.20it/s, Materializing param=model.layers.1.mlp.up_proj.weight]Loading weights:   6%|▌         | 19/339 [00:01<00:17, 18.20it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 19/339 [00:01<00:17, 18.20it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 20/339 [00:01<00:17, 18.20it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          Loading weights:   6%|▌         | 20/339 [00:01<00:17, 18.20it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]Loading weights:   6%|▌         | 21/339 [00:01<00:17, 18.20it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▌         | 21/339 [00:01<00:17, 18.20it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▋         | 22/339 [00:01<00:14, 22.41it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▋         | 22/339 [00:01<00:14, 22.41it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   6%|▋         | 22/339 [00:01<00:14, 22.41it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   7%|▋         | 23/339 [00:01<00:14, 22.41it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  Loading weights:   7%|▋         | 23/339 [00:01<00:14, 22.41it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]Loading weights:   7%|▋         | 24/339 [00:01<00:14, 22.41it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 24/339 [00:01<00:14, 22.41it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 25/339 [00:01<00:14, 22.41it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  Loading weights:   7%|▋         | 25/339 [00:01<00:14, 22.41it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]Loading weights:   8%|▊         | 26/339 [00:01<00:13, 22.41it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 26/339 [00:01<00:13, 22.41it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 27/339 [00:01<00:13, 22.41it/s, Materializing param=model.layers.2.input_layernorm.weight] Loading weights:   8%|▊         | 27/339 [00:01<00:13, 22.41it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:   8%|▊         | 28/339 [00:01<00:13, 22.41it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  Loading weights:   8%|▊         | 28/339 [00:01<00:13, 22.41it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:   9%|▊         | 29/339 [00:01<00:13, 22.41it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▊         | 29/339 [00:01<00:13, 22.41it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▉         | 30/339 [00:01<00:10, 30.42it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▉         | 30/339 [00:01<00:10, 30.42it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  Loading weights:   9%|▉         | 30/339 [00:01<00:10, 30.42it/s, Materializing param=model.layers.2.mlp.up_proj.weight]Loading weights:   9%|▉         | 31/339 [00:01<00:10, 30.42it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 31/339 [00:01<00:10, 30.42it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 32/339 [00:01<00:10, 30.42it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          Loading weights:   9%|▉         | 32/339 [00:01<00:10, 30.42it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]Loading weights:  10%|▉         | 33/339 [00:01<00:10, 30.42it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|▉         | 33/339 [00:01<00:10, 30.42it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|█         | 34/339 [00:01<00:10, 30.42it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 34/339 [00:01<00:10, 30.42it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 35/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  Loading weights:  10%|█         | 35/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]Loading weights:  11%|█         | 36/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 36/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 37/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  Loading weights:  11%|█         | 37/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]Loading weights:  11%|█         | 38/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  11%|█         | 38/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  12%|█▏        | 39/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.3.input_layernorm.weight] Loading weights:  12%|█▏        | 39/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  12%|█▏        | 40/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  Loading weights:  12%|█▏        | 40/339 [00:01<00:09, 30.42it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:01<00:07, 42.20it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:01<00:07, 42.20it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:01<00:07, 42.20it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 42/339 [00:01<00:07, 42.20it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  Loading weights:  12%|█▏        | 42/339 [00:01<00:07, 42.20it/s, Materializing param=model.layers.3.mlp.up_proj.weight]Loading weights:  13%|█▎        | 43/339 [00:01<00:07, 42.20it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 43/339 [00:01<00:07, 42.20it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 44/339 [00:01<00:06, 42.20it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          Loading weights:  13%|█▎        | 44/339 [00:01<00:06, 42.20it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]Loading weights:  13%|█▎        | 45/339 [00:01<00:06, 42.20it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  13%|█▎        | 45/339 [00:01<00:06, 42.20it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:01<00:06, 42.20it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:01<00:06, 42.20it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▍        | 47/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▍        | 47/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  Loading weights:  14%|█▍        | 47/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]Loading weights:  14%|█▍        | 48/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 48/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 49/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  Loading weights:  14%|█▍        | 49/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]Loading weights:  15%|█▍        | 50/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▍        | 50/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▌        | 51/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.4.input_layernorm.weight] Loading weights:  15%|█▌        | 51/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  15%|█▌        | 52/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  Loading weights:  15%|█▌        | 52/339 [00:01<00:06, 43.52it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:02<00:06, 43.52it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:02<00:06, 43.52it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 54/339 [00:02<00:06, 46.72it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 54/339 [00:02<00:06, 46.72it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  Loading weights:  16%|█▌        | 54/339 [00:02<00:06, 46.72it/s, Materializing param=model.layers.4.mlp.up_proj.weight]Loading weights:  16%|█▌        | 55/339 [00:02<00:06, 46.72it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  16%|█▌        | 55/339 [00:02<00:06, 46.72it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  17%|█▋        | 56/339 [00:02<00:06, 46.72it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          Loading weights:  17%|█▋        | 56/339 [00:02<00:06, 46.72it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]Loading weights:  17%|█▋        | 57/339 [00:02<00:06, 46.72it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 57/339 [00:02<00:06, 46.72it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:02<00:06, 46.72it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:02<00:06, 46.72it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 59/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  Loading weights:  17%|█▋        | 59/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]Loading weights:  18%|█▊        | 60/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 60/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 61/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  Loading weights:  18%|█▊        | 61/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]Loading weights:  18%|█▊        | 62/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  18%|█▊        | 62/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  19%|█▊        | 63/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.5.input_layernorm.weight] Loading weights:  19%|█▊        | 63/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  19%|█▉        | 64/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  Loading weights:  19%|█▉        | 64/339 [00:02<00:05, 46.72it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 66/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  Loading weights:  19%|█▉        | 66/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.mlp.up_proj.weight]Loading weights:  20%|█▉        | 67/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|█▉        | 67/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|██        | 68/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          Loading weights:  20%|██        | 68/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]Loading weights:  20%|██        | 69/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  20%|██        | 69/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  21%|██        | 70/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 70/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 71/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  Loading weights:  21%|██        | 71/339 [00:02<00:04, 55.84it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]Loading weights:  21%|██        | 72/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]Loading weights:  21%|██        | 72/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  21%|██        | 72/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  22%|██▏       | 73/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  Loading weights:  22%|██▏       | 73/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]Loading weights:  22%|██▏       | 74/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 74/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 75/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.6.input_layernorm.weight] Loading weights:  22%|██▏       | 75/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  22%|██▏       | 76/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  Loading weights:  22%|██▏       | 76/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:02<00:04, 54.48it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 78/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 78/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  Loading weights:  23%|██▎       | 78/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.mlp.up_proj.weight]Loading weights:  23%|██▎       | 79/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  23%|██▎       | 79/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  24%|██▎       | 80/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          Loading weights:  24%|██▎       | 80/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]Loading weights:  24%|██▍       | 81/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 81/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 83/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  Loading weights:  24%|██▍       | 83/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]Loading weights:  25%|██▍       | 84/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▍       | 84/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▌       | 85/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  Loading weights:  25%|██▌       | 85/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]Loading weights:  25%|██▌       | 86/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  25%|██▌       | 86/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  26%|██▌       | 87/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.7.input_layernorm.weight] Loading weights:  26%|██▌       | 87/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  26%|██▌       | 88/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  Loading weights:  26%|██▌       | 88/339 [00:02<00:04, 53.88it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:02<00:04, 61.34it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:02<00:04, 61.34it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:02<00:04, 61.34it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  27%|██▋       | 90/339 [00:02<00:04, 61.34it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  Loading weights:  27%|██▋       | 90/339 [00:02<00:04, 61.34it/s, Materializing param=model.layers.7.mlp.up_proj.weight]Loading weights:  27%|██▋       | 91/339 [00:02<00:04, 61.34it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 91/339 [00:02<00:04, 61.34it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 92/339 [00:02<00:04, 61.34it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          Loading weights:  27%|██▋       | 92/339 [00:02<00:04, 61.34it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]Loading weights:  27%|██▋       | 93/339 [00:02<00:04, 61.34it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  27%|██▋       | 93/339 [00:02<00:04, 61.34it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 95/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  Loading weights:  28%|██▊       | 95/339 [00:02<00:03, 61.34it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]Loading weights:  28%|██▊       | 96/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]Loading weights:  28%|██▊       | 96/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  28%|██▊       | 96/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  29%|██▊       | 97/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  Loading weights:  29%|██▊       | 97/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]Loading weights:  29%|██▉       | 98/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 98/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 99/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.8.input_layernorm.weight] Loading weights:  29%|██▉       | 99/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  29%|██▉       | 100/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.8.mlp.down_proj.weight] Loading weights:  29%|██▉       | 100/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|███       | 102/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  Loading weights:  30%|███       | 102/339 [00:02<00:04, 58.71it/s, Materializing param=model.layers.8.mlp.up_proj.weight]Loading weights:  30%|███       | 103/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.mlp.up_proj.weight]Loading weights:  30%|███       | 103/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  30%|███       | 103/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  31%|███       | 104/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          Loading weights:  31%|███       | 104/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]Loading weights:  31%|███       | 105/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███       | 105/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  32%|███▏      | 107/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  Loading weights:  32%|███▏      | 107/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]Loading weights:  32%|███▏      | 108/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 108/339 [00:02<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 109/339 [00:03<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  Loading weights:  32%|███▏      | 109/339 [00:03<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]Loading weights:  32%|███▏      | 110/339 [00:03<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  32%|███▏      | 110/339 [00:03<00:04, 48.75it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  33%|███▎      | 111/339 [00:03<00:04, 48.75it/s, Materializing param=model.layers.9.input_layernorm.weight] Loading weights:  33%|███▎      | 111/339 [00:03<00:04, 48.75it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  33%|███▎      | 112/339 [00:03<00:04, 48.75it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  Loading weights:  33%|███▎      | 112/339 [00:03<00:04, 48.75it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:03<00:04, 48.75it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:03<00:04, 48.75it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  34%|███▎      | 114/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  34%|███▎      | 114/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  Loading weights:  34%|███▎      | 114/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.mlp.up_proj.weight]Loading weights:  34%|███▍      | 115/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 115/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 116/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          Loading weights:  34%|███▍      | 116/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]Loading weights:  35%|███▍      | 117/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 117/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▌      | 119/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  Loading weights:  35%|███▌      | 119/339 [00:03<00:04, 54.77it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]Loading weights:  35%|███▌      | 120/339 [00:03<00:03, 54.77it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  35%|███▌      | 120/339 [00:03<00:03, 54.77it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  36%|███▌      | 121/339 [00:03<00:03, 54.77it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  Loading weights:  36%|███▌      | 121/339 [00:03<00:03, 54.77it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]Loading weights:  36%|███▌      | 122/339 [00:03<00:03, 54.77it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▌      | 122/339 [00:03<00:03, 54.77it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▋      | 123/339 [00:03<00:03, 54.77it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  36%|███▋      | 123/339 [00:03<00:03, 54.77it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  37%|███▋      | 124/339 [00:03<00:03, 54.77it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  Loading weights:  37%|███▋      | 124/339 [00:03<00:03, 54.77it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 126/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  Loading weights:  37%|███▋      | 126/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.mlp.up_proj.weight]Loading weights:  37%|███▋      | 127/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  37%|███▋      | 127/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  38%|███▊      | 128/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          Loading weights:  38%|███▊      | 128/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]Loading weights:  38%|███▊      | 129/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 129/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  39%|███▊      | 131/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  Loading weights:  39%|███▊      | 131/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]Loading weights:  39%|███▉      | 132/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 132/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 133/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  Loading weights:  39%|███▉      | 133/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]Loading weights:  40%|███▉      | 134/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 134/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 135/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.11.input_layernorm.weight] Loading weights:  40%|███▉      | 135/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  40%|████      | 136/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  Loading weights:  40%|████      | 136/339 [00:03<00:03, 54.10it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  40%|████      | 137/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  40%|████      | 137/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  40%|████      | 137/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  41%|████      | 138/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  Loading weights:  41%|████      | 138/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.mlp.up_proj.weight]Loading weights:  41%|████      | 139/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████      | 139/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████▏     | 140/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          Loading weights:  41%|████▏     | 140/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]Loading weights:  42%|████▏     | 141/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 141/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 143/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  Loading weights:  42%|████▏     | 143/339 [00:03<00:03, 60.14it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]Loading weights:  42%|████▏     | 144/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]Loading weights:  42%|████▏     | 144/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  42%|████▏     | 144/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  43%|████▎     | 145/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  Loading weights:  43%|████▎     | 145/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]Loading weights:  43%|████▎     | 146/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 146/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 147/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.12.input_layernorm.weight] Loading weights:  43%|████▎     | 147/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  44%|████▎     | 148/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  Loading weights:  44%|████▎     | 148/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:03<00:03, 55.60it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 150/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 150/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  Loading weights:  44%|████▍     | 150/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.mlp.up_proj.weight]Loading weights:  45%|████▍     | 151/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 151/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 152/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          Loading weights:  45%|████▍     | 152/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]Loading weights:  45%|████▌     | 153/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 153/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  46%|████▌     | 155/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  Loading weights:  46%|████▌     | 155/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]Loading weights:  46%|████▌     | 156/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▌     | 156/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▋     | 157/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  Loading weights:  46%|████▋     | 157/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]Loading weights:  47%|████▋     | 158/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 158/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 159/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.13.input_layernorm.weight] Loading weights:  47%|████▋     | 159/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  47%|████▋     | 160/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  Loading weights:  47%|████▋     | 160/339 [00:03<00:03, 52.97it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:03<00:03, 58.21it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:03<00:03, 58.21it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:03<00:03, 58.21it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  48%|████▊     | 162/339 [00:04<00:03, 58.21it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  Loading weights:  48%|████▊     | 162/339 [00:04<00:03, 58.21it/s, Materializing param=model.layers.13.mlp.up_proj.weight]Loading weights:  48%|████▊     | 163/339 [00:04<00:03, 58.21it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 163/339 [00:04<00:03, 58.21it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 164/339 [00:04<00:03, 58.21it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          Loading weights:  48%|████▊     | 164/339 [00:04<00:03, 58.21it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]Loading weights:  49%|████▊     | 165/339 [00:04<00:02, 58.21it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▊     | 165/339 [00:04<00:02, 58.21it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:04<00:02, 58.21it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:04<00:02, 58.21it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 167/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 167/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  Loading weights:  49%|████▉     | 167/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]Loading weights:  50%|████▉     | 168/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 168/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 169/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  Loading weights:  50%|████▉     | 169/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]Loading weights:  50%|█████     | 170/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 170/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 171/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.14.input_layernorm.weight] Loading weights:  50%|█████     | 171/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  51%|█████     | 172/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  Loading weights:  51%|█████     | 172/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  51%|█████     | 173/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████     | 173/339 [00:04<00:03, 53.67it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████▏    | 174/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████▏    | 174/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  Loading weights:  51%|█████▏    | 174/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.mlp.up_proj.weight]Loading weights:  52%|█████▏    | 175/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 175/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 176/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          Loading weights:  52%|█████▏    | 176/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]Loading weights:  52%|█████▏    | 177/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  52%|█████▏    | 177/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 179/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  Loading weights:  53%|█████▎    | 179/339 [00:04<00:03, 53.25it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]Loading weights:  53%|█████▎    | 180/339 [00:04<00:02, 53.25it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 180/339 [00:04<00:02, 53.25it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 181/339 [00:04<00:02, 53.25it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  Loading weights:  53%|█████▎    | 181/339 [00:04<00:02, 53.25it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]Loading weights:  54%|█████▎    | 182/339 [00:04<00:02, 53.25it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▎    | 182/339 [00:04<00:02, 53.25it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▍    | 183/339 [00:04<00:02, 53.25it/s, Materializing param=model.layers.15.input_layernorm.weight] Loading weights:  54%|█████▍    | 183/339 [00:04<00:02, 53.25it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  54%|█████▍    | 184/339 [00:04<00:02, 53.25it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  Loading weights:  54%|█████▍    | 184/339 [00:04<00:02, 53.25it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 186/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  Loading weights:  55%|█████▍    | 186/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.mlp.up_proj.weight]Loading weights:  55%|█████▌    | 187/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 187/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 188/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          Loading weights:  55%|█████▌    | 188/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]Loading weights:  56%|█████▌    | 189/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 189/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:04<00:02, 58.85it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▋    | 191/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▋    | 191/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  Loading weights:  56%|█████▋    | 191/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]Loading weights:  57%|█████▋    | 192/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 192/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 193/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  Loading weights:  57%|█████▋    | 193/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]Loading weights:  57%|█████▋    | 194/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  57%|█████▋    | 194/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  58%|█████▊    | 195/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.16.input_layernorm.weight] Loading weights:  58%|█████▊    | 195/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  58%|█████▊    | 196/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  Loading weights:  58%|█████▊    | 196/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:04<00:02, 54.77it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 198/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 198/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  Loading weights:  58%|█████▊    | 198/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.mlp.up_proj.weight]Loading weights:  59%|█████▊    | 199/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▊    | 199/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▉    | 200/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          Loading weights:  59%|█████▉    | 200/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]Loading weights:  59%|█████▉    | 201/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  59%|█████▉    | 201/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 203/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  Loading weights:  60%|█████▉    | 203/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]Loading weights:  60%|██████    | 204/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 204/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 205/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  Loading weights:  60%|██████    | 205/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]Loading weights:  61%|██████    | 206/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 206/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 207/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.17.input_layernorm.weight] Loading weights:  61%|██████    | 207/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  61%|██████▏   | 208/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  Loading weights:  61%|██████▏   | 208/339 [00:04<00:02, 53.50it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 210/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  Loading weights:  62%|██████▏   | 210/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.mlp.up_proj.weight]Loading weights:  62%|██████▏   | 211/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  62%|██████▏   | 211/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  63%|██████▎   | 212/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          Loading weights:  63%|██████▎   | 212/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]Loading weights:  63%|██████▎   | 213/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 213/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:04<00:02, 59.33it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 215/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 215/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  Loading weights:  63%|██████▎   | 215/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]Loading weights:  64%|██████▎   | 216/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▎   | 216/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▍   | 217/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  Loading weights:  64%|██████▍   | 217/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]Loading weights:  64%|██████▍   | 218/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  64%|██████▍   | 218/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  65%|██████▍   | 219/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.18.input_layernorm.weight] Loading weights:  65%|██████▍   | 219/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.18.input_layernorm.weight]Loading weights:  65%|██████▍   | 220/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  Loading weights:  65%|██████▍   | 220/339 [00:04<00:02, 54.70it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:05<00:02, 54.70it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:05<00:02, 54.70it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 222/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 222/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  Loading weights:  65%|██████▌   | 222/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.mlp.up_proj.weight]Loading weights:  66%|██████▌   | 223/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 223/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 224/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          Loading weights:  66%|██████▌   | 224/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]Loading weights:  66%|██████▋   | 225/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  66%|██████▋   | 225/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 227/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  Loading weights:  67%|██████▋   | 227/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]Loading weights:  67%|██████▋   | 228/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  67%|██████▋   | 228/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  68%|██████▊   | 229/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  Loading weights:  68%|██████▊   | 229/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]Loading weights:  68%|██████▊   | 230/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 230/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 231/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.19.input_layernorm.weight] Loading weights:  68%|██████▊   | 231/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  68%|██████▊   | 232/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  Loading weights:  68%|██████▊   | 232/339 [00:05<00:02, 53.45it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▉   | 234/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  Loading weights:  69%|██████▉   | 234/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.mlp.up_proj.weight]Loading weights:  69%|██████▉   | 235/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  69%|██████▉   | 235/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  70%|██████▉   | 236/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          Loading weights:  70%|██████▉   | 236/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]Loading weights:  70%|██████▉   | 237/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|██████▉   | 237/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|███████   | 238/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  70%|███████   | 238/339 [00:05<00:01, 59.33it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  71%|███████   | 239/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  71%|███████   | 239/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  Loading weights:  71%|███████   | 239/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]Loading weights:  71%|███████   | 240/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 240/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 241/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  Loading weights:  71%|███████   | 241/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]Loading weights:  71%|███████▏  | 242/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  71%|███████▏  | 242/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  72%|███████▏  | 243/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.20.input_layernorm.weight] Loading weights:  72%|███████▏  | 243/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  72%|███████▏  | 244/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  Loading weights:  72%|███████▏  | 244/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:05<00:01, 54.74it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  73%|███████▎  | 246/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  73%|███████▎  | 246/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  Loading weights:  73%|███████▎  | 246/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.mlp.up_proj.weight]Loading weights:  73%|███████▎  | 247/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 247/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 248/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          Loading weights:  73%|███████▎  | 248/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]Loading weights:  73%|███████▎  | 249/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  73%|███████▎  | 249/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▍  | 251/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  Loading weights:  74%|███████▍  | 251/339 [00:05<00:01, 53.63it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]Loading weights:  74%|███████▍  | 252/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]Loading weights:  74%|███████▍  | 252/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  74%|███████▍  | 252/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  75%|███████▍  | 253/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  Loading weights:  75%|███████▍  | 253/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]Loading weights:  75%|███████▍  | 254/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▍  | 254/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▌  | 255/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.21.input_layernorm.weight] Loading weights:  75%|███████▌  | 255/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  76%|███████▌  | 256/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  Loading weights:  76%|███████▌  | 256/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:05<00:01, 52.07it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 258/339 [00:05<00:02, 35.89it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 258/339 [00:05<00:02, 35.89it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  Loading weights:  76%|███████▌  | 258/339 [00:05<00:02, 35.89it/s, Materializing param=model.layers.21.mlp.up_proj.weight]Loading weights:  76%|███████▋  | 259/339 [00:06<00:02, 35.89it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  76%|███████▋  | 259/339 [00:06<00:02, 35.89it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  77%|███████▋  | 260/339 [00:06<00:02, 35.89it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          Loading weights:  77%|███████▋  | 260/339 [00:06<00:02, 35.89it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]Loading weights:  77%|███████▋  | 261/339 [00:06<00:02, 35.89it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 261/339 [00:06<00:02, 35.89it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:06<00:02, 35.89it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:06<00:02, 35.89it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  78%|███████▊  | 263/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  78%|███████▊  | 263/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  Loading weights:  78%|███████▊  | 263/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]Loading weights:  78%|███████▊  | 264/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 264/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 265/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  Loading weights:  78%|███████▊  | 265/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]Loading weights:  78%|███████▊  | 266/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  78%|███████▊  | 266/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  79%|███████▉  | 267/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.22.input_layernorm.weight] Loading weights:  79%|███████▉  | 267/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.22.input_layernorm.weight]Loading weights:  79%|███████▉  | 268/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  Loading weights:  79%|███████▉  | 268/339 [00:06<00:02, 34.97it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:06<00:02, 32.86it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:06<00:02, 32.86it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:06<00:02, 32.86it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  80%|███████▉  | 270/339 [00:06<00:02, 32.86it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  Loading weights:  80%|███████▉  | 270/339 [00:06<00:02, 32.86it/s, Materializing param=model.layers.22.mlp.up_proj.weight]Loading weights:  80%|███████▉  | 271/339 [00:06<00:02, 32.86it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|███████▉  | 271/339 [00:06<00:02, 32.86it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|████████  | 272/339 [00:06<00:02, 32.86it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          Loading weights:  80%|████████  | 272/339 [00:06<00:02, 32.86it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]Loading weights:  81%|████████  | 273/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]Loading weights:  81%|████████  | 273/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 273/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 274/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 274/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 275/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  Loading weights:  81%|████████  | 275/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]Loading weights:  81%|████████▏ | 276/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  81%|████████▏ | 276/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  82%|████████▏ | 277/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  Loading weights:  82%|████████▏ | 277/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]Loading weights:  82%|████████▏ | 278/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 278/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 279/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.23.input_layernorm.weight] Loading weights:  82%|████████▏ | 279/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.23.input_layernorm.weight]Loading weights:  83%|████████▎ | 280/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  Loading weights:  83%|████████▎ | 280/339 [00:06<00:02, 26.33it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:06<00:01, 30.15it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:06<00:01, 30.15it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:06<00:01, 30.15it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 282/339 [00:06<00:01, 30.15it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  Loading weights:  83%|████████▎ | 282/339 [00:06<00:01, 30.15it/s, Materializing param=model.layers.23.mlp.up_proj.weight]Loading weights:  83%|████████▎ | 283/339 [00:07<00:01, 30.15it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  83%|████████▎ | 283/339 [00:07<00:01, 30.15it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  84%|████████▍ | 284/339 [00:07<00:01, 30.15it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          Loading weights:  84%|████████▍ | 284/339 [00:07<00:01, 30.15it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]Loading weights:  84%|████████▍ | 285/339 [00:07<00:02, 24.85it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]Loading weights:  84%|████████▍ | 285/339 [00:07<00:02, 24.85it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 285/339 [00:07<00:02, 24.85it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:07<00:02, 24.85it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:07<00:02, 24.85it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  85%|████████▍ | 287/339 [00:07<00:02, 24.85it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  Loading weights:  85%|████████▍ | 287/339 [00:07<00:02, 24.85it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]Loading weights:  85%|████████▍ | 288/339 [00:07<00:02, 24.85it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▍ | 288/339 [00:07<00:02, 24.85it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▌ | 289/339 [00:07<00:02, 24.85it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  Loading weights:  85%|████████▌ | 289/339 [00:07<00:02, 24.85it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]Loading weights:  86%|████████▌ | 290/339 [00:07<00:01, 24.85it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 290/339 [00:07<00:01, 24.85it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 291/339 [00:07<00:01, 24.85it/s, Materializing param=model.layers.24.input_layernorm.weight] Loading weights:  86%|████████▌ | 291/339 [00:07<00:01, 24.85it/s, Materializing param=model.layers.24.input_layernorm.weight]Loading weights:  86%|████████▌ | 292/339 [00:07<00:01, 24.85it/s, Materializing param=model.layers.24.mlp.down_proj.weight]  Loading weights:  86%|████████▌ | 292/339 [00:07<00:01, 24.85it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:07<00:01, 28.84it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:07<00:01, 28.84it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:07<00:01, 28.84it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  87%|████████▋ | 294/339 [00:07<00:01, 28.84it/s, Materializing param=model.layers.24.mlp.up_proj.weight]  Loading weights:  87%|████████▋ | 294/339 [00:07<00:01, 28.84it/s, Materializing param=model.layers.24.mlp.up_proj.weight]Loading weights:  87%|████████▋ | 295/339 [00:07<00:01, 28.84it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 295/339 [00:07<00:01, 28.84it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 296/339 [00:07<00:01, 28.84it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]          Loading weights:  87%|████████▋ | 296/339 [00:07<00:01, 28.84it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]Loading weights:  88%|████████▊ | 297/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]Loading weights:  88%|████████▊ | 297/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 297/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 299/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]  Loading weights:  88%|████████▊ | 299/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]Loading weights:  88%|████████▊ | 300/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  88%|████████▊ | 300/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  89%|████████▉ | 301/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]  Loading weights:  89%|████████▉ | 301/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]Loading weights:  89%|████████▉ | 302/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 302/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 303/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.25.input_layernorm.weight] Loading weights:  89%|████████▉ | 303/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.25.input_layernorm.weight]Loading weights:  90%|████████▉ | 304/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.25.mlp.down_proj.weight]  Loading weights:  90%|████████▉ | 304/339 [00:07<00:01, 23.43it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:07<00:01, 28.13it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:07<00:01, 28.13it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:07<00:01, 28.13it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|█████████ | 306/339 [00:07<00:01, 28.13it/s, Materializing param=model.layers.25.mlp.up_proj.weight]  Loading weights:  90%|█████████ | 306/339 [00:07<00:01, 28.13it/s, Materializing param=model.layers.25.mlp.up_proj.weight]Loading weights:  91%|█████████ | 307/339 [00:08<00:01, 28.13it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 307/339 [00:08<00:01, 28.13it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 308/339 [00:08<00:01, 28.13it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]          Loading weights:  91%|█████████ | 308/339 [00:08<00:01, 28.13it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]Loading weights:  91%|█████████ | 309/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]Loading weights:  91%|█████████ | 309/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████ | 309/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  92%|█████████▏| 311/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]  Loading weights:  92%|█████████▏| 311/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]Loading weights:  92%|█████████▏| 312/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 312/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 313/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]  Loading weights:  92%|█████████▏| 313/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]Loading weights:  93%|█████████▎| 314/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 314/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 315/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.26.input_layernorm.weight] Loading weights:  93%|█████████▎| 315/339 [00:08<00:01, 23.41it/s, Materializing param=model.layers.26.input_layernorm.weight]Loading weights:  93%|█████████▎| 316/339 [00:08<00:00, 23.41it/s, Materializing param=model.layers.26.mlp.down_proj.weight]  Loading weights:  93%|█████████▎| 316/339 [00:08<00:00, 23.41it/s, Materializing param=model.layers.26.mlp.down_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:08<00:00, 23.62it/s, Materializing param=model.layers.26.mlp.down_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:08<00:00, 23.62it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:08<00:00, 23.62it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▍| 318/339 [00:08<00:00, 23.62it/s, Materializing param=model.layers.26.mlp.up_proj.weight]  Loading weights:  94%|█████████▍| 318/339 [00:08<00:00, 23.62it/s, Materializing param=model.layers.26.mlp.up_proj.weight]Loading weights:  94%|█████████▍| 319/339 [00:08<00:00, 23.62it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 319/339 [00:08<00:00, 23.62it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 320/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 320/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]          Loading weights:  94%|█████████▍| 320/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]Loading weights:  95%|█████████▍| 321/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 321/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▌| 323/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]  Loading weights:  95%|█████████▌| 323/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]Loading weights:  96%|█████████▌| 324/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 324/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 325/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]  Loading weights:  96%|█████████▌| 325/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]Loading weights:  96%|█████████▌| 326/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▌| 326/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▋| 327/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.27.input_layernorm.weight] Loading weights:  96%|█████████▋| 327/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.27.input_layernorm.weight]Loading weights:  97%|█████████▋| 328/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.27.mlp.down_proj.weight]  Loading weights:  97%|█████████▋| 328/339 [00:08<00:00, 23.25it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:08<00:00, 28.74it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:08<00:00, 28.74it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:08<00:00, 28.74it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 330/339 [00:08<00:00, 28.74it/s, Materializing param=model.layers.27.mlp.up_proj.weight]  Loading weights:  97%|█████████▋| 330/339 [00:08<00:00, 28.74it/s, Materializing param=model.layers.27.mlp.up_proj.weight]Loading weights:  98%|█████████▊| 331/339 [00:08<00:00, 28.74it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 331/339 [00:08<00:00, 28.74it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 332/339 [00:08<00:00, 22.71it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 332/339 [00:08<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]          Loading weights:  98%|█████████▊| 332/339 [00:08<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]Loading weights:  98%|█████████▊| 333/339 [00:08<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  98%|█████████▊| 333/339 [00:08<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:08<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:08<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▉| 335/339 [00:09<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]  Loading weights:  99%|█████████▉| 335/339 [00:09<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]Loading weights:  99%|█████████▉| 336/339 [00:09<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 336/339 [00:09<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 337/339 [00:09<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]  Loading weights:  99%|█████████▉| 337/339 [00:09<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]Loading weights: 100%|█████████▉| 338/339 [00:09<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|█████████▉| 338/339 [00:09<00:00, 22.71it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|██████████| 339/339 [00:09<00:00, 22.71it/s, Materializing param=model.norm.weight]                      Loading weights: 100%|██████████| 339/339 [00:09<00:00, 22.71it/s, Materializing param=model.norm.weight]Loading weights: 100%|██████████| 339/339 [00:09<00:00, 37.64it/s, Materializing param=model.norm.weight]
The tokenizer you are loading from './local_models/Qwen/Qwen2.5-7B-Instruct' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
Using Huggingface Hub Token from .env
Using dtype: torch.bfloat16
loading local model and tokenizer...
Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]Loading weights:   0%|          | 1/339 [00:00<00:00, 107546.26it/s, Materializing param=lm_head.weight]Loading weights:   0%|          | 1/339 [00:00<00:00, 29330.80it/s, Materializing param=lm_head.weight] Loading weights:   1%|          | 2/339 [00:02<07:29,  1.33s/it, Materializing param=lm_head.weight]   Loading weights:   1%|          | 2/339 [00:02<07:29,  1.33s/it, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 2/339 [00:02<07:29,  1.33s/it, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 3/339 [00:02<07:27,  1.33s/it, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 3/339 [00:02<07:27,  1.33s/it, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|          | 4/339 [00:02<07:26,  1.33s/it, Materializing param=model.layers.0.mlp.down_proj.weight]  Loading weights:   1%|          | 4/339 [00:02<07:26,  1.33s/it, Materializing param=model.layers.0.mlp.down_proj.weight]Loading weights:   1%|▏         | 5/339 [00:02<07:25,  1.33s/it, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   1%|▏         | 5/339 [00:02<07:25,  1.33s/it, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|▏         | 6/339 [00:02<02:06,  2.63it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|▏         | 6/339 [00:02<02:06,  2.63it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  Loading weights:   2%|▏         | 6/339 [00:02<02:06,  2.63it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   2%|▏         | 7/339 [00:02<02:06,  2.63it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 7/339 [00:02<02:06,  2.63it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   2%|▏         | 8/339 [00:02<02:05,  2.63it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          Loading weights:   2%|▏         | 8/339 [00:02<02:05,  2.63it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]Loading weights:   3%|▎         | 9/339 [00:02<02:05,  2.63it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 9/339 [00:02<02:05,  2.63it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   3%|▎         | 10/339 [00:02<02:05,  2.63it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 10/339 [00:02<02:05,  2.63it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 11/339 [00:02<00:57,  5.69it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   3%|▎         | 11/339 [00:02<00:57,  5.69it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]  Loading weights:   3%|▎         | 11/339 [00:02<00:57,  5.69it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]Loading weights:   4%|▎         | 12/339 [00:02<00:57,  5.69it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▎         | 12/339 [00:02<00:57,  5.69it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   4%|▍         | 13/339 [00:02<00:57,  5.69it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  Loading weights:   4%|▍         | 13/339 [00:02<00:57,  5.69it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]Loading weights:   4%|▍         | 14/339 [00:02<00:57,  5.69it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 14/339 [00:02<00:57,  5.69it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   4%|▍         | 15/339 [00:02<00:56,  5.69it/s, Materializing param=model.layers.1.input_layernorm.weight] Loading weights:   4%|▍         | 15/339 [00:02<00:56,  5.69it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   5%|▍         | 16/339 [00:03<00:34,  9.39it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   5%|▍         | 16/339 [00:03<00:34,  9.39it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  Loading weights:   5%|▍         | 16/339 [00:03<00:34,  9.39it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   5%|▌         | 17/339 [00:03<00:34,  9.39it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 17/339 [00:03<00:34,  9.39it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   5%|▌         | 18/339 [00:03<00:34,  9.39it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  Loading weights:   5%|▌         | 18/339 [00:03<00:34,  9.39it/s, Materializing param=model.layers.1.mlp.up_proj.weight]Loading weights:   6%|▌         | 19/339 [00:03<00:34,  9.39it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 19/339 [00:03<00:34,  9.39it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 20/339 [00:03<00:27, 11.66it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   6%|▌         | 20/339 [00:03<00:27, 11.66it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          Loading weights:   6%|▌         | 20/339 [00:03<00:27, 11.66it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]Loading weights:   6%|▌         | 21/339 [00:03<00:27, 11.66it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▌         | 21/339 [00:03<00:27, 11.66it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   6%|▋         | 22/339 [00:03<00:27, 11.66it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   6%|▋         | 22/339 [00:03<00:27, 11.66it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   7%|▋         | 23/339 [00:03<00:27, 11.66it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  Loading weights:   7%|▋         | 23/339 [00:03<00:27, 11.66it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]Loading weights:   7%|▋         | 24/339 [00:03<00:27, 11.66it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 24/339 [00:03<00:27, 11.66it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   7%|▋         | 25/339 [00:03<00:26, 11.66it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  Loading weights:   7%|▋         | 25/339 [00:03<00:26, 11.66it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]Loading weights:   8%|▊         | 26/339 [00:03<00:26, 11.66it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 26/339 [00:03<00:26, 11.66it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:   8%|▊         | 27/339 [00:03<00:26, 11.66it/s, Materializing param=model.layers.2.input_layernorm.weight] Loading weights:   8%|▊         | 27/339 [00:03<00:26, 11.66it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:   8%|▊         | 28/339 [00:03<00:26, 11.66it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  Loading weights:   8%|▊         | 28/339 [00:03<00:26, 11.66it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:   9%|▊         | 29/339 [00:03<00:15, 20.62it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:   9%|▊         | 29/339 [00:03<00:15, 20.62it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▊         | 29/339 [00:03<00:15, 20.62it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:   9%|▉         | 30/339 [00:03<00:14, 20.62it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  Loading weights:   9%|▉         | 30/339 [00:03<00:14, 20.62it/s, Materializing param=model.layers.2.mlp.up_proj.weight]Loading weights:   9%|▉         | 31/339 [00:03<00:14, 20.62it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 31/339 [00:03<00:14, 20.62it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:   9%|▉         | 32/339 [00:03<00:14, 20.62it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          Loading weights:   9%|▉         | 32/339 [00:03<00:14, 20.62it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]Loading weights:  10%|▉         | 33/339 [00:03<00:14, 20.62it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|▉         | 33/339 [00:03<00:14, 20.62it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|█         | 34/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  10%|█         | 34/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 34/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  10%|█         | 35/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  Loading weights:  10%|█         | 35/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]Loading weights:  11%|█         | 36/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 36/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  11%|█         | 37/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  Loading weights:  11%|█         | 37/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]Loading weights:  11%|█         | 38/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  11%|█         | 38/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  12%|█▏        | 39/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.3.input_layernorm.weight] Loading weights:  12%|█▏        | 39/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  12%|█▏        | 40/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  Loading weights:  12%|█▏        | 40/339 [00:03<00:13, 22.21it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:03<00:10, 28.66it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:03<00:10, 28.66it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 41/339 [00:03<00:10, 28.66it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  12%|█▏        | 42/339 [00:03<00:10, 28.66it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  Loading weights:  12%|█▏        | 42/339 [00:03<00:10, 28.66it/s, Materializing param=model.layers.3.mlp.up_proj.weight]Loading weights:  13%|█▎        | 43/339 [00:03<00:10, 28.66it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 43/339 [00:03<00:10, 28.66it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  13%|█▎        | 44/339 [00:03<00:10, 28.66it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          Loading weights:  13%|█▎        | 44/339 [00:03<00:10, 28.66it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]Loading weights:  13%|█▎        | 45/339 [00:03<00:10, 28.66it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  13%|█▎        | 45/339 [00:03<00:10, 28.66it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▎        | 46/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  14%|█▍        | 47/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  Loading weights:  14%|█▍        | 47/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]Loading weights:  14%|█▍        | 48/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 48/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  14%|█▍        | 49/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  Loading weights:  14%|█▍        | 49/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]Loading weights:  15%|█▍        | 50/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▍        | 50/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  15%|█▌        | 51/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.4.input_layernorm.weight] Loading weights:  15%|█▌        | 51/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  15%|█▌        | 52/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  Loading weights:  15%|█▌        | 52/339 [00:03<00:10, 28.42it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:03<00:08, 34.42it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:03<00:08, 34.42it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 53/339 [00:03<00:08, 34.42it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  16%|█▌        | 54/339 [00:04<00:08, 34.42it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  Loading weights:  16%|█▌        | 54/339 [00:04<00:08, 34.42it/s, Materializing param=model.layers.4.mlp.up_proj.weight]Loading weights:  16%|█▌        | 55/339 [00:04<00:08, 34.42it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  16%|█▌        | 55/339 [00:04<00:08, 34.42it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  17%|█▋        | 56/339 [00:04<00:08, 34.42it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          Loading weights:  17%|█▋        | 56/339 [00:04<00:08, 34.42it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]Loading weights:  17%|█▋        | 57/339 [00:04<00:08, 34.42it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 57/339 [00:04<00:08, 34.42it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 58/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  17%|█▋        | 59/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  Loading weights:  17%|█▋        | 59/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]Loading weights:  18%|█▊        | 60/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 60/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  18%|█▊        | 61/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  Loading weights:  18%|█▊        | 61/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]Loading weights:  18%|█▊        | 62/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  18%|█▊        | 62/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  19%|█▊        | 63/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.5.input_layernorm.weight] Loading weights:  19%|█▊        | 63/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  19%|█▉        | 64/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  Loading weights:  19%|█▉        | 64/339 [00:04<00:08, 32.50it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:04<00:07, 37.71it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:04<00:07, 37.71it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 65/339 [00:04<00:07, 37.71it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  19%|█▉        | 66/339 [00:04<00:07, 37.71it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  Loading weights:  19%|█▉        | 66/339 [00:04<00:07, 37.71it/s, Materializing param=model.layers.5.mlp.up_proj.weight]Loading weights:  20%|█▉        | 67/339 [00:04<00:07, 37.71it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|█▉        | 67/339 [00:04<00:07, 37.71it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  20%|██        | 68/339 [00:04<00:07, 37.71it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          Loading weights:  20%|██        | 68/339 [00:04<00:07, 37.71it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]Loading weights:  20%|██        | 69/339 [00:04<00:07, 37.71it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  20%|██        | 69/339 [00:04<00:07, 37.71it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  21%|██        | 70/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  21%|██        | 70/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 70/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  21%|██        | 71/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  Loading weights:  21%|██        | 71/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]Loading weights:  21%|██        | 72/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  21%|██        | 72/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  22%|██▏       | 73/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  Loading weights:  22%|██▏       | 73/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]Loading weights:  22%|██▏       | 74/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 74/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  22%|██▏       | 75/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.6.input_layernorm.weight] Loading weights:  22%|██▏       | 75/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  22%|██▏       | 76/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  Loading weights:  22%|██▏       | 76/339 [00:04<00:07, 34.59it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:04<00:06, 39.92it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:04<00:06, 39.92it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 77/339 [00:04<00:06, 39.92it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  23%|██▎       | 78/339 [00:04<00:06, 39.92it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  Loading weights:  23%|██▎       | 78/339 [00:04<00:06, 39.92it/s, Materializing param=model.layers.6.mlp.up_proj.weight]Loading weights:  23%|██▎       | 79/339 [00:04<00:06, 39.92it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  23%|██▎       | 79/339 [00:04<00:06, 39.92it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  24%|██▎       | 80/339 [00:04<00:06, 39.92it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          Loading weights:  24%|██▎       | 80/339 [00:04<00:06, 39.92it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]Loading weights:  24%|██▍       | 81/339 [00:04<00:06, 39.92it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 81/339 [00:04<00:06, 39.92it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 82/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  24%|██▍       | 83/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  Loading weights:  24%|██▍       | 83/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]Loading weights:  25%|██▍       | 84/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▍       | 84/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  25%|██▌       | 85/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  Loading weights:  25%|██▌       | 85/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]Loading weights:  25%|██▌       | 86/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  25%|██▌       | 86/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  26%|██▌       | 87/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.7.input_layernorm.weight] Loading weights:  26%|██▌       | 87/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  26%|██▌       | 88/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  Loading weights:  26%|██▌       | 88/339 [00:04<00:07, 35.72it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:04<00:06, 40.89it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:04<00:06, 40.89it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  26%|██▋       | 89/339 [00:04<00:06, 40.89it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  27%|██▋       | 90/339 [00:04<00:06, 40.89it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  Loading weights:  27%|██▋       | 90/339 [00:04<00:06, 40.89it/s, Materializing param=model.layers.7.mlp.up_proj.weight]Loading weights:  27%|██▋       | 91/339 [00:05<00:06, 40.89it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 91/339 [00:05<00:06, 40.89it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  27%|██▋       | 92/339 [00:05<00:06, 40.89it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          Loading weights:  27%|██▋       | 92/339 [00:05<00:06, 40.89it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]Loading weights:  27%|██▋       | 93/339 [00:05<00:06, 40.89it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  27%|██▋       | 93/339 [00:05<00:06, 40.89it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 94/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  28%|██▊       | 95/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  Loading weights:  28%|██▊       | 95/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]Loading weights:  28%|██▊       | 96/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  28%|██▊       | 96/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  29%|██▊       | 97/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  Loading weights:  29%|██▊       | 97/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]Loading weights:  29%|██▉       | 98/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 98/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  29%|██▉       | 99/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.8.input_layernorm.weight] Loading weights:  29%|██▉       | 99/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  29%|██▉       | 100/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.8.mlp.down_proj.weight] Loading weights:  29%|██▉       | 100/339 [00:05<00:06, 35.97it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:05<00:05, 41.27it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:05<00:05, 41.27it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|██▉       | 101/339 [00:05<00:05, 41.27it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  30%|███       | 102/339 [00:05<00:05, 41.27it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  Loading weights:  30%|███       | 102/339 [00:05<00:05, 41.27it/s, Materializing param=model.layers.8.mlp.up_proj.weight]Loading weights:  30%|███       | 103/339 [00:05<00:05, 41.27it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  30%|███       | 103/339 [00:05<00:05, 41.27it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  31%|███       | 104/339 [00:05<00:05, 41.27it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          Loading weights:  31%|███       | 104/339 [00:05<00:05, 41.27it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]Loading weights:  31%|███       | 105/339 [00:05<00:05, 41.27it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███       | 105/339 [00:05<00:05, 41.27it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  31%|███▏      | 106/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  32%|███▏      | 107/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  Loading weights:  32%|███▏      | 107/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]Loading weights:  32%|███▏      | 108/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 108/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  32%|███▏      | 109/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  Loading weights:  32%|███▏      | 109/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]Loading weights:  32%|███▏      | 110/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  32%|███▏      | 110/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  33%|███▎      | 111/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.9.input_layernorm.weight] Loading weights:  33%|███▎      | 111/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  33%|███▎      | 112/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  Loading weights:  33%|███▎      | 112/339 [00:05<00:06, 36.80it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:05<00:05, 41.90it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:05<00:05, 41.90it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  33%|███▎      | 113/339 [00:05<00:05, 41.90it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  34%|███▎      | 114/339 [00:05<00:05, 41.90it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  Loading weights:  34%|███▎      | 114/339 [00:05<00:05, 41.90it/s, Materializing param=model.layers.9.mlp.up_proj.weight]Loading weights:  34%|███▍      | 115/339 [00:05<00:05, 41.90it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 115/339 [00:05<00:05, 41.90it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  34%|███▍      | 116/339 [00:05<00:05, 41.90it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          Loading weights:  34%|███▍      | 116/339 [00:05<00:05, 41.90it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]Loading weights:  35%|███▍      | 117/339 [00:05<00:05, 41.90it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 117/339 [00:05<00:05, 41.90it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▍      | 118/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  35%|███▌      | 119/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  Loading weights:  35%|███▌      | 119/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]Loading weights:  35%|███▌      | 120/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  35%|███▌      | 120/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  36%|███▌      | 121/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  Loading weights:  36%|███▌      | 121/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]Loading weights:  36%|███▌      | 122/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▌      | 122/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  36%|███▋      | 123/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  36%|███▋      | 123/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  37%|███▋      | 124/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  Loading weights:  37%|███▋      | 124/339 [00:05<00:05, 37.38it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:05<00:05, 42.15it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:05<00:05, 42.15it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 125/339 [00:05<00:05, 42.15it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  37%|███▋      | 126/339 [00:05<00:05, 42.15it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  Loading weights:  37%|███▋      | 126/339 [00:05<00:05, 42.15it/s, Materializing param=model.layers.10.mlp.up_proj.weight]Loading weights:  37%|███▋      | 127/339 [00:05<00:05, 42.15it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  37%|███▋      | 127/339 [00:05<00:05, 42.15it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  38%|███▊      | 128/339 [00:05<00:05, 42.15it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          Loading weights:  38%|███▊      | 128/339 [00:05<00:05, 42.15it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]Loading weights:  38%|███▊      | 129/339 [00:05<00:04, 42.15it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 129/339 [00:05<00:04, 42.15it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:05<00:05, 36.81it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:05<00:05, 36.81it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  38%|███▊      | 130/339 [00:05<00:05, 36.81it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  39%|███▊      | 131/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  Loading weights:  39%|███▊      | 131/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]Loading weights:  39%|███▉      | 132/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 132/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  39%|███▉      | 133/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  Loading weights:  39%|███▉      | 133/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]Loading weights:  40%|███▉      | 134/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 134/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  40%|███▉      | 135/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.11.input_layernorm.weight] Loading weights:  40%|███▉      | 135/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  40%|████      | 136/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  Loading weights:  40%|████      | 136/339 [00:06<00:05, 36.81it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  40%|████      | 137/339 [00:06<00:04, 41.61it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  40%|████      | 137/339 [00:06<00:04, 41.61it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  40%|████      | 137/339 [00:06<00:04, 41.61it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  41%|████      | 138/339 [00:06<00:04, 41.61it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  Loading weights:  41%|████      | 138/339 [00:06<00:04, 41.61it/s, Materializing param=model.layers.11.mlp.up_proj.weight]Loading weights:  41%|████      | 139/339 [00:06<00:04, 41.61it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████      | 139/339 [00:06<00:04, 41.61it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  41%|████▏     | 140/339 [00:06<00:04, 41.61it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          Loading weights:  41%|████▏     | 140/339 [00:06<00:04, 41.61it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]Loading weights:  42%|████▏     | 141/339 [00:06<00:04, 41.61it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 141/339 [00:06<00:04, 41.61it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 142/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  42%|████▏     | 143/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  Loading weights:  42%|████▏     | 143/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]Loading weights:  42%|████▏     | 144/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  42%|████▏     | 144/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  43%|████▎     | 145/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  Loading weights:  43%|████▎     | 145/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]Loading weights:  43%|████▎     | 146/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 146/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  43%|████▎     | 147/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.12.input_layernorm.weight] Loading weights:  43%|████▎     | 147/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  44%|████▎     | 148/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  Loading weights:  44%|████▎     | 148/339 [00:06<00:05, 36.72it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:06<00:04, 41.11it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:06<00:04, 41.11it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 149/339 [00:06<00:04, 41.11it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  44%|████▍     | 150/339 [00:06<00:04, 41.11it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  Loading weights:  44%|████▍     | 150/339 [00:06<00:04, 41.11it/s, Materializing param=model.layers.12.mlp.up_proj.weight]Loading weights:  45%|████▍     | 151/339 [00:06<00:04, 41.11it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 151/339 [00:06<00:04, 41.11it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  45%|████▍     | 152/339 [00:06<00:04, 41.11it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          Loading weights:  45%|████▍     | 152/339 [00:06<00:04, 41.11it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]Loading weights:  45%|████▌     | 153/339 [00:06<00:04, 41.11it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 153/339 [00:06<00:04, 41.11it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:06<00:05, 36.27it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:06<00:05, 36.27it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  45%|████▌     | 154/339 [00:06<00:05, 36.27it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  46%|████▌     | 155/339 [00:06<00:05, 36.27it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  Loading weights:  46%|████▌     | 155/339 [00:06<00:05, 36.27it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]Loading weights:  46%|████▌     | 156/339 [00:06<00:05, 36.27it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▌     | 156/339 [00:06<00:05, 36.27it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  46%|████▋     | 157/339 [00:06<00:05, 36.27it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  Loading weights:  46%|████▋     | 157/339 [00:06<00:05, 36.27it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]Loading weights:  47%|████▋     | 158/339 [00:06<00:04, 36.27it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 158/339 [00:06<00:04, 36.27it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  47%|████▋     | 159/339 [00:06<00:04, 36.27it/s, Materializing param=model.layers.13.input_layernorm.weight] Loading weights:  47%|████▋     | 159/339 [00:06<00:04, 36.27it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  47%|████▋     | 160/339 [00:06<00:04, 36.27it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  Loading weights:  47%|████▋     | 160/339 [00:06<00:04, 36.27it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:06<00:04, 40.44it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:06<00:04, 40.44it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  47%|████▋     | 161/339 [00:06<00:04, 40.44it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  48%|████▊     | 162/339 [00:06<00:04, 40.44it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  Loading weights:  48%|████▊     | 162/339 [00:06<00:04, 40.44it/s, Materializing param=model.layers.13.mlp.up_proj.weight]Loading weights:  48%|████▊     | 163/339 [00:06<00:04, 40.44it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 163/339 [00:06<00:04, 40.44it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  48%|████▊     | 164/339 [00:06<00:04, 40.44it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          Loading weights:  48%|████▊     | 164/339 [00:06<00:04, 40.44it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]Loading weights:  49%|████▊     | 165/339 [00:06<00:04, 40.44it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▊     | 165/339 [00:06<00:04, 40.44it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 166/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  49%|████▉     | 167/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  Loading weights:  49%|████▉     | 167/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]Loading weights:  50%|████▉     | 168/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 168/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  50%|████▉     | 169/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  Loading weights:  50%|████▉     | 169/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]Loading weights:  50%|█████     | 170/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 170/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  50%|█████     | 171/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.14.input_layernorm.weight] Loading weights:  50%|█████     | 171/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  51%|█████     | 172/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  Loading weights:  51%|█████     | 172/339 [00:06<00:04, 35.99it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  51%|█████     | 173/339 [00:07<00:04, 40.98it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  51%|█████     | 173/339 [00:07<00:04, 40.98it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████     | 173/339 [00:07<00:04, 40.98it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  51%|█████▏    | 174/339 [00:07<00:04, 40.98it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  Loading weights:  51%|█████▏    | 174/339 [00:07<00:04, 40.98it/s, Materializing param=model.layers.14.mlp.up_proj.weight]Loading weights:  52%|█████▏    | 175/339 [00:07<00:04, 40.98it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 175/339 [00:07<00:04, 40.98it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  52%|█████▏    | 176/339 [00:07<00:03, 40.98it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          Loading weights:  52%|█████▏    | 176/339 [00:07<00:03, 40.98it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]Loading weights:  52%|█████▏    | 177/339 [00:07<00:03, 40.98it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  52%|█████▏    | 177/339 [00:07<00:03, 40.98it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 178/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  53%|█████▎    | 179/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  Loading weights:  53%|█████▎    | 179/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]Loading weights:  53%|█████▎    | 180/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 180/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  53%|█████▎    | 181/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  Loading weights:  53%|█████▎    | 181/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]Loading weights:  54%|█████▎    | 182/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▎    | 182/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  54%|█████▍    | 183/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.15.input_layernorm.weight] Loading weights:  54%|█████▍    | 183/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  54%|█████▍    | 184/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  Loading weights:  54%|█████▍    | 184/339 [00:07<00:04, 36.56it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:07<00:03, 41.38it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:07<00:03, 41.38it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 185/339 [00:07<00:03, 41.38it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  55%|█████▍    | 186/339 [00:07<00:03, 41.38it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  Loading weights:  55%|█████▍    | 186/339 [00:07<00:03, 41.38it/s, Materializing param=model.layers.15.mlp.up_proj.weight]Loading weights:  55%|█████▌    | 187/339 [00:07<00:03, 41.38it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 187/339 [00:07<00:03, 41.38it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  55%|█████▌    | 188/339 [00:07<00:03, 41.38it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          Loading weights:  55%|█████▌    | 188/339 [00:07<00:03, 41.38it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]Loading weights:  56%|█████▌    | 189/339 [00:07<00:03, 41.38it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 189/339 [00:07<00:03, 41.38it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:07<00:04, 36.54it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:07<00:04, 36.54it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▌    | 190/339 [00:07<00:04, 36.54it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  56%|█████▋    | 191/339 [00:07<00:04, 36.54it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  Loading weights:  56%|█████▋    | 191/339 [00:07<00:04, 36.54it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]Loading weights:  57%|█████▋    | 192/339 [00:07<00:04, 36.54it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 192/339 [00:07<00:04, 36.54it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  57%|█████▋    | 193/339 [00:07<00:03, 36.54it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  Loading weights:  57%|█████▋    | 193/339 [00:07<00:03, 36.54it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]Loading weights:  57%|█████▋    | 194/339 [00:07<00:03, 36.54it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  57%|█████▋    | 194/339 [00:07<00:03, 36.54it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  58%|█████▊    | 195/339 [00:07<00:03, 36.54it/s, Materializing param=model.layers.16.input_layernorm.weight] Loading weights:  58%|█████▊    | 195/339 [00:07<00:03, 36.54it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  58%|█████▊    | 196/339 [00:07<00:03, 36.54it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  Loading weights:  58%|█████▊    | 196/339 [00:07<00:03, 36.54it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:07<00:03, 41.12it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:07<00:03, 41.12it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 197/339 [00:07<00:03, 41.12it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  58%|█████▊    | 198/339 [00:07<00:03, 41.12it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  Loading weights:  58%|█████▊    | 198/339 [00:07<00:03, 41.12it/s, Materializing param=model.layers.16.mlp.up_proj.weight]Loading weights:  59%|█████▊    | 199/339 [00:07<00:03, 41.12it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▊    | 199/339 [00:07<00:03, 41.12it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  59%|█████▉    | 200/339 [00:07<00:03, 41.12it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          Loading weights:  59%|█████▉    | 200/339 [00:07<00:03, 41.12it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]Loading weights:  59%|█████▉    | 201/339 [00:07<00:03, 41.12it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  59%|█████▉    | 201/339 [00:07<00:03, 41.12it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 202/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  60%|█████▉    | 203/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  Loading weights:  60%|█████▉    | 203/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]Loading weights:  60%|██████    | 204/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 204/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  60%|██████    | 205/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  Loading weights:  60%|██████    | 205/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]Loading weights:  61%|██████    | 206/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 206/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  61%|██████    | 207/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.17.input_layernorm.weight] Loading weights:  61%|██████    | 207/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  61%|██████▏   | 208/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  Loading weights:  61%|██████▏   | 208/339 [00:07<00:03, 36.65it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:07<00:03, 41.72it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:07<00:03, 41.72it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 209/339 [00:07<00:03, 41.72it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  62%|██████▏   | 210/339 [00:08<00:03, 41.72it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  Loading weights:  62%|██████▏   | 210/339 [00:08<00:03, 41.72it/s, Materializing param=model.layers.17.mlp.up_proj.weight]Loading weights:  62%|██████▏   | 211/339 [00:08<00:03, 41.72it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  62%|██████▏   | 211/339 [00:08<00:03, 41.72it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  63%|██████▎   | 212/339 [00:08<00:03, 41.72it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          Loading weights:  63%|██████▎   | 212/339 [00:08<00:03, 41.72it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]Loading weights:  63%|██████▎   | 213/339 [00:08<00:03, 41.72it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 213/339 [00:08<00:03, 41.72it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 214/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  63%|██████▎   | 215/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  Loading weights:  63%|██████▎   | 215/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]Loading weights:  64%|██████▎   | 216/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▎   | 216/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  64%|██████▍   | 217/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  Loading weights:  64%|██████▍   | 217/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]Loading weights:  64%|██████▍   | 218/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  64%|██████▍   | 218/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  65%|██████▍   | 219/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.18.input_layernorm.weight] Loading weights:  65%|██████▍   | 219/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.18.input_layernorm.weight]Loading weights:  65%|██████▍   | 220/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  Loading weights:  65%|██████▍   | 220/339 [00:08<00:03, 37.19it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 221/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  65%|██████▌   | 222/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  Loading weights:  65%|██████▌   | 222/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.mlp.up_proj.weight]Loading weights:  66%|██████▌   | 223/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 223/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  66%|██████▌   | 224/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          Loading weights:  66%|██████▌   | 224/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]Loading weights:  66%|██████▋   | 225/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  66%|██████▋   | 225/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 226/339 [00:08<00:03, 35.97it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 227/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  67%|██████▋   | 227/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  Loading weights:  67%|██████▋   | 227/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]Loading weights:  67%|██████▋   | 228/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  67%|██████▋   | 228/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  68%|██████▊   | 229/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  Loading weights:  68%|██████▊   | 229/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]Loading weights:  68%|██████▊   | 230/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 230/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  68%|██████▊   | 231/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.19.input_layernorm.weight] Loading weights:  68%|██████▊   | 231/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  68%|██████▊   | 232/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  Loading weights:  68%|██████▊   | 232/339 [00:08<00:02, 39.77it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:08<00:02, 43.82it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:08<00:02, 43.82it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▊   | 233/339 [00:08<00:02, 43.82it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  69%|██████▉   | 234/339 [00:08<00:02, 43.82it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  Loading weights:  69%|██████▉   | 234/339 [00:08<00:02, 43.82it/s, Materializing param=model.layers.19.mlp.up_proj.weight]Loading weights:  69%|██████▉   | 235/339 [00:08<00:02, 43.82it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  69%|██████▉   | 235/339 [00:08<00:02, 43.82it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  70%|██████▉   | 236/339 [00:08<00:02, 43.82it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          Loading weights:  70%|██████▉   | 236/339 [00:08<00:02, 43.82it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]Loading weights:  70%|██████▉   | 237/339 [00:08<00:02, 43.82it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|██████▉   | 237/339 [00:08<00:02, 43.82it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|███████   | 238/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  70%|███████   | 238/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  70%|███████   | 238/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  71%|███████   | 239/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  Loading weights:  71%|███████   | 239/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]Loading weights:  71%|███████   | 240/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 240/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  71%|███████   | 241/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  Loading weights:  71%|███████   | 241/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]Loading weights:  71%|███████▏  | 242/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  71%|███████▏  | 242/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  72%|███████▏  | 243/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.20.input_layernorm.weight] Loading weights:  72%|███████▏  | 243/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  72%|███████▏  | 244/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  Loading weights:  72%|███████▏  | 244/339 [00:08<00:02, 38.58it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:08<00:02, 43.35it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:08<00:02, 43.35it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  72%|███████▏  | 245/339 [00:08<00:02, 43.35it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  73%|███████▎  | 246/339 [00:08<00:02, 43.35it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  Loading weights:  73%|███████▎  | 246/339 [00:08<00:02, 43.35it/s, Materializing param=model.layers.20.mlp.up_proj.weight]Loading weights:  73%|███████▎  | 247/339 [00:09<00:02, 43.35it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 247/339 [00:09<00:02, 43.35it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  73%|███████▎  | 248/339 [00:09<00:02, 43.35it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          Loading weights:  73%|███████▎  | 248/339 [00:09<00:02, 43.35it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]Loading weights:  73%|███████▎  | 249/339 [00:09<00:02, 43.35it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  73%|███████▎  | 249/339 [00:09<00:02, 43.35it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▎  | 250/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  74%|███████▍  | 251/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  Loading weights:  74%|███████▍  | 251/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]Loading weights:  74%|███████▍  | 252/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  74%|███████▍  | 252/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  75%|███████▍  | 253/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  Loading weights:  75%|███████▍  | 253/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]Loading weights:  75%|███████▍  | 254/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▍  | 254/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  75%|███████▌  | 255/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.21.input_layernorm.weight] Loading weights:  75%|███████▌  | 255/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  76%|███████▌  | 256/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  Loading weights:  76%|███████▌  | 256/339 [00:09<00:02, 38.03it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:09<00:01, 42.86it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:09<00:01, 42.86it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 257/339 [00:09<00:01, 42.86it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  76%|███████▌  | 258/339 [00:09<00:01, 42.86it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  Loading weights:  76%|███████▌  | 258/339 [00:09<00:01, 42.86it/s, Materializing param=model.layers.21.mlp.up_proj.weight]Loading weights:  76%|███████▋  | 259/339 [00:09<00:01, 42.86it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  76%|███████▋  | 259/339 [00:09<00:01, 42.86it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  77%|███████▋  | 260/339 [00:09<00:01, 42.86it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          Loading weights:  77%|███████▋  | 260/339 [00:09<00:01, 42.86it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]Loading weights:  77%|███████▋  | 261/339 [00:09<00:01, 42.86it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 261/339 [00:09<00:01, 42.86it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:09<00:02, 37.55it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:09<00:02, 37.55it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  77%|███████▋  | 262/339 [00:09<00:02, 37.55it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  78%|███████▊  | 263/339 [00:09<00:02, 37.55it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  Loading weights:  78%|███████▊  | 263/339 [00:09<00:02, 37.55it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]Loading weights:  78%|███████▊  | 264/339 [00:09<00:01, 37.55it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 264/339 [00:09<00:01, 37.55it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  78%|███████▊  | 265/339 [00:09<00:01, 37.55it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  Loading weights:  78%|███████▊  | 265/339 [00:09<00:01, 37.55it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]Loading weights:  78%|███████▊  | 266/339 [00:09<00:01, 37.55it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  78%|███████▊  | 266/339 [00:09<00:01, 37.55it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights:  79%|███████▉  | 267/339 [00:09<00:01, 37.55it/s, Materializing param=model.layers.22.input_layernorm.weight] Loading weights:  79%|███████▉  | 267/339 [00:09<00:01, 37.55it/s, Materializing param=model.layers.22.input_layernorm.weight]Loading weights:  79%|███████▉  | 268/339 [00:09<00:01, 37.55it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  Loading weights:  79%|███████▉  | 268/339 [00:09<00:01, 37.55it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.22.mlp.down_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  79%|███████▉  | 269/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]Loading weights:  80%|███████▉  | 270/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  Loading weights:  80%|███████▉  | 270/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.22.mlp.up_proj.weight]Loading weights:  80%|███████▉  | 271/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|███████▉  | 271/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]Loading weights:  80%|████████  | 272/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          Loading weights:  80%|████████  | 272/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]Loading weights:  81%|████████  | 273/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 273/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 274/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]Loading weights:  81%|████████  | 274/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 274/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]Loading weights:  81%|████████  | 275/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  Loading weights:  81%|████████  | 275/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]Loading weights:  81%|████████▏ | 276/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  81%|████████▏ | 276/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]Loading weights:  82%|████████▏ | 277/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  Loading weights:  82%|████████▏ | 277/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]Loading weights:  82%|████████▏ | 278/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 278/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]Loading weights:  82%|████████▏ | 279/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.23.input_layernorm.weight] Loading weights:  82%|████████▏ | 279/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.23.input_layernorm.weight]Loading weights:  83%|████████▎ | 280/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  Loading weights:  83%|████████▎ | 280/339 [00:09<00:01, 37.71it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.23.mlp.down_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 281/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]Loading weights:  83%|████████▎ | 282/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  Loading weights:  83%|████████▎ | 282/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.23.mlp.up_proj.weight]Loading weights:  83%|████████▎ | 283/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  83%|████████▎ | 283/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]Loading weights:  84%|████████▍ | 284/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          Loading weights:  84%|████████▍ | 284/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]Loading weights:  84%|████████▍ | 285/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 285/339 [00:09<00:01, 42.54it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  84%|████████▍ | 286/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]Loading weights:  85%|████████▍ | 287/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  Loading weights:  85%|████████▍ | 287/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]Loading weights:  85%|████████▍ | 288/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▍ | 288/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]Loading weights:  85%|████████▌ | 289/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  Loading weights:  85%|████████▌ | 289/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]Loading weights:  86%|████████▌ | 290/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 290/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]Loading weights:  86%|████████▌ | 291/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.24.input_layernorm.weight] Loading weights:  86%|████████▌ | 291/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.24.input_layernorm.weight]Loading weights:  86%|████████▌ | 292/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.24.mlp.down_proj.weight]  Loading weights:  86%|████████▌ | 292/339 [00:09<00:01, 37.80it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:10<00:01, 42.16it/s, Materializing param=model.layers.24.mlp.down_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:10<00:01, 42.16it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  86%|████████▋ | 293/339 [00:10<00:01, 42.16it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]Loading weights:  87%|████████▋ | 294/339 [00:10<00:01, 42.16it/s, Materializing param=model.layers.24.mlp.up_proj.weight]  Loading weights:  87%|████████▋ | 294/339 [00:10<00:01, 42.16it/s, Materializing param=model.layers.24.mlp.up_proj.weight]Loading weights:  87%|████████▋ | 295/339 [00:10<00:01, 42.16it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 295/339 [00:10<00:01, 42.16it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]Loading weights:  87%|████████▋ | 296/339 [00:10<00:01, 42.16it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]          Loading weights:  87%|████████▋ | 296/339 [00:10<00:01, 42.16it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]Loading weights:  88%|████████▊ | 297/339 [00:10<00:00, 42.16it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 297/339 [00:10<00:00, 42.16it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 298/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]Loading weights:  88%|████████▊ | 299/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]  Loading weights:  88%|████████▊ | 299/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]Loading weights:  88%|████████▊ | 300/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  88%|████████▊ | 300/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]Loading weights:  89%|████████▉ | 301/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]  Loading weights:  89%|████████▉ | 301/339 [00:10<00:01, 37.29it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]Loading weights:  89%|████████▉ | 302/339 [00:10<00:00, 37.29it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 302/339 [00:10<00:00, 37.29it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]Loading weights:  89%|████████▉ | 303/339 [00:10<00:00, 37.29it/s, Materializing param=model.layers.25.input_layernorm.weight] Loading weights:  89%|████████▉ | 303/339 [00:10<00:00, 37.29it/s, Materializing param=model.layers.25.input_layernorm.weight]Loading weights:  90%|████████▉ | 304/339 [00:10<00:00, 37.29it/s, Materializing param=model.layers.25.mlp.down_proj.weight]  Loading weights:  90%|████████▉ | 304/339 [00:10<00:00, 37.29it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:10<00:00, 42.14it/s, Materializing param=model.layers.25.mlp.down_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:10<00:00, 42.14it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|████████▉ | 305/339 [00:10<00:00, 42.14it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]Loading weights:  90%|█████████ | 306/339 [00:10<00:00, 42.14it/s, Materializing param=model.layers.25.mlp.up_proj.weight]  Loading weights:  90%|█████████ | 306/339 [00:10<00:00, 42.14it/s, Materializing param=model.layers.25.mlp.up_proj.weight]Loading weights:  91%|█████████ | 307/339 [00:10<00:00, 42.14it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 307/339 [00:10<00:00, 42.14it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]Loading weights:  91%|█████████ | 308/339 [00:10<00:00, 42.14it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]          Loading weights:  91%|█████████ | 308/339 [00:10<00:00, 42.14it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]Loading weights:  91%|█████████ | 309/339 [00:10<00:00, 42.14it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████ | 309/339 [00:10<00:00, 42.14it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  91%|█████████▏| 310/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]Loading weights:  92%|█████████▏| 311/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]  Loading weights:  92%|█████████▏| 311/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]Loading weights:  92%|█████████▏| 312/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 312/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]Loading weights:  92%|█████████▏| 313/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]  Loading weights:  92%|█████████▏| 313/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]Loading weights:  93%|█████████▎| 314/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 314/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]Loading weights:  93%|█████████▎| 315/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.26.input_layernorm.weight] Loading weights:  93%|█████████▎| 315/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.26.input_layernorm.weight]Loading weights:  93%|█████████▎| 316/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.26.mlp.down_proj.weight]  Loading weights:  93%|█████████▎| 316/339 [00:10<00:00, 37.10it/s, Materializing param=model.layers.26.mlp.down_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:10<00:00, 42.03it/s, Materializing param=model.layers.26.mlp.down_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:10<00:00, 42.03it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▎| 317/339 [00:10<00:00, 42.03it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]Loading weights:  94%|█████████▍| 318/339 [00:10<00:00, 42.03it/s, Materializing param=model.layers.26.mlp.up_proj.weight]  Loading weights:  94%|█████████▍| 318/339 [00:10<00:00, 42.03it/s, Materializing param=model.layers.26.mlp.up_proj.weight]Loading weights:  94%|█████████▍| 319/339 [00:10<00:00, 42.03it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 319/339 [00:10<00:00, 42.03it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]Loading weights:  94%|█████████▍| 320/339 [00:10<00:00, 42.03it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]          Loading weights:  94%|█████████▍| 320/339 [00:10<00:00, 42.03it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]Loading weights:  95%|█████████▍| 321/339 [00:10<00:00, 42.03it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 321/339 [00:10<00:00, 42.03it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▍| 322/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]Loading weights:  95%|█████████▌| 323/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]  Loading weights:  95%|█████████▌| 323/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]Loading weights:  96%|█████████▌| 324/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 324/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]Loading weights:  96%|█████████▌| 325/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]  Loading weights:  96%|█████████▌| 325/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]Loading weights:  96%|█████████▌| 326/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▌| 326/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]Loading weights:  96%|█████████▋| 327/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.27.input_layernorm.weight] Loading weights:  96%|█████████▋| 327/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.27.input_layernorm.weight]Loading weights:  97%|█████████▋| 328/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.27.mlp.down_proj.weight]  Loading weights:  97%|█████████▋| 328/339 [00:10<00:00, 36.68it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:11<00:00, 41.70it/s, Materializing param=model.layers.27.mlp.down_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:11<00:00, 41.70it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 329/339 [00:11<00:00, 41.70it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]Loading weights:  97%|█████████▋| 330/339 [00:11<00:00, 41.70it/s, Materializing param=model.layers.27.mlp.up_proj.weight]  Loading weights:  97%|█████████▋| 330/339 [00:11<00:00, 41.70it/s, Materializing param=model.layers.27.mlp.up_proj.weight]Loading weights:  98%|█████████▊| 331/339 [00:11<00:00, 41.70it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 331/339 [00:11<00:00, 41.70it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]Loading weights:  98%|█████████▊| 332/339 [00:11<00:00, 41.70it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]          Loading weights:  98%|█████████▊| 332/339 [00:11<00:00, 41.70it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]Loading weights:  98%|█████████▊| 333/339 [00:11<00:00, 41.70it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  98%|█████████▊| 333/339 [00:11<00:00, 41.70it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:11<00:00, 37.10it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:11<00:00, 37.10it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▊| 334/339 [00:11<00:00, 37.10it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]Loading weights:  99%|█████████▉| 335/339 [00:11<00:00, 37.10it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]  Loading weights:  99%|█████████▉| 335/339 [00:11<00:00, 37.10it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]Loading weights:  99%|█████████▉| 336/339 [00:11<00:00, 37.10it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 336/339 [00:11<00:00, 37.10it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]Loading weights:  99%|█████████▉| 337/339 [00:11<00:00, 37.10it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]  Loading weights:  99%|█████████▉| 337/339 [00:11<00:00, 37.10it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]Loading weights: 100%|█████████▉| 338/339 [00:11<00:00, 37.10it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|█████████▉| 338/339 [00:11<00:00, 37.10it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]Loading weights: 100%|██████████| 339/339 [00:11<00:00, 37.10it/s, Materializing param=model.norm.weight]                      Loading weights: 100%|██████████| 339/339 [00:11<00:00, 37.10it/s, Materializing param=model.norm.weight]Loading weights: 100%|██████████| 339/339 [00:11<00:00, 30.22it/s, Materializing param=model.norm.weight]
================================================================================
Send prompt for semantic check:
Generated ASP code:

exam(Exam, Duration, Is_large).

student(Student, Exam).

period(P, Date, Time, Duration, Is_late, Penalty).

room(R, Capacity, Penalty).

order_constrain(Exam1, Exam2).

same_time_constraint(Exam1, Exam2).

different_time_constraint(Exam1, Exam2).

own_room_constraint(Exam).

Intended semantics:
Instance Variables:
- Exams: A set of exams that need to be scheduled.
    - Variables: duration, is_large
- Students: A set of students that are taking a set of exams.
    - Variables: exam
- Periods: A set of periods in which the exams can be scheduled.
    - Variables: date, time, duration, is_late, penalty
- Rooms: A set of rooms in which the exams can be scheduled.
    - Variables: capacity, penalty
- Order_Constrains: A set of constraints that specify the order in which some exams must be scheduled.
    - Variables: exam1, exam2
- Same_Time_Constraints: A set of constraints that specify the exams that must be scheduled at the same time.
    - Variables: exam1, exam2
- Different_Time_Constraints: A set of constraints that specify the exams that must not be scheduled at the same time.
    - Variables: exam1, exam2
- Own_Room_Constraints: A set of constraints that specify the exams that must not be scheduled in a room on their own.
    - Variables: exam


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
% Instance Template
exam(Exam, Duration, Is_large).

student(Student, Exam).

period(P, Date, Time, Duration, Is_late, Penalty).

room(R, Capacity, Penalty).

order_constrain(Exam1, Exam2).

same_time_constraint(Exam1, Exam2).

different_time_constraint(Exam1, Exam2).

own_room_constraint(Exam).

REPAIRED RESPONSE:

exam(Exam, Duration, Is_large).

student(Student, Exam).

period(P, Date, Time, Duration, Is_late, Penalty).

room(R, Capacity, Penalty).

order_constrain(Exam1, Exam2).

same_time_constraint(Exam1, Exam2).

different_time_constraint(Exam1, Exam2).

own_room_constraint(Exam).

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): True

####################################################################################
Instance Template:

exam(Exam, Duration, Is_large).

student(Student, Exam).

period(P, Date, Time, Duration, Is_late, Penalty).

room(R, Capacity, Penalty).

order_constrain(Exam1, Exam2).

same_time_constraint(Exam1, Exam2).

different_time_constraint(Exam1, Exam2).

own_room_constraint(Exam).
================================================================================
Send prompt for semantic check:
Generated ASP code:

{ exam_to_period(Exam, P, Date, Time, Duration, Is_late, Penalty, Room) : room(Room, _, _), period(P, Date, Time, Duration, Is_late, Penalty) } 1 :- exam(Exam).

Intended semantics:Goal:
An assignment of exams to periods and rooms.
    - Variables: period, room


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
% 1
{ exam_to_period(Exam, P, Date, Time, Duration, Is_late, Penalty, Room) : room(Room, _, _), period(P, Date, Time, Duration, Is_late, Penalty) } 1 :- exam(Exam).

REPAIRED RESPONSE:

{ exam_to_period(Exam, P, Date, Time, Duration, Is_late, Penalty, Room) : room(Room, _, _), period(P, Date, Time, Duration, Is_late, Penalty) } 1 :- exam(Exam).

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): True

####################################################################################


Generator

{ exam_to_period(Exam, P, Date, Time, Duration, Is_late, Penalty, Room) : room(Room, _, _), period(P, Date, Time, Duration, Is_late, Penalty) } 1 :- exam(Exam).


Hard Constraints

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- student(Student, Exam1), student(Student, Exam2), exam_to_period(Exam1, _, _, _, _, _, _, _), exam_to_period(Exam2, _, _, _, _, _, _, _), different_time_constraint(Exam1, Exam2).

Intended semantics:- No student sits more than one examination at the same time.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that no student can take two exams in the same period, but it does not ensure that no student sits more than one examination at the same time across different periods."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- student(Student, Exam1), student(Student, Exam2), exam_to_period(Exam1, _, _, _, _, _, _, _), exam_to_period(Exam2, _, _, _, _, _, _, _), different_time_constraint(Exam1, Exam2).

####################################################################################
- No student sits more than one examination at the same time.
:- student(Student, Exam1), student(Student, Exam2), exam_to_period(Exam1, _, _, _, _, _, _, _), exam_to_period(Exam2, _, _, _, _, _, _, _), different_time_constraint(Exam1, Exam2).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- room(R, Capacity, _), exam_to_period(Exam1, R, _, _, _, _, _), exam_to_period(Exam2, R, _, _, _, _, _), student(Student, Exam1), student(Student, Exam2), Exam1 != Exam2, #count{Exam : exam_to_period(Exam, R, _, _, _, _, _, _), student(Student, Exam)} > Capacity.

Intended semantics:- The number of students taking exams in a room at the same time should not exceed the capacity of the room.
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- room(R, Capacity, _), exam_to_period(Exam1, R, _, _, _, _, _), exam_to_period(Exam2, R, _, _, _, _, _), student(Student, Exam1), student(Student, Exam2), Exam1 != Exam2, #count{Exam : exam_to_period(Exam, R, _, _, _, _, _, _), student(Student, Exam)} > Capacity.

####################################################################################
- The number of students taking exams in a room at the same time should not exceed the capacity of the room.
 
:- room(R, Capacity, _), exam_to_period(Exam1, R, _, _, _, _, _), exam_to_period(Exam2, R, _, _, _, _, _), student(Student, Exam1), student(Student, Exam2), Exam1 != Exam2, #count{Exam : exam_to_period(Exam, R, _, _, _, _, _, _), student(Student, Exam)} > Capacity.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- exam_to_period(Exam, P1, Date1, Time1, Duration1, Is_late1, Penalty1, Room1), exam_to_period(Exam, P2, Date2, Time2, Duration2, Is_late2, Penalty2, Room2), P1 != P2, Date1 = Date2, abs(Time1 - Time2) < Duration1 + Duration2.

Intended semantics:- Period Lengths are not violated.

Semantic check result:
{
  "match": "false",
  "reason": "The code does not enforce that the periods do not overlap in length. It only checks that the periods do not clash in time."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- exam_to_period(Exam, P1, Date1, Time1, Duration1, Is_late1, Penalty1, Room1), exam_to_period(Exam, P2, Date2, Time2, Duration2, Is_late2, Penalty2, Room2), P1 != P2, Date1 = Date2, abs(Time1 - Time2) < Duration1 + Duration2.

####################################################################################
- Period Lengths are not violated.
:- exam_to_period(Exam, P1, Date1, Time1, Duration1, Is_late1, Penalty1, Room1), exam_to_period(Exam, P2, Date2, Time2, Duration2, Is_late2, Penalty2, Room2), P1 != P2, Date1 = Date2, abs(Time1 - Time2) < Duration1 + Duration2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- exam_to_period(Exam1, _, _, _, _, _, _, _), exam_to_period(Exam2, _, _, _, _, _, _, _), order_constrain(Exam1, Exam2), exam_to_period(Exam2, P2, Date2, Time2, _, _, _, _), exam_to_period(Exam1, P1, Date1, Time1, _, _, _, _), P1 >= P2, Date1 >= Date2, Time1 >= Time2.

Intended semantics:- Some exams must be before other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that the periods of some exams must be before the periods of other exams, which is a stronger constraint than just requiring some exams to be before others. It specifies exact ordering in terms of periods, dates, and times."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- exam_to_period(Exam1, _, _, _, _, _, _, _), exam_to_period(Exam2, _, _, _, _, _, _, _), order_constrain(Exam1, Exam2), exam_to_period(Exam2, P2, Date2, Time2, _, _, _, _), exam_to_period(Exam1, P1, Date1, Time1, _, _, _, _), P1 >= P2, Date1 >= Date2, Time1 >= Time2.

####################################################################################
- Some exams must be before other exams.
:- exam_to_period(Exam1, _, _, _, _, _, _, _), exam_to_period(Exam2, _, _, _, _, _, _, _), order_constrain(Exam1, Exam2), exam_to_period(Exam2, P2, Date2, Time2, _, _, _, _), exam_to_period(Exam1, P1, Date1, Time1, _, _, _, _), P1 >= P2, Date1 >= Date2, Time1 >= Time2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
same_time_constraint(Exam1, Exam2).

Intended semantics:- Some exams must be at the same time as other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code uses a predicate name with incorrect syntax and does not specify the exams involved in the constraint."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
same_time_constraint(Exam1, Exam2).

####################################################################################
- Some exams must be at the same time as other exams.
same_time_constraint(Exam1, Exam2).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- exam_to_period(Exam1, P1, Date1, Time1, _, _, _, _), exam_to_period(Exam2, P2, Date2, Time2, _, _, _, _), different_time_constraint(Exam1, Exam2), P1 == P2, Date1 == Date2, Time1 != Time2.

Intended semantics:- Some exams must be at a differen time than other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The code incorrectly enforces that exams must be at a different time in the same period and on the same date, which is not the intended semantics."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- exam_to_period(Exam1, P1, Date1, Time1, _, _, _, _), exam_to_period(Exam2, P2, Date2, Time2, _, _, _, _), different_time_constraint(Exam1, Exam2), P1 == P2, Date1 == Date2, Time1 != Time2.

####################################################################################
- Some exams must be at a differen time than other exams.
:- exam_to_period(Exam1, P1, Date1, Time1, _, _, _, _), exam_to_period(Exam2, P2, Date2, Time2, _, _, _, _), different_time_constraint(Exam1, Exam2), P1 == P2, Date1 == Date2, Time1 != Time2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
own_room_constraint(Exam).

Intended semantics:- Some exams must take place in a room with no other exams.

Semantic check result:
{
  "match": "false",
  "reason": "The generated ASP code does not enforce any constraints related to exams taking place in a room with no other exams. It only defines the predicate without any associated rules."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
own_room_constraint(Exam).

####################################################################################
- Some exams must take place in a room with no other exams.
own_room_constraint(Exam).


Soft Constraints:

================================================================================
Initial response with syntax error:
penalty("ExamOverlapDayConsecutive",exam_to_period(Exam1,P1,Date1,Time1,Duration1,Is_late1,_,Room1),7) :- exam_to_period(Exam1,P1,Date1,Time1,Duration1,Is_late1,_,Room1), exam_to_period(Exam2,P2,Date2,Time2,Duration2,Is_late2,_,Room2), Date1 = Date2, abs(P2-P1) =< 1, exam(Exam1), exam(Exam2), not Exam1 = Exam2.

Error: <string>:1:264-265: error: syntax error, unexpected <

 Starting syntax repair attempts...
================================================================================
--------------------------------------------------------------------------------
Correction attempt 1:
penalty("ExamOverlapDayConsecutive", exam_to_period(Exam1, P1, Date1, Time1, Duration1, Is_late1, _, Room1), 7) :-
    exam_to_period(Exam2, P2, Date2, Time2, Duration2, Is_late2, _, Room2),
    Date1 = Date2,
    abs(P2 - P1) =< 1,
    exam(Exam1),
    exam(Exam2),
    not Exam1 = Exam2.

penalty("ExamOverlapDayNonConsecutive", exam_to_period(Exam1, P1, Date1, Time1, Duration1, Is_late1, _, Room1), 5) :-
    exam_to_period(Exam2, P2, Date2, Time2, Duration2, Is_late2, _, Room2),
    Date1 = Date2,
    abs(P2 - P1) > 1,
    exam(Exam1),
    exam(Exam2),
    not Exam1 = Exam2.

--------------------------------------------------------------------------------
Multiple statement blocks returned by LLM - Correction attempt 2:
penalty("ExamOverlapDayConsecutive", exam_to_period(Exam1, P1, Date1, Time1, Duration1, Is_late1, _, Room1), 7) :-
    exam_to_period(Exam2, P2, Date2, Time2, Duration2, Is_late2, _, Room2),
    Date1 = Date2,
    abs(P2 - P1) =< 1,
    exam(Exam1),
    exam(Exam2),
    not Exam1 = Exam2.

penalty("ExamOverlapDayNonConsecutive", exam_to_period(Exam1, P1, Date1, Time1, Duration1, Is_late1, _, Room1), 5) :-
    exam_to_period(Exam2, P2, Date2, Time2, Duration2, Is_late2, _, Room2),
    Date1 = Date2,
    abs(P2 - P1) > 1,
    exam(Exam1),
    exam(Exam2),
    not Exam1 = Exam2.

--------------------------------------------------------------------------------
Multiple statement blocks returned by LLM - Correction attempt 3:
penalty("ExamOverlapDayConsecutive", exam_to_period(Exam1, P1, Date1, Time1, Duration1, Is_late1, _, Room1), 7) :-
    exam_to_period(Exam2, P2, Date2, Time2, Duration2, Is_late2, _, Room2),
    Date1 = Date2,
    abs(P2 - P1) =< 1,
    exam(Exam1),
    exam(Exam2),
    not Exam1 = Exam2.

penalty("ExamOverlapDayNonConsecutive", exam_to_period(Exam1, P1, Date1, Time1, Duration1, Is_late1, _, Room1), 5) :-
    exam_to_period(Exam2, P2, Date2, Time2, Duration2, Is_late2, _, Room2),
    Date1 = Date2,
    abs(P2 - P1) > 1,
    exam(Exam1),
    exam(Exam2),
    not Exam1 = Exam2.

--------------------------------------------------------------------------------
Syntax error still present: <string>:4:19-20: error: syntax error, unexpected <

================================================================================
Syntax errors remain after repair attempts. Cannot proceed to semantic checking.
Total syntax errors: 1

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED , BUT UNSUCCESSFULLY
INITIAL RESPONSE:
penalty("ExamOverlapDayConsecutive",exam_to_period(Exam1,P1,Date1,Time1,Duration1,Is_late1,_,Room1),7) :- exam_to_period(Exam1,P1,Date1,Time1,Duration1,Is_late1,_,Room1), exam_to_period(Exam2,P2,Date2,Time2,Duration2,Is_late2,_,Room2), Date1 = Date2, abs(P2-P1) =< 1, exam(Exam1), exam(Exam2), not Exam1 = Exam2.
penalty("ExamOverlapDayNotConsecutive",exam_to_period(Exam1,P1,Date1,Time1,Duration1,Is_late1,_,Room1),5) :- exam_to_period(Exam1,P1,Date1,Time1,Duration1,Is_late1,_,Room1), exam_to_period(Exam2,P2,Date2,Time2,Duration2,Is_late2,_,Room2), Date1 = Date2, abs(P2-P1) > 1, exam(Exam1), exam(Exam2), not Exam1 = Exam2.

REPAIRED RESPONSE:
penalty("ExamOverlapDayConsecutive", exam_to_period(Exam1, P1, Date1, Time1, Duration1, Is_late1, _, Room1), 7) :-
    exam_to_period(Exam2, P2, Date2, Time2, Duration2, Is_late2, _, Room2),
    Date1 = Date2,
    abs(P2 - P1) =< 1,
    exam(Exam1),
    exam(Exam2),
    not Exam1 = Exam2.

penalty("ExamOverlapDayNonConsecutive", exam_to_period(Exam1, P1, Date1, Time1, Duration1, Is_late1, _, Room1), 5) :-
    exam_to_period(Exam2, P2, Date2, Time2, Duration2, Is_late2, _, Room2),
    Date1 = Date2,
    abs(P2 - P1) > 1,
    exam(Exam1),
    exam(Exam2),
    not Exam1 = Exam2.
penalty("ExamOverlapDayNotConsecutive",exam_to_period(Exam1,P1,Date1,Time1,Duration1,Is_late1,_,Room1),5) :- exam_to_period(Exam1,P1,Date1,Time1,Duration1,Is_late1,_,Room1), exam_to_period(Exam2,P2,Date2,Time2,Duration2,Is_late2,_,Room2), Date1 = Date2, abs(P2-P1) > 1, exam(Exam1), exam(Exam2), not Exam1 = Exam2.

Total syntax errors remaining after repair attempts: 1

Semantics correct (according to LLM): False

####################################################################################
- Students should not have more than one exam in the same day
 Penalty: 7 for two exams in the same day that are in consecutive periods.
 Penalty: 5 for two exams in the same day that are not in consecutive periods.
penalty("ExamOverlapDayConsecutive", exam_to_period(Exam1, P1, Date1, Time1, Duration1, Is_late1, _, Room1), 7) :-
    exam_to_period(Exam2, P2, Date2, Time2, Duration2, Is_late2, _, Room2),
    Date1 = Date2,
    abs(P2 - P1) =< 1,
    exam(Exam1),
    exam(Exam2),
    not Exam1 = Exam2.

penalty("ExamOverlapDayNonConsecutive", exam_to_period(Exam1, P1, Date1, Time1, Duration1, Is_late1, _, Room1), 5) :-
    exam_to_period(Exam2, P2, Date2, Time2, Duration2, Is_late2, _, Room2),
    Date1 = Date2,
    abs(P2 - P1) > 1,
    exam(Exam1),
    exam(Exam2),
    not Exam1 = Exam2.
penalty("ExamOverlapDayNotConsecutive",exam_to_period(Exam1,P1,Date1,Time1,Duration1,Is_late1,_,Room1),5) :- exam_to_period(Exam1,P1,Date1,Time1,Duration1,Is_late1,_,Room1), exam_to_period(Exam2,P2,Date2,Time2,Duration2,Is_late2,_,Room2), Date1 = Date2, abs(P2-P1) > 1, exam(Exam1), exam(Exam2), not Exam1 = Exam2.

================================================================================
Initial response with syntax error:
penalty("SameRoomDifferentLength",exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room),10) :-
exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room),
exam_to_period(Exam2, P, Date, Time, Duration2, Is_late2, Penalty2, Room),
exam(Exam1, Duration1, _),
exam(Exam2, Duration2, _),
Duration1 \= Duration2.

Error: <string>:6:12-13: error: syntax error, unexpected =

 Starting syntax repair attempts...
================================================================================
--------------------------------------------------------------------------------
Correction attempt 1:
penalty("SameRoomDifferentLength", exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room), 10) :-
    exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room),
    exam_to_period(Exam2, P, Date, Time, Duration2, Is_late2, Penalty2, Room),
    exam(Exam1, Duration1, _),
    exam(Exam2, Duration2, _),
    Duration1 \= Duration2.

--------------------------------------------------------------------------------
Syntax error still present: <string>:6:16-17: error: syntax error, unexpected =

--------------------------------------------------------------------------------
Correction attempt 2:
penalty("SameRoomDifferentLength", exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room), 10) :-
    exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room),
    exam_to_period(Exam2, P, Date, Time, Duration2, Is_late2, Penalty2, Room),
    exam(Exam1, Duration1, _),
    exam(Exam2, Duration2, _),
    Duration1 \= Duration2.

--------------------------------------------------------------------------------
Syntax error still present: <string>:6:16-17: error: syntax error, unexpected =

--------------------------------------------------------------------------------
Correction attempt 3:
penalty("SameRoomDifferentLength", exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room), 10) :-
    exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room),
    exam_to_period(Exam2, P, Date, Time, Duration2, Is_late2, Penalty2, Room),
    exam(Exam1, Duration1, _),
    exam(Exam2, Duration2, _),
    Duration1 \= Duration2.

--------------------------------------------------------------------------------
Syntax error still present: <string>:6:16-17: error: syntax error, unexpected =

================================================================================
Syntax errors remain after repair attempts. Cannot proceed to semantic checking.
Total syntax errors: 1

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED , BUT UNSUCCESSFULLY
INITIAL RESPONSE:
penalty("SameRoomDifferentLength",exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room),10) :-
    exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room),
    exam_to_period(Exam2, P, Date, Time, Duration2, Is_late2, Penalty2, Room),
    exam(Exam1, Duration1, _),
    exam(Exam2, Duration2, _),
    Duration1 \= Duration2.

REPAIRED RESPONSE:
penalty("SameRoomDifferentLength", exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room), 10) :-
    exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room),
    exam_to_period(Exam2, P, Date, Time, Duration2, Is_late2, Penalty2, Room),
    exam(Exam1, Duration1, _),
    exam(Exam2, Duration2, _),
    Duration1 \= Duration2.

Total syntax errors remaining after repair attempts: 1

Semantics correct (according to LLM): False

####################################################################################
- Exams of different lengths should not be scheduled in the same room in the same period
 Penalty: 10 for each exam of different length scheduled in the same room in the same period.
penalty("SameRoomDifferentLength", exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room), 10) :-
    exam_to_period(Exam1, P, Date, Time, Duration1, Is_late1, Penalty1, Room),
    exam_to_period(Exam2, P, Date, Time, Duration2, Is_late2, Penalty2, Room),
    exam(Exam1, Duration1, _),
    exam(Exam2, Duration2, _),
    Duration1 \= Duration2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("PeriodPenalty",exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room),Penalty*(1)) :- exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room), period(P,Date,Time,Duration,Is_late,Penalty).

Intended semantics:- Some periods have associated penalties
 Penalty: 1 for each time an exam is scheduled in a period with an associated penalty. Weighted by the penalty value of the period.

Semantic check result:
{
  "match": "false",
  "reason": "The generated code incorrectly multiplies the penalty by 1, which has no effect. It should directly sum the penalties."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("PeriodPenalty",exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room),Penalty*(1)) :- exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room), period(P,Date,Time,Duration,Is_late,Penalty).

####################################################################################
- Some periods have associated penalties
 Penalty: 1 for each time an exam is scheduled in a period with an associated penalty. Weighted by the penalty value of the period.
penalty("PeriodPenalty",exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room),Penalty*(1)) :- exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room), period(P,Date,Time,Duration,Is_late,Penalty).

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("RoomPenalty",exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room),Penalty*room(Room,_,Penalty)) :- exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room), room(Room,_,Penalty).

Intended semantics:- Some rooms have associated penalties
 Penalty: 1 for each time an exam is scheduled in a room with an associated penalty. Weighted by the penalty value of the room.

Semantic check result:
{
  "match": "false",
  "reason": "The generated code incorrectly multiplies the penalty by the room's penalty value, leading to an exponential increase in the penalty. The intended semantics do not involve such multiplication."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("RoomPenalty",exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room),Penalty*room(Room,_,Penalty)) :- exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room), room(Room,_,Penalty).

####################################################################################
- Some rooms have associated penalties
 Penalty: 1 for each time an exam is scheduled in a room with an associated penalty. Weighted by the penalty value of the room.
penalty("RoomPenalty",exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room),Penalty*room(Room,_,Penalty)) :- exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room), room(Room,_,Penalty).

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("ExamSpread",student(Student,Exam1),3) :-
student(Student,Exam1),
student(Student,Exam2),
exam_to_period(Exam1,P1,Date1,Time1,Duration1,IsLate1,_,Room1),
exam_to_period(Exam2,P2,Date2,Time2,Duration2,IsLate2,_,Room2),
abs(P1-P2) <= 7,
P1 < P2.

Intended semantics:- Students should not have more than one exam within a spread of 7 periods.
 Penalty: 3 for each time a student has two exams within a spread of periods.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
penalty("ExamSpread",student(Student,Exam1),3) :-
    student(Student,Exam1),
    student(Student,Exam2),
    exam_to_period(Exam1,P1,Date1,Time1,Duration1,IsLate1,_,Room1),
    exam_to_period(Exam2,P2,Date2,Time2,Duration2,IsLate2,_,Room2),
    abs(P1-P2) <= 7,
    P1 < P2.

REPAIRED RESPONSE:
penalty("ExamSpread",student(Student,Exam1),3) :-
student(Student,Exam1),
student(Student,Exam2),
exam_to_period(Exam1,P1,Date1,Time1,Duration1,IsLate1,_,Room1),
exam_to_period(Exam2,P2,Date2,Time2,Duration2,IsLate2,_,Room2),
abs(P1-P2) <= 7,
P1 < P2.

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): True

####################################################################################
- Students should not have more than one exam within a spread of 7 periods.
 Penalty: 3 for each time a student has two exams within a spread of periods.
penalty("ExamSpread",student(Student,Exam1),3) :-
student(Student,Exam1),
student(Student,Exam2),
exam_to_period(Exam1,P1,Date1,Time1,Duration1,IsLate1,_,Room1),
exam_to_period(Exam2,P2,Date2,Time2,Duration2,IsLate2,_,Room2),
abs(P1-P2) <= 7,
P1 < P2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("LatePeriodForLargeExam",exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room),(Penalty*5)) :- exam(Exam,_,Is_large), exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room), Is_late = "yes".

Intended semantics:- An exam that is large should not be scheduled in a period that is late.
 Penalty: 5 for each large exam scheduled in a period that is late.

Semantic check result:
{
  "match": "false",
  "reason": "The code incorrectly calculates the penalty as 'Penalty*5' instead of just '5' for each large exam scheduled in a late period."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("LatePeriodForLargeExam",exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room),(Penalty*5)) :- exam(Exam,_,Is_large), exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room), Is_late = "yes".

####################################################################################
- An exam that is large should not be scheduled in a period that is late.
 Penalty: 5 for each large exam scheduled in a period that is late.
penalty("LatePeriodForLargeExam",exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room),(Penalty*5)) :- exam(Exam,_,Is_large), exam_to_period(Exam,P,Date,Time,Duration,Is_late,Penalty,Room), Is_late = "yes".

Full program saved to Results/experiment/examination_timetabling_Qwen2.5-7B-Instruct_k=3_n=0_20260110_174338.lp
================================================================================
Send prompt for semantic check:
Generated ASP code:

event(Event).

room(Room, Capacity).

timeslot(Day, Hour).

student(Student, Event).

feature_requirement(FEATURE, Event).

room_feature(FEATURE, Room).

precedence_constraint(EVENT1, EVENT2).

timeslot_constraint(EVENT, TIMESLOT).

Intended semantics:
- Events: A set of N events.
    - Variables: none
- Rooms: A set of rooms in which the events can be scheduled.
    - Variables: capacity
- Timeslots: A set of timeslots in which the events can be scheduled.
    - Variables: day, hour
- Students: A set of students that are attending a set of events.
    - Variables: event
- Feature Requirements: A set of features which events may have.
    - Variables: feature, event
- Room Features: A set of features which rooms may have.
    - Variables: feature, room
- Precedence Constraints: A set of constraints that specify if some events must be scheduled before other events.
    - Variables: event1, event2
- Timeslot Constraints: A set of constraints for each event that specify the timeslots in which the event can be scheduled.
    - Variables: event, timeslot


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
% Instance Template
event(Event).

room(Room, Capacity).

timeslot(Day, Hour).

student(Student, Event).

feature_requirement(FEATURE, Event).

room_feature(FEATURE, Room).

precedence_constraint(EVENT1, EVENT2).

timeslot_constraint(EVENT, TIMESLOT).

REPAIRED RESPONSE:

event(Event).

room(Room, Capacity).

timeslot(Day, Hour).

student(Student, Event).

feature_requirement(FEATURE, Event).

room_feature(FEATURE, Room).

precedence_constraint(EVENT1, EVENT2).

timeslot_constraint(EVENT, TIMESLOT).

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): True

####################################################################################
Instance Template:

event(Event).

room(Room, Capacity).

timeslot(Day, Hour).

student(Student, Event).

feature_requirement(FEATURE, Event).

room_feature(FEATURE, Room).

precedence_constraint(EVENT1, EVENT2).

timeslot_constraint(EVENT, TIMESLOT).
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { assigned(Event, Timeslot, Room) : room(Room) } 1 :- event(Event).

room(Room) :- assigned(_, _, Room).

Intended semantics:
An assignment of events to timeslots and rooms.
    - Variables: timeslot, room


Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each event must be assigned to exactly one room, but it does not ensure that each event is assigned to exactly one timeslot and room combination."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
1 { assigned(Event, Timeslot, Room) : room(Room) } 1 :- event(Event).

room(Room) :- assigned(_, _, Room).

####################################################################################


Generator
1 { assigned(Event, Timeslot, Room) : room(Room) } 1 :- event(Event).

room(Room) :- assigned(_, _, Room).


Hard Constraints

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(student(Student1, Event1), Timeslot1, Room1), assigned(student(Student1, Event2), Timeslot2, Room2), Timeslot1 != Timeslot2, Day1 = Day2.

Intended semantics:- Students must not have overlapping events.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that students cannot have events in different rooms on the same day, but it does not ensure that students do not have overlapping events in the same room or different rooms on different days."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(student(Student1, Event1), Timeslot1, Room1), assigned(student(Student1, Event2), Timeslot2, Room2), Timeslot1 != Timeslot2, Day1 = Day2.

####################################################################################
- Students must not have overlapping events.
:- assigned(student(Student1, Event1), Timeslot1, Room1), assigned(student(Student1, Event2), Timeslot2, Room2), Timeslot1 != Timeslot2, Day1 = Day2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), student(Student, Event1), student(Student, Event2), room(Room, Capacity), #count{Student: student(Student, Event1), student(Student, Event2)} > Capacity.

Intended semantics:- The amount of students in a room at the same time should not exceed the capacity of the room
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), student(Student, Event1), student(Student, Event2), room(Room, Capacity), #count{Student: student(Student, Event1), student(Student, Event2)} > Capacity.

####################################################################################
- The amount of students in a room at the same time should not exceed the capacity of the room
 
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), student(Student, Event1), student(Student, Event2), room(Room, Capacity), #count{Student: student(Student, Event1), student(Student, Event2)} > Capacity.

================================================================================
Initial response with syntax error:
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), \+ feature_requirement(FEATURE, Event).

Error: <string>:1:66-67: error: syntax error, unexpected \

 Starting syntax repair attempts...
================================================================================
--------------------------------------------------------------------------------
Correction attempt 1:
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), \+ feature_requirement(FEATURE, Event).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:66-67: error: syntax error, unexpected \

--------------------------------------------------------------------------------
Correction attempt 2:
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), \+ feature_requirement(FEATURE, Event).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:66-67: error: syntax error, unexpected \

--------------------------------------------------------------------------------
Correction attempt 3:
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), \+ feature_requirement(FEATURE, Event).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:66-67: error: syntax error, unexpected \

================================================================================
Syntax errors remain after repair attempts. Cannot proceed to semantic checking.
Total syntax errors: 1

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), \+ feature_requirement(FEATURE, Event).

####################################################################################
- Rooms must have the required features for the event.
:- assigned(Event, Timeslot, Room), room_feature(FEATURE, Room), \+ feature_requirement(FEATURE, Event).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), Event1 != Event2.

Intended semantics:- There must be no more than one event in a room at a time.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), Event1 != Event2.

####################################################################################
- There must be no more than one event in a room at a time.
:- assigned(Event1, Timeslot, Room), assigned(Event2, Timeslot, Room), Event1 != Event2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Event, Timeslot, Room), not timeslot_constraint(Event, Timeslot).

Intended semantics:- Events may only be scheduled in designated timeslots.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Event, Timeslot, Room), not timeslot_constraint(Event, Timeslot).

####################################################################################
- Events may only be scheduled in designated timeslots.
:- assigned(Event, Timeslot, Room), not timeslot_constraint(Event, Timeslot).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(EVENT1, TIMESLOT1, _), assigned(EVENT2, TIMESLOT2, _), precedence_constraint(EVENT1, EVENT2), TIMESLOT1 > TIMESLOT2.

Intended semantics:- Some events must be scheduled in a specific order.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(EVENT1, TIMESLOT1, _), assigned(EVENT2, TIMESLOT2, _), precedence_constraint(EVENT1, EVENT2), TIMESLOT1 > TIMESLOT2.

####################################################################################
- Some events must be scheduled in a specific order.
:- assigned(EVENT1, TIMESLOT1, _), assigned(EVENT2, TIMESLOT2, _), precedence_constraint(EVENT1, EVENT2), TIMESLOT1 > TIMESLOT2.


Soft Constraints:

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("LastTimeslotConstraint",student(Student,Event),1) :- assigned(Event,Timeslot,Room), student(Student,Event), Timeslot = (Day,4), room(Room).

Intended semantics:- Students should not have events scheduled in the last timeslot of the day
 Penalty: 1 for each student with an event in the last timeslot of the day.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("LastTimeslotConstraint",student(Student,Event),1) :- assigned(Event,Timeslot,Room), student(Student,Event), Timeslot = (Day,4), room(Room).

####################################################################################
- Students should not have events scheduled in the last timeslot of the day
 Penalty: 1 for each student with an event in the last timeslot of the day.
penalty("LastTimeslotConstraint",student(Student,Event),1) :- assigned(Event,Timeslot,Room), student(Student,Event), Timeslot = (Day,4), room(Room).

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("ConsecutiveTimeslots",student(Student,Event),(N-1)*1) :-
student(Student,Event),
3 { timeslot(Day,Hour) : assigned(Event,Day,Hour) } 3,
N = { timeslot(Day,Hour) : assigned(Event,Day,Hour) }.

Intended semantics:- Students should not have events scheduled in three or more consecutive timeslots
 Penalty: 1 for each student with an event in three or more consecutive timeslots.

Semantic check result:
{
  "match": "false",
  "reason": "The code incorrectly calculates the number of consecutive timeslots and does not enforce the exact number of consecutive timeslots as required."
}

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
penalty("ConsecutiveTimeslots",student(Student,Event),(N-1)*1) :- 
    student(Student,Event),
    3 { timeslot(Day,Hour) : assigned(Event,Day,Hour) } 3,
    N = { timeslot(Day,Hour) : assigned(Event,Day,Hour) }.

REPAIRED RESPONSE:
penalty("ConsecutiveTimeslots",student(Student,Event),(N-1)*1) :-
student(Student,Event),
3 { timeslot(Day,Hour) : assigned(Event,Day,Hour) } 3,
N = { timeslot(Day,Hour) : assigned(Event,Day,Hour) }.

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): False

####################################################################################
- Students should not have events scheduled in three or more consecutive timeslots
 Penalty: 1 for each student with an event in three or more consecutive timeslots.
penalty("ConsecutiveTimeslots",student(Student,Event),(N-1)*1) :-
student(Student,Event),
3 { timeslot(Day,Hour) : assigned(Event,Day,Hour) } 3,
N = { timeslot(Day,Hour) : assigned(Event,Day,Hour) }.

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("SingleEventPerDay", single_event_per_day(Student, Day), 1) :-
#count{Event : assigned(Student, Timeslot, Room), timeslot(Day, _), student(Student)} = 1.

Intended semantics:- Students should not have exactly one event in a day
 Penalty: 1 for each time a student has exactly one event in a day.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces a penalty for students having exactly one event per day, whereas the intended semantics is to prevent students from having exactly one event in a day."
}

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
penalty("SingleEventPerDay", single_event_per_day(Student, Day), 1) :- 
    #count{Event : assigned(Student, Timeslot, Room), timeslot(Day, _), student(Student)} = 1.

REPAIRED RESPONSE:
penalty("SingleEventPerDay", single_event_per_day(Student, Day), 1) :-
#count{Event : assigned(Student, Timeslot, Room), timeslot(Day, _), student(Student)} = 1.

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): False

####################################################################################
- Students should not have exactly one event in a day
 Penalty: 1 for each time a student has exactly one event in a day.
 
penalty("SingleEventPerDay", single_event_per_day(Student, Day), 1) :-
#count{Event : assigned(Student, Timeslot, Room), timeslot(Day, _), student(Student)} = 1.

Full program saved to Results/experiment/post_enrollment_based_course_time_tabling_Qwen2.5-7B-Instruct_k=3_n=0_20260110_175159.lp
================================================================================
Send prompt for semantic check:
Generated ASP code:

nurse(Nurse).

day(Day).

shift_type(Shift_type, Duration).

nurse_requirement(Shift, Min, Max).

work_requirement(Nurse, Min, Max).

shift_requirement(Shift_type, Min, Max, Preferred).

Intended semantics:
Instance Variables:
- Nurses: A set of numbered nurses.
    - Variables: none
- Days: A set of numbered days.
    - Variables: none
- Shift_types: A set of shifts types that the nurses can be assigned to.
    - Variables: shift_type, duration
- nurse_requirements: Min and max number of nurses required for each shift type.
    - Variables: shift, min, max
- work_requirements: Min and max number of hours each nurse must work.
    - Variables: min, max
- shift_requirements: For each shift type, there is a minimum and maximum number of times a nurse must work this shift. There is also a preferred number of each shift type.
    - Variables: shift_type, min, max, preferred


Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
% Instance Template
nurse(Nurse).

day(Day).

shift_type(Shift_type, Duration).

nurse_requirement(Shift, Min, Max).

work_requirement(Nurse, Min, Max).

shift_requirement(Shift_type, Min, Max, Preferred).

REPAIRED RESPONSE:

nurse(Nurse).

day(Day).

shift_type(Shift_type, Duration).

nurse_requirement(Shift, Min, Max).

work_requirement(Nurse, Min, Max).

shift_requirement(Shift_type, Min, Max, Preferred).

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): True

####################################################################################
Instance Template:

nurse(Nurse).

day(Day).

shift_type(Shift_type, Duration).

nurse_requirement(Shift, Min, Max).

work_requirement(Nurse, Min, Max).

shift_requirement(Shift_type, Min, Max, Preferred).
================================================================================
Initial response with syntax error:
1 { assigned(Nurse, Shift_type, DDay) : day/DDay, shift_type/Shift_type } 1 :- nurse(Nurse).

Error: <string>:1:49-50: error: syntax error, unexpected ","

 Starting syntax repair attempts...
================================================================================
--------------------------------------------------------------------------------
Correction attempt 1:
1 { assigned(Nurse, Shift_type, DDay) : DDay, shift_type(Shift_type) } 1 :- nurse(Nurse).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:45-46: error: syntax error, unexpected ","

--------------------------------------------------------------------------------
Correction attempt 2:
1 { assigned(Nurse, Shift_type, DDay) : DDay, shift_type(Shift_type) } 1 :- nurse(Nurse).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:45-46: error: syntax error, unexpected ","

--------------------------------------------------------------------------------
Correction attempt 3:
1 { assigned(Nurse, Shift_type, DDay) : DDay, shift_type(Shift_type) } 1 :- nurse(Nurse).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:45-46: error: syntax error, unexpected ","

================================================================================
Syntax errors remain after repair attempts. Cannot proceed to semantic checking.
Total syntax errors: 1

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED , BUT UNSUCCESSFULLY
INITIAL RESPONSE:
1 { assigned(Nurse, Shift_type, DDay) : day/DDay, shift_type/Shift_type } 1 :- nurse(Nurse).

REPAIRED RESPONSE:
1 { assigned(Nurse, Shift_type, DDay) : DDay, shift_type(Shift_type) } 1 :- nurse(Nurse).

Total syntax errors remaining after repair attempts: 1

Semantics correct (according to LLM): False

####################################################################################


Generator
1 { assigned(Nurse, Shift_type, DDay) : DDay, shift_type(Shift_type) } 1 :- nurse(Nurse).


Hard Constraints

================================================================================
Initial response with syntax error:
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} < nurse_requirement(Shift_type, Min).

Error: <string>:1:82-83: error: syntax error, unexpected ","

 Starting syntax repair attempts...
================================================================================
--------------------------------------------------------------------------------
Correction attempt 1:
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} < nurse_requirement(Shift_type, Min).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:82-83: error: syntax error, unexpected ","

--------------------------------------------------------------------------------
Correction attempt 2:
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} < nurse_requirement(Shift_type, Min).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:82-83: error: syntax error, unexpected ","

--------------------------------------------------------------------------------
Correction attempt 3:
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} < nurse_requirement(Shift_type, Min).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:82-83: error: syntax error, unexpected ","

================================================================================
Initial response with syntax error:
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} > nurse_requirement(Shift_type, Max).

Error: <string>:1:82-83: error: syntax error, unexpected ","

 Starting syntax repair attempts...
================================================================================
--------------------------------------------------------------------------------
Correction attempt 1:
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} > nurse_requirement(Shift_type, Max).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:82-83: error: syntax error, unexpected ","

--------------------------------------------------------------------------------
Correction attempt 2:
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} > nurse_requirement(Shift_type, Max).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:82-83: error: syntax error, unexpected ","

--------------------------------------------------------------------------------
Correction attempt 3:
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} > nurse_requirement(Shift_type, Max).

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:82-83: error: syntax error, unexpected ","

================================================================================
Syntax errors remain after repair attempts. Cannot proceed to semantic checking.
Total syntax errors: 2

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} < nurse_requirement(Shift_type, Min).
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} > nurse_requirement(Shift_type, Max).

####################################################################################
- Every day, the number of nurses assigned to each shift must be between the specified minimum and maximum
 
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} < nurse_requirement(Shift_type, Min).
:- shift_type(Shift_type), #count{Nurse : assigned(Nurse, Shift_type, DDay), DDay, shift_type(Shift_type)} > nurse_requirement(Shift_type, Max).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} < work_requirement(Nurse, Min, Max), nurse(Nurse).
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} > work_requirement(Nurse, Max, Max), nurse(Nurse).

Intended semantics:- Each nurse must work at least the specified minimum and at most the specified maximum number of hours
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each nurse works either less than the minimum or more than the maximum number of hours, which is the opposite of the intended semantics."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} < work_requirement(Nurse, Min, Max), nurse(Nurse).
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} > work_requirement(Nurse, Max, Max), nurse(Nurse).

####################################################################################
- Each nurse must work at least the specified minimum and at most the specified maximum number of hours
 
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} < work_requirement(Nurse, Min, Max), nurse(Nurse).
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration)} > work_requirement(Nurse, Max, Max), nurse(Nurse).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #count{DDay : assigned(Nurse, vacation, DDay)} != 30.

Intended semantics:- Nurses must have exactly 30 days of vacation.
 

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each nurse must have exactly 30 days of vacation, not exactly 31 days."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- nurse(Nurse), #count{DDay : assigned(Nurse, vacation, DDay)} != 30.

####################################################################################
- Nurses must have exactly 30 days of vacation.
 
:- nurse(Nurse), #count{DDay : assigned(Nurse, vacation, DDay)} != 30.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Nurse, night, DDay), assigned(Nurse, morning, DDay+1).
:- assigned(Nurse, night, DDay), assigned(Nurse, afternoon, DDay+1).

Intended semantics:- If a nurse works a night shift, they can not work a morning or afternoon shift the next day

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Nurse, night, DDay), assigned(Nurse, morning, DDay+1).
:- assigned(Nurse, night, DDay), assigned(Nurse, afternoon, DDay+1).

####################################################################################
- If a nurse works a night shift, they can not work a morning or afternoon shift the next day
:- assigned(Nurse, night, DDay), assigned(Nurse, morning, DDay+1).
:- assigned(Nurse, night, DDay), assigned(Nurse, afternoon, DDay+1).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- assigned(Nurse, morning, DDay), assigned(Nurse, afternoon, DDay+1).

Intended semantics:- If a nurse works an afternoon shift, they cannot work a morning shift the next day.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- assigned(Nurse, morning, DDay), assigned(Nurse, afternoon, DDay+1).

####################################################################################
- If a nurse works an afternoon shift, they cannot work a morning shift the next day.
:- assigned(Nurse, morning, DDay), assigned(Nurse, afternoon, DDay+1).

================================================================================
Initial response with syntax error:
1 { assigned(Nurse, rest, DDay) : DDay, day(DDay) } 2 :-
nurse(Nurse),
#count{DDay : day(DDay), assigned(Nurse, rest, DDay), DDay > 0, mod(DDay - 1, 14) = 0} < 2.

Error: <string>:1:39-40: error: syntax error, unexpected ","

 Starting syntax repair attempts...
================================================================================
--------------------------------------------------------------------------------
Correction attempt 1:
1 { assigned(Nurse, rest, DDay) : DDay, day(DDay) } 2 :-
nurse(Nurse),
#count{DDay : day(DDay), assigned(Nurse, rest, DDay), DDay > 0, (DDay - 1) mod 14 = 0} < 2.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:39-40: error: syntax error, unexpected ","

--------------------------------------------------------------------------------
Correction attempt 2:
1 { assigned(Nurse, rest, DDay) : DDay, day(DDay) } 2 :-
nurse(Nurse),
#count{DDay : day(DDay), assigned(Nurse, rest, DDay), DDay > 0, (DDay - 1) mod 14 = 0} < 2.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:39-40: error: syntax error, unexpected ","

--------------------------------------------------------------------------------
Correction attempt 3:
1 { assigned(Nurse, rest, DDay) : DDay, day(DDay) } 2 :-
nurse(Nurse),
#count{DDay : day(DDay), assigned(Nurse, rest, DDay), DDay > 0, (DDay - 1) mod 14 = 0} < 2.

--------------------------------------------------------------------------------
Syntax error still present: <string>:1:39-40: error: syntax error, unexpected ","

================================================================================
Syntax errors remain after repair attempts. Cannot proceed to semantic checking.
Total syntax errors: 1

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED , BUT UNSUCCESSFULLY
INITIAL RESPONSE:
1 { assigned(Nurse, rest, DDay) : DDay, day(DDay) } 2 :-
    nurse(Nurse),
    #count{DDay : day(DDay), assigned(Nurse, rest, DDay), DDay > 0, mod(DDay - 1, 14) = 0} < 2.

REPAIRED RESPONSE:
1 { assigned(Nurse, rest, DDay) : DDay, day(DDay) } 2 :-
nurse(Nurse),
#count{DDay : day(DDay), assigned(Nurse, rest, DDay), DDay > 0, (DDay - 1) mod 14 = 0} < 2.

Total syntax errors remaining after repair attempts: 1

Semantics correct (according to LLM): False

####################################################################################
- Each nurse has at least two ordinary rest days for every window of fourteen days
 
1 { assigned(Nurse, rest, DDay) : DDay, day(DDay) } 2 :-
nurse(Nurse),
#count{DDay : day(DDay), assigned(Nurse, rest, DDay), DDay > 0, (DDay - 1) mod 14 = 0} < 2.

================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { assigned(Nurse, special_rest, DDay) : assigned(Nurse, night, DDay-1), assigned(Nurse, night, DDay) } 1 :- nurse(Nurse).

Intended semantics:- Nurses working on two consecutive nights deserve one special_rest day in addition to the ordinary rest days

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that if a nurse works two consecutive nights, they must get a special_rest day, but it does not ensure that this special_rest day is in addition to the ordinary rest day. It only guarantees one special_rest day for two consecutive nights, which might not align with the intention of having an extra rest day."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
1 { assigned(Nurse, special_rest, DDay) : assigned(Nurse, night, DDay-1), assigned(Nurse, night, DDay) } 1 :- nurse(Nurse).

####################################################################################
- Nurses working on two consecutive nights deserve one special_rest day in addition to the ordinary rest days
1 { assigned(Nurse, special_rest, DDay) : assigned(Nurse, night, DDay-1), assigned(Nurse, night, DDay) } 1 :- nurse(Nurse).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration) } < Min, work_requirement(Nurse, Min, Max).
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration) } > Max, work_requirement(Nurse, Min, Max).

Intended semantics:- The total number of hours worked by each nurse must be between the specified minimum and maximum
 

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration) } < Min, work_requirement(Nurse, Min, Max).
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration) } > Max, work_requirement(Nurse, Min, Max).

####################################################################################
- The total number of hours worked by each nurse must be between the specified minimum and maximum
 
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration) } < Min, work_requirement(Nurse, Min, Max).
:- nurse(Nurse), #sum{Hours, DDay: assigned(Nurse, Shift_type, DDay), shift_type(Shift_type, Duration) } > Max, work_requirement(Nurse, Min, Max).


Soft Constraints:

================================================================================
Send prompt for semantic check:
Generated ASP code:
shift_requirement(Shift_type, Min, Max, Preferred) :- shift_type(Shift_type).
penalty("ShiftPreference", shift_requirement(Shift_type, Min, Max, Preferred), (Preferred - Count)*(1 + (Count - Preferred)*1)) :-
#count{DDay, Nurse : assigned(Nurse, Shift_type, DDay)} = Count,
shift_type(Shift_type),
nurse(Nurse).

Intended semantics:- Nurses work the preferred number of shifts for each shift type
 Penalty: 1 for each shift below the preferred number
 Penalty: 1 for each shift above the preferred number
 

Semantic check result:
{
  "match": "false",
  "reason": "The generated ASP code incorrectly calculates the penalty. It includes an extra term (1 + (Count - Preferred)*1) which inflates the penalty calculation."
}

================================================================================
####################################################################################
PROGRAM PART WAS REPAIRED SUCCESSFULLY
INITIAL RESPONSE:
shift_requirement(Shift_type, Min, Max, Preferred) :- shift_type(Shift_type).
penalty("ShiftPreference", shift_requirement(Shift_type, Min, Max, Preferred), (Preferred - Count)*(1 + (Count - Preferred)*1)) :-
    #count{DDay, Nurse : assigned(Nurse, Shift_type, DDay)} = Count,
    shift_type(Shift_type),
    nurse(Nurse).

REPAIRED RESPONSE:
shift_requirement(Shift_type, Min, Max, Preferred) :- shift_type(Shift_type).
penalty("ShiftPreference", shift_requirement(Shift_type, Min, Max, Preferred), (Preferred - Count)*(1 + (Count - Preferred)*1)) :-
#count{DDay, Nurse : assigned(Nurse, Shift_type, DDay)} = Count,
shift_type(Shift_type),
nurse(Nurse).

Total syntax errors remaining after repair attempts: 0

Semantics correct (according to LLM): False

####################################################################################
- Nurses work the preferred number of shifts for each shift type
 Penalty: 1 for each shift below the preferred number
 Penalty: 1 for each shift above the preferred number
 
shift_requirement(Shift_type, Min, Max, Preferred) :- shift_type(Shift_type).
penalty("ShiftPreference", shift_requirement(Shift_type, Min, Max, Preferred), (Preferred - Count)*(1 + (Count - Preferred)*1)) :-
#count{DDay, Nurse : assigned(Nurse, Shift_type, DDay)} = Count,
shift_type(Shift_type),
nurse(Nurse).

Full program saved to Results/experiment/nurse_scheduling_Qwen2.5-7B-Instruct_k=3_n=0_20260110_175842.lp
================================================================================
Send prompt for semantic check:
Generated ASP code:
team(Team).

venue(Venue).

availability(Venue, Date).

Intended semantics:
- Teams: a set of teams that plays in the league
    - Variables: team
- Venues: a set of venues that is available
    - Variables: venues
- Availabilities: A set of dates at which a given venue is available 
    - Variables: venues, dates
    

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
team(Team).

venue(Venue).

availability(Venue, Date).

####################################################################################
Instance Template:
team(Team).

venue(Venue).

availability(Venue, Date).
================================================================================
Send prompt for semantic check:
Generated ASP code:
1 { game(Team1, Team2, Venue, GameDay) : team(Team1), team(Team2), venue(Venue), gameday(GameDay) } 1 :- team(Team1), team(Team2).

Intended semantics:
An assignment of games to pairs of teams, venues and game days.
    - Variables: team1, team2, venue, gameday

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
1 { game(Team1, Team2, Venue, GameDay) : team(Team1), team(Team2), venue(Venue), gameday(GameDay) } 1 :- team(Team1), team(Team2).

####################################################################################


Generator
1 { game(Team1, Team2, Venue, GameDay) : team(Team1), team(Team2), venue(Venue), gameday(GameDay) } 1 :- team(Team1), team(Team2).


Hard Constraints

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- game(Team1, Team2, Venue, GameDay1), game(Team1, Team2, _, GameDay2), GameDay1 != GameDay2, team(Team1), team(Team2).

Intended semantics:- No team plays each other more than once at different gamedays.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- game(Team1, Team2, Venue, GameDay1), game(Team1, Team2, _, GameDay2), GameDay1 != GameDay2, team(Team1), team(Team2).

####################################################################################
- No team plays each other more than once at different gamedays.
:- game(Team1, Team2, Venue, GameDay1), game(Team1, Team2, _, GameDay2), GameDay1 != GameDay2, team(Team1), team(Team2).

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- team(Team1), team(Team2), Team1 != Team2, 1 { game(Team1, Team2, _, _) : gameday(_) } 0.

Intended semantics:- Every team plays every other team at least once.

Semantic check result:
{
  "match": "false",
  "reason": "The code enforces that each pair of teams plays at least once, but it does not specify that every team must play every other team. It only ensures that each pair of distinct teams has at least one game together."
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- team(Team1), team(Team2), Team1 != Team2, 1 { game(Team1, Team2, _, _) : gameday(_) } 0.

####################################################################################
- Every team plays every other team at least once.
:- team(Team1), team(Team2), Team1 != Team2, 1 { game(Team1, Team2, _, _) : gameday(_) } 0.

================================================================================
Send prompt for semantic check:
Generated ASP code:
:- game(Team, Team, _, _).

Intended semantics:- No team plays itself.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
:- game(Team, Team, _, _).

####################################################################################
- No team plays itself.
:- game(Team, Team, _, _).


Soft Constraints:

================================================================================
Send prompt for semantic check:
Generated ASP code:
penalty("VenueStability",game(Team1,Team2,Venue1,GameDay),1) :- game(Team1,Team2,Venue1,GameDay), game(Team1,Team2,Venue2,GameDay), Venue1 != Venue2.

Intended semantics:- Teams should play at the same venue as little as possible.

Semantic check result:
{
  "match": "true",
  "reason": ""
}

================================================================================
####################################################################################
PROGRAM PART DID NOT NEED REPAIR!
RESPONSE:
penalty("VenueStability",game(Team1,Team2,Venue1,GameDay),1) :- game(Team1,Team2,Venue1,GameDay), game(Team1,Team2,Venue2,GameDay), Venue1 != Venue2.

####################################################################################
- Teams should play at the same venue as little as possible.
penalty("VenueStability",game(Team1,Team2,Venue1,GameDay),1) :- game(Team1,Team2,Venue1,GameDay), game(Team1,Team2,Venue2,GameDay), Venue1 != Venue2.

Full program saved to Results/experiment/sports scheduling_Qwen2.5-7B-Instruct_k=3_n=0_20260110_180621.lp
