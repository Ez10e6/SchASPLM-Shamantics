{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not detected. Using MPS/CPU configuration (Quantization disabled).\n",
      "Using Huggingface Hub Token from .env\n",
      "--- Loading fine-tuned model from direct path: ./local_models/ft_qwen_mlx ---\n",
      "loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4788250ba74ee7807983f068bda7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './local_models/ft_qwen_mlx' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################\n",
      "PROGRAM PART DID NOT NEED REPAIR!\n",
      "RESPONSE:\n",
      "% Instance Template\n",
      "exam(E, Duration, Large).\n",
      "\n",
      "student(S, Exam).\n",
      "\n",
      "period(P, Date, Time, Duration, Late, Penalty).\n",
      "\n",
      "room(R, Capacity, Penalty).\n",
      "\n",
      "order_constrain(E1, E2).\n",
      "\n",
      "same_time_constrain(E1, E2).\n",
      "\n",
      "different_time_constrain(E1, E2).\n",
      "\n",
      "own_room_constrain(E).\n",
      "\n",
      "Total statement blocks: 16\n",
      "\n",
      "####################################################################################\n",
      "Instance Template:\n",
      "% Instance Template\n",
      "exam(E, Duration, Large).\n",
      "\n",
      "student(S, Exam).\n",
      "\n",
      "period(P, Date, Time, Duration, Late, Penalty).\n",
      "\n",
      "room(R, Capacity, Penalty).\n",
      "\n",
      "order_constrain(E1, E2).\n",
      "\n",
      "same_time_constrain(E1, E2).\n",
      "\n",
      "different_time_constrain(E1, E2).\n",
      "\n",
      "own_room_constrain(E).\n"
     ]
    }
   ],
   "source": [
    "# We illustrate the generation of a scheduling problem using a local model  (using chain-of-thought and few-shot prompting)\n",
    "# Notice you have to have your api-key under .env\n",
    "\n",
    "# Doing all the imports in this cell because each local GPU run needs a kernel restart. \n",
    "# Without a kernel restart, CPU is used instead of GPU after loading a local model once.\n",
    "from LLM import bots\n",
    "from ASP_Scheduler.problem_descriptions import all_problems\n",
    "from ASP_Scheduler import scheduler\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from utils import logger\n",
    "\n",
    "###########################################################\n",
    "#                        SETTINGS                         #\n",
    "###########################################################\n",
    "\n",
    "# GENERAL SETTINGS\n",
    "RUN_LOCAL_MODEL = True         # Set to True to run a local model, False to run a remote model via OpenAI API\n",
    "PRINT = True                    # Set to True to print intermediate outputs\n",
    "PROGRAM_FOLDER = \"./results\"     # Folder to save programs (set to None to disable saving)\n",
    "METRICS_LOG_FILE = 'metrics/metrics.csv'\n",
    "\n",
    "# REMOTE SETTINGS\n",
    "REMOTE_PIPE = None # None defaults to meta-llama/Meta-Llama-3-8B-Instruct\n",
    "#REMOTE_PIPE = 'deepseek' # 'deepseek' model on OpenAI API\n",
    "\n",
    "# LOCAL SETTINGS\n",
    "#CHECKPOINT, CHECKPOINT_SHORT_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\", \"Llama-3-8B-Instruct\" \n",
    "#CHECKPOINT, CHECKPOINT_SHORT_NAME = \"Qwen/Qwen2.5-7B-Instruct\", \"Qwen2.5-7B-Instruct\"\n",
    "CHECKPOINT, CHECKPOINT_SHORT_NAME = \"./local_models/ft_qwen_mlx\", \"Qwen2.5-7B-FT\"  # Local finetuned Qwen-2.5-7B with MLX-LoRA\n",
    "\n",
    "# DYNAMIC HARDWARE CONFIGURATION\n",
    "if torch.cuda.is_available():\n",
    "    # Settings for NVIDIA GPUs\n",
    "    print(\"CUDA detected. Using NVIDIA configuration.\")\n",
    "    os.environ[\"BNB_CUDA_VERSION\"] = \"123\"  # Force bnb version for Windows/Cuda if needed\n",
    "    QUANTIZATION_CONFIG = '4bit'          # '4bit', '8bit' supported on CUDA\n",
    "else:\n",
    "    # Settings for Mac (MPS) or CPU\n",
    "    print(\"CUDA not detected. Using MPS/CPU configuration (Quantization disabled).\")\n",
    "    QUANTIZATION_CONFIG = None            # bitsandbytes quantization is not supported on MPS yet\n",
    "\n",
    "# SAMPLING / REPRODUCIBILITY SETTINGS\n",
    "# - Set SEED = -1 to disable fixed seeding (non-deterministic runs). Set to an integer for reproducible runs.\n",
    "SEED = -1\n",
    "MAX_NEW_TOKENS = 512 #Max tokens for response. Should be in balance with the model's context size\n",
    "\n",
    "TEMPERATURE = 0.1  # Less deterministic\n",
    "TOP_P = 0.9        # Less deterministic\n",
    "#TEMPERATURE = 0.01 # More deterministic (same as local settings of original experiments)\n",
    "#TOP_P = 0          # More deterministic (same as local settings of original experiments)\n",
    "\n",
    "# PROBLEM SETTINGS\n",
    "# PROBLEM_NAMES = ['sports scheduling']\n",
    "# PROBLEM_NAMES = ['nurse_scheduling']\n",
    "# PROBLEM_NAMES = ['nurse_scheduling', 'sports scheduling']\n",
    "# PROBLEM_NAMES = ['post_enrollment_based_course_time_tabling', 'examination_timetabling']\n",
    "PROBLEM_NAMES = list(all_problems.keys())  # To run the program for ALL available problem names\n",
    "MAX_REPAIRS = 5  # Maximum number of repair attempts per statement block\n",
    "RUNS_PER_PROBLEM = 1  # Number of runs per problem for averaging results\n",
    "\n",
    "if RUN_LOCAL_MODEL:\n",
    "    # To work locally, we need to manually load the pipeline \n",
    "    PIPE = bots.load_pipe(model_checkpoint=CHECKPOINT, local_dir=\"./local_models\", quantization_config=QUANTIZATION_CONFIG, save=True)  \n",
    "else:\n",
    "    # For remote models, we set pipe to a string with the model name\n",
    "    PIPE = REMOTE_PIPE\n",
    "\n",
    "\n",
    "# Run the LLM scheduler per problem\n",
    "for problem_name in PROBLEM_NAMES:\n",
    "    for run_id in range(RUNS_PER_PROBLEM):        \n",
    "        # Initialize the metrics logger\n",
    "        # Build a model identifier string for the logfile (include LOCAL/REMOTE)\n",
    "        model_id = (f\"{CHECKPOINT} (LOCAL, QUANTIZATION: {QUANTIZATION_CONFIG})\" if RUN_LOCAL_MODEL else (f\"{REMOTE_PIPE} (REMOTE)\" if REMOTE_PIPE is not None else \"Meta-Llama-3-8B-Instruct\"))\n",
    "        logger.init_logger(filename=METRICS_LOG_FILE,\n",
    "                           problem_ID=problem_name,\n",
    "                           max_fix_attempts=MAX_REPAIRS,\n",
    "                           model=model_id,\n",
    "                           temperature=TEMPERATURE,\n",
    "                           top_p=TOP_P,\n",
    "                           seed=SEED)\n",
    "\n",
    "        full_program = scheduler.full_ASP_program(\n",
    "            all_problems[problem_name],    # Input problem specifications for examination timetabling\n",
    "            pipe=PIPE,                     # Input the PIPEline object for the LLM\n",
    "            printer=PRINT,                 # Set to True to print intermediate outputs\n",
    "            k=MAX_REPAIRS,                 # Max repairs\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            seed=(None if SEED == -1 else SEED),\n",
    "            max_new_tokens=MAX_NEW_TOKENS)\n",
    "                            \n",
    "\n",
    "        if PROGRAM_FOLDER is not None:\n",
    "            # Save the full program to a file\n",
    "            os.makedirs(PROGRAM_FOLDER, exist_ok=True)\n",
    "            timestamp = logger.time_stamp()\n",
    "            if RUN_LOCAL_MODEL:\n",
    "                model_string = CHECKPOINT_SHORT_NAME\n",
    "                if QUANTIZATION_CONFIG is not None:\n",
    "                    # Append quantization info like \" (quant 4bit)\" or \" (quant 8bit)\"\n",
    "                    model_string = f\"{model_string} (quant {QUANTIZATION_CONFIG})\"\n",
    "            else:\n",
    "                model_string = REMOTE_PIPE if REMOTE_PIPE is not None else \"Meta-Llama-3-8B-Instruct\"\n",
    "            max_repairs_string = f\"_k={MAX_REPAIRS}\" if MAX_REPAIRS is not None else \"\"\n",
    "            program_filename = os.path.join(PROGRAM_FOLDER, f\"{problem_name}_{model_string}{max_repairs_string}_{timestamp}.lp\")\n",
    "            with open(program_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(full_program)\n",
    "            if PRINT:\n",
    "                print(f\"Full program saved to {program_filename}\")\n",
    "        else:\n",
    "            # Print the full program as it is returned by the scheduler\n",
    "            print('----------------------------FULL PROGRAM----------------------------')\n",
    "            print(full_program)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schasplm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
