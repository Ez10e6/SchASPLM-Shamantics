{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not detected. Using MPS/CPU configuration (Quantization disabled).\n",
      "Using Huggingface Hub Token from .env\n",
      "Using dtype: torch.bfloat16\n",
      "loading local model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ede833ef0d04ddcb30d83c38a842d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Huggingface Hub Token from .env\n",
      "Using dtype: torch.bfloat16\n",
      "loading local model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './local_models/Qwen/Qwen2.5-7B-Instruct' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aad4bb0503640e6b4430950ce40b94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to create generator on mps. Fallback to CPU. Error: The following `model_kwargs` are not used by the model: ['generator'] (note: typos in the generate arguments will also show up in this list)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['generator'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/LLM/bots.py:157\u001b[39m, in \u001b[36mLocal_Bot.infer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m     gen = torch.Generator(device=device).manual_seed(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.seed))\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpipe_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:293\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[33;03mComplete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[32m    246\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m      ids of the generated text.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/pipelines/base.py:1278\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/pipelines/base.py:1285\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1284\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/pipelines/base.py:1177\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1176\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1177\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1178\u001b[39m model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:397\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    395\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/generation/utils.py:2508\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2506\u001b[39m     decoding_method = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), GENERATION_MODES_MAPPING[generation_mode])\n\u001b[32m-> \u001b[39m\u001b[32m2508\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2509\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_generation_mode(generation_mode, generation_config, generation_mode_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/generation/utils.py:1655\u001b[39m, in \u001b[36mGenerationMixin._validate_model_kwargs\u001b[39m\u001b[34m(self, model_kwargs)\u001b[39m\n\u001b[32m   1654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[32m-> \u001b[39m\u001b[32m1655\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1656\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (note: typos in the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1657\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m generate arguments will also show up in this list)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1658\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The following `model_kwargs` are not used by the model: ['generator'] (note: typos in the generate arguments will also show up in this list)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     84\u001b[39m semantics_model_id = (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHECKPOINT_SEMANTICS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (LOCAL, QUANTIZATION: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQUANTIZATION_CONFIG\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RUN_LOCAL_MODEL \u001b[38;5;28;01melse\u001b[39;00m (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREMOTE_PIPE_SEMANTICS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (REMOTE)\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m REMOTE_PIPE_SEMANTICS \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMeta-Llama-3-8B-Instruct\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     85\u001b[39m logger.init_logger(filename=METRICS_LOG_FILE,\n\u001b[32m     86\u001b[39m                    problem_ID=problem_name,\n\u001b[32m     87\u001b[39m                    max_fix_attempts=MAX_SYNTAX_REPAIRS,\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m                    top_p=TOP_P,\n\u001b[32m     92\u001b[39m                    seed=SEED)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m full_program = \u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfull_ASP_program\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_problems\u001b[49m\u001b[43m[\u001b[49m\u001b[43mproblem_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Input problem specifications for examination timetabling\u001b[39;49;00m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Input the PIPEline object for the LLM\u001b[39;49;00m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43msemantic_validation_pipe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEMANTICS_PIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Input the PIPEline object for the semantics validation LLM\u001b[39;49;00m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprinter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPRINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Set to True to print intermediate outputs\u001b[39;49;00m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_SYNTAX_REPAIRS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Max repairs\u001b[39;49;00m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_SEMANTIC_REPAIRS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Max repairs\u001b[39;49;00m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_NEW_TOKENS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m PROGRAM_FOLDER \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# Save the full program to a file\u001b[39;00m\n\u001b[32m    109\u001b[39m     os.makedirs(PROGRAM_FOLDER, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/ASP_Scheduler/scheduler.py:695\u001b[39m, in \u001b[36mfull_ASP_program\u001b[39m\u001b[34m(problem, printer, pipe, semantic_validation_pipe, k, n, temperature, top_p, seed, max_new_tokens)\u001b[39m\n\u001b[32m    692\u001b[39m problem_description, instance_description, generator_description, hard_constraint_descriptions, soft_constraint_descriptions = extract_descriptions(problem)\n\u001b[32m    694\u001b[39m \u001b[38;5;66;03m# Generate an instance template based on instance description (we still provide problem description for the repair prompt)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m instance_template = \u001b[43mget_partial_program\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_prompt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msystem_prompts/instance.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstance_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_prompt_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproblem_description\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_description\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43msemantic_validation_pipe\u001b[49m\u001b[43m=\u001b[49m\u001b[43msemantic_validation_pipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprinter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprinter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mInstance Template:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m + instance_template) \u001b[38;5;28;01mif\u001b[39;00m printer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[38;5;66;03m# Generate a generator based on generator description and instance template (we still provide problem description for the repair prompt)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/ASP_Scheduler/scheduler.py:542\u001b[39m, in \u001b[36mget_partial_program\u001b[39m\u001b[34m(system_prompt_path, prompt, system_prompt_variables, pipe, semantic_validation_pipe, k, n, printer, temperature, top_p, seed, max_new_tokens)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# Load the bot and get the response\u001b[39;00m\n\u001b[32m    541\u001b[39m asp_generator_bot = bots.load_bot(system_prompt, pipe, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p, seed=seed)\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m generated_code = \u001b[43masp_generator_bot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    543\u001b[39m generated_code = utils.remove_backtick_lines(generated_code)    \n\u001b[32m    544\u001b[39m initial_generated_code = generated_code  \u001b[38;5;66;03m# Keep a copy of the initial response for printing later\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/LLM/bots.py:119\u001b[39m, in \u001b[36mLocal_Bot.prompt\u001b[39m\u001b[34m(self, content)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, content):\n\u001b[32m    118\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_to_prompt(\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, content)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_to_prompt(\u001b[33m'\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m'\u001b[39m, response)\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Projects/Ass3_LLM_ASP/SchASPLM-Shamantics/LLM/bots.py:161\u001b[39m, in \u001b[36mLocal_Bot.infer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    159\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Failed to create generator on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Fallback to CPU. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    160\u001b[39m         gen = torch.Generator(device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m).manual_seed(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.seed))\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpipe_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    163\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.pipe(\u001b[38;5;28mself\u001b[39m.messages, do_sample=do_sample, **pipe_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:293\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, **kwargs):\n\u001b[32m    244\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[33;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[32m    246\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/pipelines/base.py:1278\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1271\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1272\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1275\u001b[39m         )\n\u001b[32m   1276\u001b[39m     )\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/pipelines/base.py:1285\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1283\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1284\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/pipelines/base.py:1177\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1175\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1176\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1177\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1178\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:397\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    395\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    400\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/generation/utils.py:2508\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2504\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2505\u001b[39m     \u001b[38;5;66;03m# type() required to access the unbound class-level method\u001b[39;00m\n\u001b[32m   2506\u001b[39m     decoding_method = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), GENERATION_MODES_MAPPING[generation_mode])\n\u001b[32m-> \u001b[39m\u001b[32m2508\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2509\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_generation_mode(generation_mode, generation_config, generation_mode_kwargs)\n\u001b[32m   2511\u001b[39m \u001b[38;5;66;03m# Deprecation-related step: set Hub repo for deprecated strategies.\u001b[39;00m\n\u001b[32m   2512\u001b[39m \u001b[38;5;66;03m# NOTE: This must come after initializing generation_config, since we need it to determine if this is a deprecated mode.\u001b[39;00m\n\u001b[32m   2513\u001b[39m \u001b[38;5;66;03m# It must also be before any preparation steps, since Hub repos expect to be loaded before preparation steps.\u001b[39;00m\n\u001b[32m   2514\u001b[39m \u001b[38;5;66;03m# TODO joao, manuel: remove this in v4.62.0\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/schasplm/lib/python3.11/site-packages/transformers/generation/utils.py:1655\u001b[39m, in \u001b[36mGenerationMixin._validate_model_kwargs\u001b[39m\u001b[34m(self, model_kwargs)\u001b[39m\n\u001b[32m   1652\u001b[39m         unused_model_args.append(key)\n\u001b[32m   1654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[32m-> \u001b[39m\u001b[32m1655\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1656\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (note: typos in the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1657\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m generate arguments will also show up in this list)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1658\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The following `model_kwargs` are not used by the model: ['generator'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# We illustrate the generation of a scheduling problem using a local model  (using chain-of-thought and few-shot prompting)\n",
    "# Notice you have to have your api-key under .env\n",
    "\n",
    "# Doing all the imports in this cell because each local GPU run needs a kernel restart. \n",
    "# Without a kernel restart, CPU is used instead of GPU after loading a local model once.\n",
    "from LLM import bots\n",
    "from ASP_Scheduler.problem_descriptions import all_problems\n",
    "from ASP_Scheduler import scheduler\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from utils import logger\n",
    "\n",
    "###########################################################\n",
    "#                        SETTINGS                         #\n",
    "###########################################################\n",
    "\n",
    "# GENERAL SETTINGS\n",
    "RUN_LOCAL_MODEL = True         # Set to True to run a local model, False to run a remote model via OpenAI API\n",
    "PRINT = True                    # Set to True to print intermediate outputs\n",
    "PROGRAM_FOLDER = 'Results/temp'     # Folder to save programs (set to None to disable saving)\n",
    "METRICS_LOG_FILE = 'metrics/metrics.csv'\n",
    "\n",
    "# REMOTE SETTINGS\n",
    "# REMOTE_PIPE = None # None defaults to meta-llama/Meta-Llama-3-8B-Instruct\n",
    "# REMOTE_PIPE = 'deepseek' # 'deepseek' model on OpenAI API\n",
    "# REMOTE_PIPE_SEMANTICS = None # None defaults to meta-llama/Meta-Llama-3-8B-Instruct\n",
    "# REMOTE_PIPE_SEMANTICS = 'deepseek' # 'deepseek' model on OpenAI API\n",
    "\n",
    "# LOCAL SETTINGS\n",
    "# CHECKPOINT, CHECKPOINT_SHORT_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\", \"Llama-3-8B-Instruct\" \n",
    "# CHECKPOINT, CHECKPOINT_SHORT_NAME = \"Qwen/Qwen2.5-7B-Instruct\", \"Qwen2.5-7B-Instruct\"\n",
    "# CHECKPOINT, CHECKPOINT_SHORT_NAME = \"ft_llama_mlx\", \"ft_llama_mlx\"\n",
    "CHECKPOINT, CHECKPOINT_SHORT_NAME = \"ft_qwen_mlx\", \"ft_qwen_mlx\"\n",
    "# CHECKPOINT_SEMANTICS, CHECKPOINT_SHORT_NAME_SEMANTICS = \"meta-llama/Meta-Llama-3-8B-Instruct\", \"Llama-3-8B-Instruct\" \n",
    "CHECKPOINT_SEMANTICS, CHECKPOINT_SHORT_NAME_SEMANTICS = \"Qwen/Qwen2.5-7B-Instruct\", \"Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# DYNAMIC HARDWARE CONFIGURATION\n",
    "if torch.cuda.is_available():\n",
    "    # Settings for NVIDIA GPUs\n",
    "    print(\"CUDA detected. Using NVIDIA configuration.\")\n",
    "    os.environ[\"BNB_CUDA_VERSION\"] = \"123\"  # Force bnb version for Windows/Cuda if needed\n",
    "    QUANTIZATION_CONFIG = '4bit'          # '4bit', '8bit' supported on CUDA\n",
    "else:\n",
    "    # Settings for Mac (MPS) or CPU\n",
    "    print(\"CUDA not detected. Using MPS/CPU configuration (Quantization disabled).\")\n",
    "    QUANTIZATION_CONFIG = None            # bitsandbytes quantization is not supported on MPS yet\n",
    "\n",
    "# SAMPLING / REPRODUCIBILITY SETTINGS\n",
    "# - Set SEED = -1 to disable fixed seeding (non-deterministic runs). Set to an integer for reproducible runs.\n",
    "SEED = -1\n",
    "MAX_NEW_TOKENS = 512 #Max tokens for response. Should be in balance with the model's context size\n",
    "\n",
    "# TEMPERATURE = 0.2  # Less deterministic\n",
    "# TOP_P = 0.9        # Less deterministic\n",
    "TEMPERATURE = 0.01 # More deterministic (same as local settings of original experiments)\n",
    "TOP_P = 0          # More deterministic (same as local settings of original experiments)\n",
    "\n",
    "# PROBLEM SETTINGS\n",
    "# PROBLEM_NAMES = ['sports scheduling']\n",
    "PROBLEM_NAMES = ['nurse_scheduling']\n",
    "# PROBLEM_NAMES = ['nurse_scheduling', 'sports scheduling']\n",
    "# PROBLEM_NAMES = ['post_enrollment_based_course_time_tabling', 'examination_timetabling']\n",
    "# PROBLEM_NAMES = list(all_problems.keys())  # To run the program for ALL available problem names\n",
    "MAX_SYNTAX_REPAIRS = 3  # Maximum number of repair attempts per statement block for syntax errors\n",
    "MAX_SEMANTIC_REPAIRS = 0  # Maximum number of repair attempts per statement block for semantic errors\n",
    "RUNS_PER_PROBLEM = 1  # Number of runs per problem for averaging results\n",
    "\n",
    "if RUN_LOCAL_MODEL:\n",
    "    # To work locally, we need to manually load the pipeline \n",
    "    PIPE = bots.load_pipe(model_checkpoint=CHECKPOINT, local_dir=\"./local_models\", quantization_config=QUANTIZATION_CONFIG, save=True)\n",
    "    SEMANTICS_PIPE = bots.load_pipe(model_checkpoint=CHECKPOINT_SEMANTICS, local_dir=\"./local_models\", quantization_config=QUANTIZATION_CONFIG, save=True)  \n",
    "else:\n",
    "    # For remote models, we set pipe to a string with the model name\n",
    "    PIPE = REMOTE_PIPE\n",
    "    SEMANTICS_PIPE = REMOTE_PIPE_SEMANTICS\n",
    "\n",
    "# Run the LLM scheduler per problem\n",
    "for problem_name in PROBLEM_NAMES:\n",
    "    for run_id in range(RUNS_PER_PROBLEM):        \n",
    "        # Initialize the metrics logger\n",
    "        # Build a model identifier string for the logfile (include LOCAL/REMOTE)\n",
    "        model_id = (f\"{CHECKPOINT} (LOCAL, QUANTIZATION: {QUANTIZATION_CONFIG})\" if RUN_LOCAL_MODEL else (f\"{REMOTE_PIPE} (REMOTE)\" if REMOTE_PIPE is not None else \"Meta-Llama-3-8B-Instruct\"))\n",
    "        semantics_model_id = (f\"{CHECKPOINT_SEMANTICS} (LOCAL, QUANTIZATION: {QUANTIZATION_CONFIG})\" if RUN_LOCAL_MODEL else (f\"{REMOTE_PIPE_SEMANTICS} (REMOTE)\" if REMOTE_PIPE_SEMANTICS is not None else \"Meta-Llama-3-8B-Instruct\"))\n",
    "        logger.init_logger(filename=METRICS_LOG_FILE,\n",
    "                           problem_ID=problem_name,\n",
    "                           max_fix_attempts=MAX_SYNTAX_REPAIRS,\n",
    "                           model=model_id,\n",
    "                           semantics_model=semantics_model_id,\n",
    "                           temperature=TEMPERATURE,\n",
    "                           top_p=TOP_P,\n",
    "                           seed=SEED)\n",
    "\n",
    "        full_program = scheduler.full_ASP_program(\n",
    "            all_problems[problem_name],    # Input problem specifications for examination timetabling\n",
    "            pipe=PIPE,                     # Input the PIPEline object for the LLM\n",
    "            semantic_validation_pipe=SEMANTICS_PIPE, # Input the PIPEline object for the semantics validation LLM\n",
    "            printer=PRINT,                 # Set to True to print intermediate outputs\n",
    "            k=MAX_SYNTAX_REPAIRS,                 # Max repairs\n",
    "            n=MAX_SEMANTIC_REPAIRS,                 # Max repairs\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            seed=(None if SEED == -1 else SEED),\n",
    "            max_new_tokens=MAX_NEW_TOKENS)\n",
    "                            \n",
    "\n",
    "        if PROGRAM_FOLDER is not None:\n",
    "            # Save the full program to a file\n",
    "            os.makedirs(PROGRAM_FOLDER, exist_ok=True)\n",
    "            timestamp = logger.time_stamp()\n",
    "            if RUN_LOCAL_MODEL:\n",
    "                model_string = CHECKPOINT_SHORT_NAME\n",
    "                if QUANTIZATION_CONFIG is not None:\n",
    "                    # Append quantization info like \" (quant 4bit)\" or \" (quant 8bit)\"\n",
    "                    model_string = f\"{model_string} (quant {QUANTIZATION_CONFIG})\"\n",
    "            else:\n",
    "                model_string = REMOTE_PIPE if REMOTE_PIPE is not None else \"Meta-Llama-3-8B-Instruct\"\n",
    "            max_repairs_string = f\"_k={MAX_SYNTAX_REPAIRS}\" if MAX_SYNTAX_REPAIRS is not None else \"\"\n",
    "            program_filename = os.path.join(PROGRAM_FOLDER, f\"{problem_name}_{model_string}{max_repairs_string}_{timestamp}.lp\")\n",
    "            with open(program_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(full_program)\n",
    "            if PRINT:\n",
    "                print(f\"Full program saved to {program_filename}\")\n",
    "        else:\n",
    "            # Print the full program as it is returned by the scheduler\n",
    "            print('----------------------------FULL PROGRAM----------------------------')\n",
    "            print(full_program)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schasplm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
